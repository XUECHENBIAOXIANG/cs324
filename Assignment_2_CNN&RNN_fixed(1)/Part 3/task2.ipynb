{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T06:15:47.138882Z",
     "start_time": "2024-05-15T06:15:47.135253Z"
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of dataset:  10000\n",
      "[0/251] Loss: 2.307733 (2.307733) Accuracy: 0.031250 (0.031250)\n",
      "[10/251] Loss: 2.324692 (2.306239) Accuracy: 0.062500 (0.088068)\n",
      "[20/251] Loss: 2.317623 (2.310406) Accuracy: 0.031250 (0.095238)\n",
      "[30/251] Loss: 2.291564 (2.308781) Accuracy: 0.093750 (0.094758)\n",
      "[40/251] Loss: 2.286667 (2.306372) Accuracy: 0.156250 (0.101372)\n",
      "[50/251] Loss: 2.314883 (2.305637) Accuracy: 0.062500 (0.101716)\n",
      "[60/251] Loss: 2.311466 (2.305459) Accuracy: 0.062500 (0.102459)\n",
      "[70/251] Loss: 2.301569 (2.304864) Accuracy: 0.093750 (0.101673)\n",
      "[80/251] Loss: 2.200818 (2.299099) Accuracy: 0.281250 (0.112654)\n",
      "[90/251] Loss: 2.281042 (2.291137) Accuracy: 0.093750 (0.124657)\n",
      "[100/251] Loss: 2.232127 (2.282382) Accuracy: 0.156250 (0.134901)\n",
      "[110/251] Loss: 2.193349 (2.276725) Accuracy: 0.187500 (0.141329)\n",
      "[120/251] Loss: 2.287381 (2.272675) Accuracy: 0.156250 (0.145661)\n",
      "[130/251] Loss: 2.237418 (2.265285) Accuracy: 0.187500 (0.154342)\n",
      "[140/251] Loss: 2.156276 (2.261146) Accuracy: 0.312500 (0.160018)\n",
      "[150/251] Loss: 2.132659 (2.255243) Accuracy: 0.406250 (0.169909)\n",
      "[160/251] Loss: 2.106302 (2.248244) Accuracy: 0.468750 (0.179348)\n",
      "[170/251] Loss: 2.058809 (2.240077) Accuracy: 0.406250 (0.189876)\n",
      "[180/251] Loss: 2.022108 (2.233739) Accuracy: 0.468750 (0.197686)\n",
      "[190/251] Loss: 2.099163 (2.224762) Accuracy: 0.375000 (0.208770)\n",
      "[200/251] Loss: 2.114142 (2.215803) Accuracy: 0.312500 (0.218750)\n",
      "[210/251] Loss: 2.173507 (2.208242) Accuracy: 0.187500 (0.226600)\n",
      "[220/251] Loss: 2.138179 (2.201373) Accuracy: 0.375000 (0.234729)\n",
      "[230/251] Loss: 1.984029 (2.194654) Accuracy: 0.531250 (0.244048)\n",
      "[240/251] Loss: 1.975942 (2.187906) Accuracy: 0.500000 (0.251556)\n",
      "[250/251] Loss: 1.625293 (2.177276) Accuracy: 1.000000 (0.265563)\n",
      "[0/63] Loss: 2.129636 (2.129636) Accuracy: 0.343750 (0.343750)\n",
      "[10/63] Loss: 2.191884 (2.194041) Accuracy: 0.218750 (0.252841)\n",
      "[20/63] Loss: 2.262838 (2.194233) Accuracy: 0.125000 (0.241071)\n",
      "[30/63] Loss: 2.195461 (2.195973) Accuracy: 0.281250 (0.238911)\n",
      "[40/63] Loss: 2.302284 (2.205771) Accuracy: 0.125000 (0.224085)\n",
      "[50/63] Loss: 2.210754 (2.205864) Accuracy: 0.250000 (0.225490)\n",
      "[60/63] Loss: 2.257457 (2.204599) Accuracy: 0.062500 (0.225410)\n",
      "Epoch: 1/100, Train Loss: 2.1773, Train Acc: 0.2656, Val. Loss: 2.2030, Val. Acc: 0.2274\n",
      "[0/251] Loss: 2.187757 (2.187757) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 1.952165 (2.038385) Accuracy: 0.531250 (0.451705)\n",
      "[20/251] Loss: 1.977791 (2.028460) Accuracy: 0.468750 (0.446429)\n",
      "[30/251] Loss: 1.886620 (2.003843) Accuracy: 0.656250 (0.478831)\n",
      "[40/251] Loss: 2.028929 (2.006340) Accuracy: 0.468750 (0.474848)\n",
      "[50/251] Loss: 2.033423 (1.991504) Accuracy: 0.406250 (0.487745)\n",
      "[60/251] Loss: 1.928899 (1.986514) Accuracy: 0.531250 (0.492828)\n",
      "[70/251] Loss: 1.868320 (1.982728) Accuracy: 0.656250 (0.497359)\n",
      "[80/251] Loss: 1.920941 (1.973854) Accuracy: 0.562500 (0.510031)\n",
      "[90/251] Loss: 1.890797 (1.971573) Accuracy: 0.593750 (0.513736)\n",
      "[100/251] Loss: 1.856983 (1.966088) Accuracy: 0.593750 (0.520730)\n",
      "[110/251] Loss: 1.765195 (1.961025) Accuracy: 0.812500 (0.528716)\n",
      "[120/251] Loss: 1.931594 (1.958585) Accuracy: 0.562500 (0.531508)\n",
      "[130/251] Loss: 1.888747 (1.948290) Accuracy: 0.593750 (0.542462)\n",
      "[140/251] Loss: 1.861326 (1.943758) Accuracy: 0.625000 (0.546764)\n",
      "[150/251] Loss: 1.798215 (1.939030) Accuracy: 0.718750 (0.551531)\n",
      "[160/251] Loss: 1.786610 (1.933357) Accuracy: 0.687500 (0.556677)\n",
      "[170/251] Loss: 1.783285 (1.929460) Accuracy: 0.687500 (0.561404)\n",
      "[180/251] Loss: 1.752544 (1.924191) Accuracy: 0.718750 (0.566298)\n",
      "[190/251] Loss: 1.914239 (1.917121) Accuracy: 0.562500 (0.575262)\n",
      "[200/251] Loss: 1.949915 (1.910498) Accuracy: 0.500000 (0.581934)\n",
      "[210/251] Loss: 1.799920 (1.905699) Accuracy: 0.718750 (0.587826)\n",
      "[220/251] Loss: 1.774727 (1.900061) Accuracy: 0.718750 (0.593891)\n",
      "[230/251] Loss: 1.865789 (1.893775) Accuracy: 0.625000 (0.600785)\n",
      "[240/251] Loss: 1.708997 (1.888516) Accuracy: 0.843750 (0.605939)\n",
      "[250/251] Loss: 2.416360 (1.885872) Accuracy: 0.000000 (0.608566)\n",
      "[0/63] Loss: 1.665849 (1.665849) Accuracy: 0.843750 (0.843750)\n",
      "[10/63] Loss: 1.732563 (1.739760) Accuracy: 0.812500 (0.781250)\n",
      "[20/63] Loss: 1.694988 (1.750150) Accuracy: 0.812500 (0.763393)\n",
      "[30/63] Loss: 1.802381 (1.762962) Accuracy: 0.687500 (0.743952)\n",
      "[40/63] Loss: 1.744816 (1.764868) Accuracy: 0.781250 (0.745427)\n",
      "[50/63] Loss: 1.748184 (1.760368) Accuracy: 0.750000 (0.752451)\n",
      "[60/63] Loss: 1.765457 (1.760027) Accuracy: 0.750000 (0.752049)\n",
      "Epoch: 2/100, Train Loss: 1.8859, Train Acc: 0.6086, Val. Loss: 1.7594, Val. Acc: 0.7532\n",
      "[0/251] Loss: 1.765595 (1.765595) Accuracy: 0.750000 (0.750000)\n",
      "[10/251] Loss: 1.742956 (1.728990) Accuracy: 0.781250 (0.778409)\n",
      "[20/251] Loss: 1.715668 (1.741797) Accuracy: 0.750000 (0.763393)\n",
      "[30/251] Loss: 1.786113 (1.752673) Accuracy: 0.718750 (0.753024)\n",
      "[40/251] Loss: 1.779863 (1.747657) Accuracy: 0.687500 (0.761433)\n",
      "[50/251] Loss: 1.670065 (1.737457) Accuracy: 0.875000 (0.776348)\n",
      "[60/251] Loss: 1.648043 (1.725637) Accuracy: 0.906250 (0.791496)\n",
      "[70/251] Loss: 1.655189 (1.722517) Accuracy: 0.875000 (0.795335)\n",
      "[80/251] Loss: 1.678053 (1.716702) Accuracy: 0.843750 (0.799383)\n",
      "[90/251] Loss: 1.711677 (1.714607) Accuracy: 0.812500 (0.802541)\n",
      "[100/251] Loss: 1.907175 (1.712076) Accuracy: 0.500000 (0.802599)\n",
      "[110/251] Loss: 1.644040 (1.710617) Accuracy: 0.875000 (0.804054)\n",
      "[120/251] Loss: 1.662198 (1.708957) Accuracy: 0.843750 (0.805269)\n",
      "[130/251] Loss: 1.702025 (1.706436) Accuracy: 0.812500 (0.807729)\n",
      "[140/251] Loss: 1.628395 (1.702043) Accuracy: 0.843750 (0.811170)\n",
      "[150/251] Loss: 1.636273 (1.699831) Accuracy: 0.875000 (0.812914)\n",
      "[160/251] Loss: 1.612871 (1.696140) Accuracy: 0.906250 (0.816770)\n",
      "[170/251] Loss: 1.769079 (1.694495) Accuracy: 0.687500 (0.817800)\n",
      "[180/251] Loss: 1.714097 (1.692545) Accuracy: 0.750000 (0.818543)\n",
      "[190/251] Loss: 1.547202 (1.688062) Accuracy: 0.968750 (0.822808)\n",
      "[200/251] Loss: 1.671237 (1.686531) Accuracy: 0.781250 (0.822917)\n",
      "[210/251] Loss: 1.590090 (1.682759) Accuracy: 0.906250 (0.825977)\n",
      "[220/251] Loss: 1.508920 (1.679659) Accuracy: 1.000000 (0.828196)\n",
      "[230/251] Loss: 1.653008 (1.676870) Accuracy: 0.812500 (0.829816)\n",
      "[240/251] Loss: 1.773245 (1.676307) Accuracy: 0.656250 (0.829616)\n",
      "[250/251] Loss: 1.615522 (1.674706) Accuracy: 1.000000 (0.831051)\n",
      "[0/63] Loss: 1.569540 (1.569540) Accuracy: 0.906250 (0.906250)\n",
      "[10/63] Loss: 1.612603 (1.587331) Accuracy: 0.875000 (0.906250)\n",
      "[20/63] Loss: 1.578522 (1.599920) Accuracy: 0.875000 (0.885417)\n",
      "[30/63] Loss: 1.690332 (1.611054) Accuracy: 0.812500 (0.875000)\n",
      "[40/63] Loss: 1.599126 (1.611132) Accuracy: 0.875000 (0.875000)\n",
      "[50/63] Loss: 1.599630 (1.608110) Accuracy: 0.875000 (0.876838)\n",
      "[60/63] Loss: 1.591422 (1.608826) Accuracy: 0.906250 (0.875512)\n",
      "Epoch: 3/100, Train Loss: 1.6747, Train Acc: 0.8311, Val. Loss: 1.6085, Val. Acc: 0.8753\n",
      "[0/251] Loss: 1.574674 (1.574674) Accuracy: 0.906250 (0.906250)\n",
      "[10/251] Loss: 1.595344 (1.569655) Accuracy: 0.843750 (0.914773)\n",
      "[20/251] Loss: 1.534660 (1.564248) Accuracy: 1.000000 (0.941964)\n",
      "[30/251] Loss: 1.526944 (1.554872) Accuracy: 1.000000 (0.960685)\n",
      "[40/251] Loss: 1.694366 (1.558906) Accuracy: 0.812500 (0.962652)\n",
      "[50/251] Loss: 1.577034 (1.558899) Accuracy: 0.968750 (0.966299)\n",
      "[60/251] Loss: 1.569244 (1.562082) Accuracy: 0.937500 (0.962602)\n",
      "[70/251] Loss: 1.526824 (1.569712) Accuracy: 1.000000 (0.951144)\n",
      "[80/251] Loss: 1.532876 (1.565340) Accuracy: 1.000000 (0.957176)\n",
      "[90/251] Loss: 1.509525 (1.559859) Accuracy: 1.000000 (0.961882)\n",
      "[100/251] Loss: 1.527351 (1.555344) Accuracy: 1.000000 (0.965656)\n",
      "[110/251] Loss: 1.598916 (1.568103) Accuracy: 0.875000 (0.948198)\n",
      "[120/251] Loss: 1.546909 (1.570974) Accuracy: 0.937500 (0.943182)\n",
      "[130/251] Loss: 1.509233 (1.566306) Accuracy: 1.000000 (0.947519)\n",
      "[140/251] Loss: 1.489159 (1.561762) Accuracy: 1.000000 (0.951241)\n",
      "[150/251] Loss: 1.492006 (1.557919) Accuracy: 1.000000 (0.954470)\n",
      "[160/251] Loss: 1.498621 (1.554160) Accuracy: 1.000000 (0.957298)\n",
      "[170/251] Loss: 1.486505 (1.550569) Accuracy: 1.000000 (0.959795)\n",
      "[180/251] Loss: 1.491539 (1.547496) Accuracy: 1.000000 (0.962017)\n",
      "[190/251] Loss: 1.490079 (1.544582) Accuracy: 1.000000 (0.964005)\n",
      "[200/251] Loss: 1.484947 (1.541807) Accuracy: 1.000000 (0.965796)\n",
      "[210/251] Loss: 1.486342 (1.539210) Accuracy: 1.000000 (0.967417)\n",
      "[220/251] Loss: 1.487732 (1.536715) Accuracy: 1.000000 (0.968891)\n",
      "[230/251] Loss: 1.496421 (1.534517) Accuracy: 1.000000 (0.970238)\n",
      "[240/251] Loss: 1.490194 (1.532570) Accuracy: 1.000000 (0.971473)\n",
      "[250/251] Loss: 1.519438 (1.532855) Accuracy: 1.000000 (0.970369)\n",
      "[0/63] Loss: 1.628952 (1.628952) Accuracy: 0.812500 (0.812500)\n",
      "[10/63] Loss: 1.721784 (1.739126) Accuracy: 0.718750 (0.715909)\n",
      "[20/63] Loss: 1.721863 (1.742293) Accuracy: 0.718750 (0.711310)\n",
      "[30/63] Loss: 1.876975 (1.762049) Accuracy: 0.562500 (0.693548)\n",
      "[40/63] Loss: 1.724691 (1.764041) Accuracy: 0.750000 (0.695122)\n",
      "[50/63] Loss: 1.703640 (1.756991) Accuracy: 0.750000 (0.700980)\n",
      "[60/63] Loss: 1.740145 (1.753572) Accuracy: 0.718750 (0.704406)\n",
      "Epoch: 4/100, Train Loss: 1.5329, Train Acc: 0.9704, Val. Loss: 1.7521, Val. Acc: 0.7055\n",
      "[0/251] Loss: 1.749349 (1.749349) Accuracy: 0.687500 (0.687500)\n",
      "[10/251] Loss: 1.487277 (1.527387) Accuracy: 1.000000 (0.954545)\n",
      "[20/251] Loss: 1.475961 (1.506121) Accuracy: 1.000000 (0.976190)\n",
      "[30/251] Loss: 1.475145 (1.496951) Accuracy: 1.000000 (0.983871)\n",
      "[40/251] Loss: 1.482308 (1.492905) Accuracy: 1.000000 (0.987805)\n",
      "[50/251] Loss: 1.476489 (1.489976) Accuracy: 1.000000 (0.990196)\n",
      "[60/251] Loss: 1.481582 (1.487822) Accuracy: 1.000000 (0.991803)\n",
      "[70/251] Loss: 1.502361 (1.499659) Accuracy: 1.000000 (0.981074)\n",
      "[80/251] Loss: 1.472860 (1.501458) Accuracy: 1.000000 (0.979167)\n",
      "[90/251] Loss: 1.471361 (1.498266) Accuracy: 1.000000 (0.981456)\n",
      "[100/251] Loss: 1.470800 (1.495869) Accuracy: 1.000000 (0.983292)\n",
      "[110/251] Loss: 1.470498 (1.493750) Accuracy: 1.000000 (0.984797)\n",
      "[120/251] Loss: 1.474683 (1.492004) Accuracy: 1.000000 (0.986054)\n",
      "[130/251] Loss: 1.471309 (1.490486) Accuracy: 1.000000 (0.987118)\n",
      "[140/251] Loss: 1.474418 (1.489141) Accuracy: 1.000000 (0.988032)\n",
      "[150/251] Loss: 1.469910 (1.487948) Accuracy: 1.000000 (0.988825)\n",
      "[160/251] Loss: 1.468370 (1.486886) Accuracy: 1.000000 (0.989519)\n",
      "[170/251] Loss: 1.472756 (1.485918) Accuracy: 1.000000 (0.990132)\n",
      "[180/251] Loss: 1.467977 (1.485038) Accuracy: 1.000000 (0.990677)\n",
      "[190/251] Loss: 1.469518 (1.484189) Accuracy: 1.000000 (0.991165)\n",
      "[200/251] Loss: 1.471483 (1.483412) Accuracy: 1.000000 (0.991604)\n",
      "[210/251] Loss: 1.470878 (1.482729) Accuracy: 1.000000 (0.992002)\n",
      "[220/251] Loss: 1.469111 (1.482059) Accuracy: 1.000000 (0.992364)\n",
      "[230/251] Loss: 1.468719 (1.481456) Accuracy: 1.000000 (0.992695)\n",
      "[240/251] Loss: 1.468357 (1.480912) Accuracy: 1.000000 (0.992998)\n",
      "[250/251] Loss: 1.461625 (1.480366) Accuracy: 1.000000 (0.993277)\n",
      "[0/63] Loss: 1.466148 (1.466148) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.466961 (1.466970) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.465402 (1.467396) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.467436 (1.467412) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.467264 (1.467377) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.466420 (1.467221) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.467330 (1.467254) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 5/100, Train Loss: 1.4804, Train Acc: 0.9933, Val. Loss: 1.4672, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.467102 (1.467102) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.466813 (1.466849) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.467140 (1.466876) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.465773 (1.466613) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.466919 (1.466434) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.465803 (1.466421) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.466318 (1.466365) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.465481 (1.466246) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.796532 (1.479709) Accuracy: 0.656250 (0.985725)\n",
      "[90/251] Loss: 1.636615 (1.510225) Accuracy: 0.812500 (0.953297)\n",
      "[100/251] Loss: 1.593809 (1.523025) Accuracy: 0.875000 (0.940903)\n",
      "[110/251] Loss: 1.594830 (1.529826) Accuracy: 0.875000 (0.934685)\n",
      "[120/251] Loss: 1.560029 (1.531427) Accuracy: 0.906250 (0.933368)\n",
      "[130/251] Loss: 1.586554 (1.535742) Accuracy: 0.875000 (0.929151)\n",
      "[140/251] Loss: 1.681213 (1.543308) Accuracy: 0.781250 (0.921764)\n",
      "[150/251] Loss: 1.466379 (1.543171) Accuracy: 1.000000 (0.922185)\n",
      "[160/251] Loss: 1.465219 (1.538359) Accuracy: 1.000000 (0.927019)\n",
      "[170/251] Loss: 1.465661 (1.534102) Accuracy: 1.000000 (0.931287)\n",
      "[180/251] Loss: 1.463616 (1.530281) Accuracy: 1.000000 (0.935083)\n",
      "[190/251] Loss: 1.464065 (1.526845) Accuracy: 1.000000 (0.938482)\n",
      "[200/251] Loss: 1.464918 (1.523765) Accuracy: 1.000000 (0.941542)\n",
      "[210/251] Loss: 1.465834 (1.520975) Accuracy: 1.000000 (0.944313)\n",
      "[220/251] Loss: 1.464693 (1.518432) Accuracy: 1.000000 (0.946833)\n",
      "[230/251] Loss: 1.464930 (1.516106) Accuracy: 1.000000 (0.949134)\n",
      "[240/251] Loss: 1.463949 (1.513956) Accuracy: 1.000000 (0.951245)\n",
      "[250/251] Loss: 1.461813 (1.511955) Accuracy: 1.000000 (0.953187)\n",
      "[0/63] Loss: 1.463667 (1.463667) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.464096 (1.464116) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.463149 (1.464316) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.464245 (1.464310) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.464266 (1.464306) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.463768 (1.464218) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.464233 (1.464226) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 6/100, Train Loss: 1.5120, Train Acc: 0.9532, Val. Loss: 1.4642, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.464671 (1.464671) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.463442 (1.464138) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.463451 (1.464160) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.463511 (1.464111) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.463880 (1.464051) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.464177 (1.464055) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.464052 (1.463986) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.463027 (1.463926) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.463447 (1.463874) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.463682 (1.463869) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.463196 (1.463826) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.463578 (1.463781) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.463366 (1.463737) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.463135 (1.463703) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.463462 (1.463662) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.463260 (1.463613) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.462988 (1.463576) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.462713 (1.463540) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.462895 (1.463507) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.462712 (1.463466) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.463116 (1.463442) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.462334 (1.463404) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.462642 (1.463373) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.463211 (1.463338) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.462427 (1.463303) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.462062 (1.463271) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.462308 (1.462308) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.462555 (1.462516) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.462124 (1.462622) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.462751 (1.462622) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.462558 (1.462618) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.462326 (1.462574) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.462512 (1.462577) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 7/100, Train Loss: 1.4633, Train Acc: 1.0000, Val. Loss: 1.4626, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.462982 (1.462982) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.462295 (1.462410) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.462225 (1.462374) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.462397 (1.462381) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.462153 (1.462391) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.462117 (1.462360) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.462095 (1.462350) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.462294 (1.462350) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461998 (1.462325) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.462085 (1.462294) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.462143 (1.462287) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.462416 (1.462261) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.462112 (1.462252) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461916 (1.462234) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461958 (1.462219) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461922 (1.462199) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461876 (1.462188) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461723 (1.462169) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461787 (1.462151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461746 (1.462135) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461747 (1.462119) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461669 (1.462101) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461667 (1.462086) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461710 (1.462070) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461585 (1.462054) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.462078 (1.462042) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461674 (1.461674) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461731 (1.461685) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461507 (1.461727) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461716 (1.461723) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461689 (1.461720) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461593 (1.461703) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461658 (1.461703) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 8/100, Train Loss: 1.4620, Train Acc: 1.0000, Val. Loss: 1.4617, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461758 (1.461758) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461582 (1.461697) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461611 (1.461658) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461613 (1.461635) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461605 (1.461636) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461559 (1.461628) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461482 (1.461618) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461537 (1.461604) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461529 (1.461595) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461562 (1.461593) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461642 (1.461583) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461482 (1.461576) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461427 (1.461572) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461409 (1.461563) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461401 (1.461555) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461381 (1.461547) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461435 (1.461541) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461412 (1.461533) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461384 (1.461527) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461390 (1.461518) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461368 (1.461511) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461327 (1.461505) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461380 (1.461500) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461365 (1.461494) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461335 (1.461488) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461355 (1.461482) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461306 (1.461306) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461359 (1.461342) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461277 (1.461357) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461349 (1.461355) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461361 (1.461355) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461314 (1.461350) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461339 (1.461350) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 9/100, Train Loss: 1.4615, Train Acc: 1.0000, Val. Loss: 1.4613, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461286 (1.461286) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461330 (1.461353) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461326 (1.461341) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461339 (1.461335) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461305 (1.461329) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461301 (1.461325) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461283 (1.461321) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461288 (1.461318) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461280 (1.461314) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461286 (1.461310) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461336 (1.461307) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461256 (1.461304) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461256 (1.461302) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461300 (1.461299) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461261 (1.461296) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461278 (1.461294) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461264 (1.461291) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461237 (1.461288) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461234 (1.461285) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461228 (1.461283) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461217 (1.461280) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461227 (1.461278) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461203 (1.461275) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461238 (1.461273) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461223 (1.461271) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461268) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461206 (1.461206) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461216 (1.461216) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461196 (1.461221) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461215 (1.461220) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461221 (1.461220) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461206 (1.461218) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461215 (1.461218) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 10/100, Train Loss: 1.4613, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461214 (1.461214) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461197 (1.461221) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461226 (1.461218) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461197 (1.461214) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461186 (1.461212) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461206 (1.461211) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461203 (1.461211) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461202 (1.461210) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461226 (1.461209) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461198 (1.461207) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461187 (1.461206) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461192 (1.461205) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461175 (1.461204) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461202 (1.461203) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461192 (1.461202) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461190 (1.461201) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461177 (1.461199) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461186 (1.461199) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461174 (1.461198) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461177 (1.461197) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461180 (1.461196) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461180 (1.461195) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461185 (1.461194) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461178 (1.461193) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461176 (1.461193) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461192 (1.461192) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461168 (1.461168) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461173 (1.461173) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461167 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461173 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461175 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461170 (1.461174) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461173 (1.461174) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 11/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461172 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461179 (1.461173) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461169 (1.461173) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461174 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461173 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461166 (1.461171) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461166 (1.461171) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461164 (1.461171) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461170 (1.461170) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461170 (1.461170) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461165 (1.461170) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461167 (1.461169) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461162 (1.461169) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461163 (1.461169) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461167 (1.461168) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461161 (1.461168) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461160 (1.461168) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461161 (1.461167) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461203 (1.465881) Accuracy: 1.000000 (0.995338)\n",
      "[190/251] Loss: 1.461164 (1.465637) Accuracy: 1.000000 (0.995582)\n",
      "[200/251] Loss: 1.461209 (1.465416) Accuracy: 1.000000 (0.995802)\n",
      "[210/251] Loss: 1.461172 (1.465215) Accuracy: 1.000000 (0.996001)\n",
      "[220/251] Loss: 1.461164 (1.465032) Accuracy: 1.000000 (0.996182)\n",
      "[230/251] Loss: 1.461173 (1.464866) Accuracy: 1.000000 (0.996347)\n",
      "[240/251] Loss: 1.461165 (1.464719) Accuracy: 1.000000 (0.996499)\n",
      "[250/251] Loss: 1.461153 (1.464578) Accuracy: 1.000000 (0.996638)\n",
      "[0/63] Loss: 1.461166 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461178 (1.461176) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461166 (1.461201) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461231 (1.461195) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461173 (1.461191) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461165 (1.461188) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461176 (1.461188) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 12/100, Train Loss: 1.4646, Train Acc: 0.9966, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461197 (1.461197) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461173 (1.461177) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461170 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461170 (1.461177) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461169 (1.461176) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461163 (1.461174) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461165 (1.461173) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461165 (1.461173) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461171 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461165 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461170 (1.461171) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461166 (1.461171) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461176 (1.461170) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461168 (1.461170) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461160 (1.461169) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461164 (1.461169) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461164 (1.461169) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461163 (1.461169) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461168 (1.461169) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461164 (1.461168) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461164 (1.461168) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461164 (1.461168) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461161 (1.461168) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461172 (1.461168) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461156 (1.461167) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461154 (1.461167) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461160 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461162 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461158 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461163 (1.461165) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461164 (1.461165) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461161 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461163 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 13/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461160 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461161 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461167 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461162 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461160 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461163 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461164 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461164 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461164 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461161 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461162 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461159 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461158 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461166 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461160 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461160 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461162 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461159 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461162 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461164 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461159 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461158 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461156 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461158 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461160 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461161 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461158 (1.461158) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461159 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461157 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461159 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461160 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461159 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461160 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 14/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461158 (1.461158) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461159 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461159 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461161 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461162 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461159 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461160 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461157 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461157 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461159 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461160 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461161 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461158 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461157 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461156 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461158 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461157 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461155 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461158 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461159 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461161 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461154 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461155 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461160 (1.461158) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461157 (1.461158) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461158) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461157 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461155 (1.461158) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461157 (1.461158) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461158 (1.461158) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461157 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461157 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 15/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461157 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461158 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461160 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461157 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461157 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461155 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461156 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461157 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461156 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461155 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461155 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461158 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461158 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461157 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461157 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461154 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 16/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461157 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461156 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461153 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461153 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461152 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461152 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 17/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461154 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461154 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461153 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 18/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 19/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461159 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461156 (1.461170) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461155 (1.461165) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461167 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461167) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461168) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 20/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461166 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.466831) Accuracy: 1.000000 (0.994318)\n",
      "[20/251] Loss: 1.461151 (1.464126) Accuracy: 1.000000 (0.997024)\n",
      "[30/251] Loss: 1.461151 (1.463166) Accuracy: 1.000000 (0.997984)\n",
      "[40/251] Loss: 1.461151 (1.462675) Accuracy: 1.000000 (0.998476)\n",
      "[50/251] Loss: 1.461151 (1.462376) Accuracy: 1.000000 (0.998775)\n",
      "[60/251] Loss: 1.461151 (1.462175) Accuracy: 1.000000 (0.998975)\n",
      "[70/251] Loss: 1.461151 (1.462031) Accuracy: 1.000000 (0.999120)\n",
      "[80/251] Loss: 1.461151 (1.461922) Accuracy: 1.000000 (0.999228)\n",
      "[90/251] Loss: 1.461151 (1.461838) Accuracy: 1.000000 (0.999313)\n",
      "[100/251] Loss: 1.461151 (1.461770) Accuracy: 1.000000 (0.999381)\n",
      "[110/251] Loss: 1.461151 (1.461714) Accuracy: 1.000000 (0.999437)\n",
      "[120/251] Loss: 1.461151 (1.461667) Accuracy: 1.000000 (0.999483)\n",
      "[130/251] Loss: 1.461151 (1.461628) Accuracy: 1.000000 (0.999523)\n",
      "[140/251] Loss: 1.461151 (1.461594) Accuracy: 1.000000 (0.999557)\n",
      "[150/251] Loss: 1.461151 (1.461565) Accuracy: 1.000000 (0.999586)\n",
      "[160/251] Loss: 1.461151 (1.461539) Accuracy: 1.000000 (0.999612)\n",
      "[170/251] Loss: 1.461151 (1.461516) Accuracy: 1.000000 (0.999635)\n",
      "[180/251] Loss: 1.461151 (1.461496) Accuracy: 1.000000 (0.999655)\n",
      "[190/251] Loss: 1.461151 (1.461478) Accuracy: 1.000000 (0.999673)\n",
      "[200/251] Loss: 1.461151 (1.461462) Accuracy: 1.000000 (0.999689)\n",
      "[210/251] Loss: 1.461151 (1.461447) Accuracy: 1.000000 (0.999704)\n",
      "[220/251] Loss: 1.461151 (1.461434) Accuracy: 1.000000 (0.999717)\n",
      "[230/251] Loss: 1.461151 (1.461421) Accuracy: 1.000000 (0.999729)\n",
      "[240/251] Loss: 1.461151 (1.461410) Accuracy: 1.000000 (0.999741)\n",
      "[250/251] Loss: 1.461150 (1.461400) Accuracy: 1.000000 (0.999751)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 21/100, Train Loss: 1.4614, Train Acc: 0.9998, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 22/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 23/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 24/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 25/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 26/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 27/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 28/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 29/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 30/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 31/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 32/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 33/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 34/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 35/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 36/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 37/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 38/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 39/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 40/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 41/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 42/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 43/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 44/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 45/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 46/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 47/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 48/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 49/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 50/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 51/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 52/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 53/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 54/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 55/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 56/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 57/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 58/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 59/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 60/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 61/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 62/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 63/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 64/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 65/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 66/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 67/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 68/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 69/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 70/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 71/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 72/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 73/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 74/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 75/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 76/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 77/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 78/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 79/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 80/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 81/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 82/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 83/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 84/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 85/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 86/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 87/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 88/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 89/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 90/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 91/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 92/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 93/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 94/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 95/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 96/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 97/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 98/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 99/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 100/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "Done training.\n",
      "Total length of dataset:  10000\n",
      "[0/251] Loss: 2.300673 (2.300673) Accuracy: 0.000000 (0.000000)\n",
      "[10/251] Loss: 2.332001 (2.317639) Accuracy: 0.093750 (0.090909)\n",
      "[20/251] Loss: 2.311556 (2.313695) Accuracy: 0.093750 (0.087798)\n",
      "[30/251] Loss: 2.318692 (2.313500) Accuracy: 0.093750 (0.085685)\n",
      "[40/251] Loss: 2.305789 (2.312157) Accuracy: 0.125000 (0.087652)\n",
      "[50/251] Loss: 2.275340 (2.311671) Accuracy: 0.187500 (0.087623)\n",
      "[60/251] Loss: 2.305202 (2.310109) Accuracy: 0.031250 (0.091701)\n",
      "[70/251] Loss: 2.296373 (2.308510) Accuracy: 0.156250 (0.098592)\n",
      "[80/251] Loss: 2.305721 (2.309220) Accuracy: 0.000000 (0.094136)\n",
      "[90/251] Loss: 2.331678 (2.308617) Accuracy: 0.093750 (0.098214)\n",
      "[100/251] Loss: 2.300962 (2.308479) Accuracy: 0.125000 (0.098700)\n",
      "[110/251] Loss: 2.331521 (2.308131) Accuracy: 0.062500 (0.100225)\n",
      "[120/251] Loss: 2.288327 (2.308266) Accuracy: 0.156250 (0.099432)\n",
      "[130/251] Loss: 2.323880 (2.308494) Accuracy: 0.000000 (0.097805)\n",
      "[140/251] Loss: 2.305831 (2.308298) Accuracy: 0.093750 (0.097961)\n",
      "[150/251] Loss: 2.308905 (2.308107) Accuracy: 0.062500 (0.097682)\n",
      "[160/251] Loss: 2.319422 (2.307544) Accuracy: 0.062500 (0.099767)\n",
      "[170/251] Loss: 2.314760 (2.307461) Accuracy: 0.093750 (0.101608)\n",
      "[180/251] Loss: 2.320766 (2.307672) Accuracy: 0.000000 (0.100656)\n",
      "[190/251] Loss: 2.307026 (2.307566) Accuracy: 0.062500 (0.100622)\n",
      "[200/251] Loss: 2.298403 (2.307361) Accuracy: 0.125000 (0.100902)\n",
      "[210/251] Loss: 2.310789 (2.307068) Accuracy: 0.062500 (0.101451)\n",
      "[220/251] Loss: 2.328624 (2.307112) Accuracy: 0.000000 (0.100820)\n",
      "[230/251] Loss: 2.287416 (2.306732) Accuracy: 0.187500 (0.101867)\n",
      "[240/251] Loss: 2.297359 (2.306816) Accuracy: 0.093750 (0.100882)\n",
      "[250/251] Loss: 2.339508 (2.306841) Accuracy: 0.000000 (0.101469)\n",
      "[0/63] Loss: 2.321712 (2.321712) Accuracy: 0.062500 (0.062500)\n",
      "[10/63] Loss: 2.311959 (2.310026) Accuracy: 0.062500 (0.102273)\n",
      "[20/63] Loss: 2.293802 (2.308081) Accuracy: 0.093750 (0.099702)\n",
      "[30/63] Loss: 2.331200 (2.308256) Accuracy: 0.031250 (0.093750)\n",
      "[40/63] Loss: 2.296137 (2.308639) Accuracy: 0.125000 (0.093750)\n",
      "[50/63] Loss: 2.319096 (2.308796) Accuracy: 0.156250 (0.094363)\n",
      "[60/63] Loss: 2.315590 (2.308868) Accuracy: 0.125000 (0.093750)\n",
      "Epoch: 1/100, Train Loss: 2.3068, Train Acc: 0.1015, Val. Loss: 2.3092, Val. Acc: 0.0923\n",
      "[0/251] Loss: 2.290099 (2.290099) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.285573 (2.297968) Accuracy: 0.187500 (0.136364)\n",
      "[20/251] Loss: 2.309945 (2.300021) Accuracy: 0.062500 (0.114583)\n",
      "[30/251] Loss: 2.287111 (2.299949) Accuracy: 0.156250 (0.117944)\n",
      "[40/251] Loss: 2.306323 (2.300123) Accuracy: 0.093750 (0.118140)\n",
      "[50/251] Loss: 2.301494 (2.301229) Accuracy: 0.093750 (0.112132)\n",
      "[60/251] Loss: 2.302544 (2.301457) Accuracy: 0.093750 (0.108607)\n",
      "[70/251] Loss: 2.294133 (2.300422) Accuracy: 0.093750 (0.113116)\n",
      "[80/251] Loss: 2.317712 (2.300810) Accuracy: 0.062500 (0.113426)\n",
      "[90/251] Loss: 2.298686 (2.301997) Accuracy: 0.156250 (0.109547)\n",
      "[100/251] Loss: 2.299148 (2.301858) Accuracy: 0.125000 (0.111696)\n",
      "[110/251] Loss: 2.311809 (2.301030) Accuracy: 0.062500 (0.114583)\n",
      "[120/251] Loss: 2.304930 (2.301072) Accuracy: 0.093750 (0.113636)\n",
      "[130/251] Loss: 2.292905 (2.300970) Accuracy: 0.125000 (0.115697)\n",
      "[140/251] Loss: 2.337534 (2.301514) Accuracy: 0.031250 (0.115691)\n",
      "[150/251] Loss: 2.301045 (2.301478) Accuracy: 0.125000 (0.117136)\n",
      "[160/251] Loss: 2.278276 (2.301564) Accuracy: 0.218750 (0.117042)\n",
      "[170/251] Loss: 2.336857 (2.302044) Accuracy: 0.062500 (0.115680)\n",
      "[180/251] Loss: 2.306439 (2.302321) Accuracy: 0.093750 (0.114468)\n",
      "[190/251] Loss: 2.331929 (2.302176) Accuracy: 0.031250 (0.115347)\n",
      "[200/251] Loss: 2.315387 (2.301535) Accuracy: 0.062500 (0.117071)\n",
      "[210/251] Loss: 2.309319 (2.301940) Accuracy: 0.093750 (0.116410)\n",
      "[220/251] Loss: 2.304812 (2.302233) Accuracy: 0.156250 (0.116092)\n",
      "[230/251] Loss: 2.299871 (2.302135) Accuracy: 0.125000 (0.115936)\n",
      "[240/251] Loss: 2.300817 (2.302393) Accuracy: 0.125000 (0.115405)\n",
      "[250/251] Loss: 2.225961 (2.302016) Accuracy: 0.000000 (0.114915)\n",
      "[0/63] Loss: 2.314613 (2.314613) Accuracy: 0.093750 (0.093750)\n",
      "[10/63] Loss: 2.322411 (2.311934) Accuracy: 0.062500 (0.107955)\n",
      "[20/63] Loss: 2.304660 (2.311393) Accuracy: 0.125000 (0.096726)\n",
      "[30/63] Loss: 2.337195 (2.313216) Accuracy: 0.031250 (0.093750)\n",
      "[40/63] Loss: 2.286393 (2.312378) Accuracy: 0.187500 (0.095274)\n",
      "[50/63] Loss: 2.297774 (2.310471) Accuracy: 0.156250 (0.098039)\n",
      "[60/63] Loss: 2.305720 (2.310704) Accuracy: 0.125000 (0.097848)\n",
      "Epoch: 2/100, Train Loss: 2.3020, Train Acc: 0.1149, Val. Loss: 2.3109, Val. Acc: 0.0979\n",
      "[0/251] Loss: 2.319922 (2.319922) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.305569 (2.301311) Accuracy: 0.093750 (0.096591)\n",
      "[20/251] Loss: 2.301754 (2.299030) Accuracy: 0.093750 (0.108631)\n",
      "[30/251] Loss: 2.308793 (2.300516) Accuracy: 0.062500 (0.106855)\n",
      "[40/251] Loss: 2.322170 (2.301184) Accuracy: 0.093750 (0.102896)\n",
      "[50/251] Loss: 2.256396 (2.299908) Accuracy: 0.218750 (0.108456)\n",
      "[60/251] Loss: 2.299639 (2.299083) Accuracy: 0.093750 (0.112193)\n",
      "[70/251] Loss: 2.317181 (2.300355) Accuracy: 0.125000 (0.108715)\n",
      "[80/251] Loss: 2.316521 (2.301239) Accuracy: 0.062500 (0.107639)\n",
      "[90/251] Loss: 2.265336 (2.300821) Accuracy: 0.187500 (0.110577)\n",
      "[100/251] Loss: 2.301263 (2.301258) Accuracy: 0.125000 (0.111077)\n",
      "[110/251] Loss: 2.307050 (2.301164) Accuracy: 0.156250 (0.113176)\n",
      "[120/251] Loss: 2.320073 (2.301351) Accuracy: 0.093750 (0.115444)\n",
      "[130/251] Loss: 2.306883 (2.301786) Accuracy: 0.093750 (0.114265)\n",
      "[140/251] Loss: 2.283023 (2.301121) Accuracy: 0.218750 (0.115248)\n",
      "[150/251] Loss: 2.322870 (2.301116) Accuracy: 0.000000 (0.114238)\n",
      "[160/251] Loss: 2.289320 (2.300891) Accuracy: 0.156250 (0.113548)\n",
      "[170/251] Loss: 2.306469 (2.300817) Accuracy: 0.125000 (0.113487)\n",
      "[180/251] Loss: 2.281051 (2.301079) Accuracy: 0.187500 (0.112569)\n",
      "[190/251] Loss: 2.302027 (2.301334) Accuracy: 0.062500 (0.111584)\n",
      "[200/251] Loss: 2.316928 (2.301309) Accuracy: 0.031250 (0.111474)\n",
      "[210/251] Loss: 2.318699 (2.301437) Accuracy: 0.093750 (0.111226)\n",
      "[220/251] Loss: 2.300310 (2.301493) Accuracy: 0.093750 (0.110577)\n",
      "[230/251] Loss: 2.300688 (2.301523) Accuracy: 0.093750 (0.109984)\n",
      "[240/251] Loss: 2.306862 (2.301323) Accuracy: 0.031250 (0.110996)\n",
      "[250/251] Loss: 2.329739 (2.301370) Accuracy: 0.000000 (0.111180)\n",
      "[0/63] Loss: 2.300616 (2.300616) Accuracy: 0.062500 (0.062500)\n",
      "[10/63] Loss: 2.313790 (2.305264) Accuracy: 0.031250 (0.105114)\n",
      "[20/63] Loss: 2.298175 (2.303233) Accuracy: 0.093750 (0.116071)\n",
      "[30/63] Loss: 2.307659 (2.303576) Accuracy: 0.062500 (0.105847)\n",
      "[40/63] Loss: 2.300327 (2.303768) Accuracy: 0.062500 (0.100610)\n",
      "[50/63] Loss: 2.299539 (2.302077) Accuracy: 0.156250 (0.104779)\n",
      "[60/63] Loss: 2.298833 (2.302638) Accuracy: 0.125000 (0.103996)\n",
      "Epoch: 3/100, Train Loss: 2.3014, Train Acc: 0.1112, Val. Loss: 2.3029, Val. Acc: 0.1022\n",
      "[0/251] Loss: 2.319232 (2.319232) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.299611 (2.302537) Accuracy: 0.125000 (0.119318)\n",
      "[20/251] Loss: 2.288300 (2.302943) Accuracy: 0.187500 (0.120536)\n",
      "[30/251] Loss: 2.292129 (2.302261) Accuracy: 0.125000 (0.118952)\n",
      "[40/251] Loss: 2.309366 (2.300467) Accuracy: 0.062500 (0.121189)\n",
      "[50/251] Loss: 2.294367 (2.301071) Accuracy: 0.093750 (0.117647)\n",
      "[60/251] Loss: 2.302349 (2.300924) Accuracy: 0.093750 (0.118340)\n",
      "[70/251] Loss: 2.306754 (2.300123) Accuracy: 0.156250 (0.123239)\n",
      "[80/251] Loss: 2.297166 (2.300753) Accuracy: 0.062500 (0.118441)\n",
      "[90/251] Loss: 2.268334 (2.299292) Accuracy: 0.218750 (0.122596)\n",
      "[100/251] Loss: 2.319797 (2.298812) Accuracy: 0.125000 (0.122834)\n",
      "[110/251] Loss: 2.316237 (2.299302) Accuracy: 0.062500 (0.121059)\n",
      "[120/251] Loss: 2.284491 (2.299256) Accuracy: 0.156250 (0.121384)\n",
      "[130/251] Loss: 2.318683 (2.299438) Accuracy: 0.031250 (0.120706)\n",
      "[140/251] Loss: 2.281358 (2.298989) Accuracy: 0.187500 (0.120124)\n",
      "[150/251] Loss: 2.311427 (2.299335) Accuracy: 0.125000 (0.119412)\n",
      "[160/251] Loss: 2.294963 (2.299304) Accuracy: 0.093750 (0.118401)\n",
      "[170/251] Loss: 2.307002 (2.299452) Accuracy: 0.062500 (0.118787)\n",
      "[180/251] Loss: 2.303690 (2.299313) Accuracy: 0.125000 (0.118612)\n",
      "[190/251] Loss: 2.312828 (2.299653) Accuracy: 0.062500 (0.118128)\n",
      "[200/251] Loss: 2.304074 (2.299359) Accuracy: 0.093750 (0.118315)\n",
      "[210/251] Loss: 2.297245 (2.299536) Accuracy: 0.125000 (0.117150)\n",
      "[220/251] Loss: 2.313263 (2.299297) Accuracy: 0.093750 (0.116657)\n",
      "[230/251] Loss: 2.315807 (2.299458) Accuracy: 0.031250 (0.115936)\n",
      "[240/251] Loss: 2.292871 (2.299764) Accuracy: 0.125000 (0.115145)\n",
      "[250/251] Loss: 2.308227 (2.299855) Accuracy: 0.000000 (0.114915)\n",
      "[0/63] Loss: 2.293848 (2.293848) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.313977 (2.308147) Accuracy: 0.125000 (0.085227)\n",
      "[20/63] Loss: 2.302265 (2.303909) Accuracy: 0.093750 (0.095238)\n",
      "[30/63] Loss: 2.313807 (2.307311) Accuracy: 0.093750 (0.086694)\n",
      "[40/63] Loss: 2.310151 (2.307068) Accuracy: 0.031250 (0.087652)\n",
      "[50/63] Loss: 2.300943 (2.305825) Accuracy: 0.156250 (0.093137)\n",
      "[60/63] Loss: 2.308175 (2.304830) Accuracy: 0.093750 (0.097336)\n",
      "Epoch: 4/100, Train Loss: 2.2999, Train Acc: 0.1149, Val. Loss: 2.3048, Val. Acc: 0.0973\n",
      "[0/251] Loss: 2.317868 (2.317868) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.313486 (2.301073) Accuracy: 0.093750 (0.119318)\n",
      "[20/251] Loss: 2.295498 (2.303799) Accuracy: 0.125000 (0.119048)\n",
      "[30/251] Loss: 2.283913 (2.300891) Accuracy: 0.125000 (0.116935)\n",
      "[40/251] Loss: 2.295621 (2.300564) Accuracy: 0.187500 (0.119665)\n",
      "[50/251] Loss: 2.289290 (2.299035) Accuracy: 0.156250 (0.123162)\n",
      "[60/251] Loss: 2.305270 (2.299382) Accuracy: 0.062500 (0.122439)\n",
      "[70/251] Loss: 2.323786 (2.299078) Accuracy: 0.093750 (0.119718)\n",
      "[80/251] Loss: 2.315848 (2.298835) Accuracy: 0.062500 (0.121914)\n",
      "[90/251] Loss: 2.308460 (2.298956) Accuracy: 0.031250 (0.118475)\n",
      "[100/251] Loss: 2.309557 (2.299189) Accuracy: 0.062500 (0.116955)\n",
      "[110/251] Loss: 2.317122 (2.298771) Accuracy: 0.093750 (0.118525)\n",
      "[120/251] Loss: 2.303923 (2.298333) Accuracy: 0.093750 (0.120093)\n",
      "[130/251] Loss: 2.289293 (2.297994) Accuracy: 0.218750 (0.121660)\n",
      "[140/251] Loss: 2.296386 (2.297973) Accuracy: 0.031250 (0.120346)\n",
      "[150/251] Loss: 2.300137 (2.297792) Accuracy: 0.125000 (0.121482)\n",
      "[160/251] Loss: 2.285519 (2.297805) Accuracy: 0.156250 (0.121506)\n",
      "[170/251] Loss: 2.299363 (2.297917) Accuracy: 0.093750 (0.121162)\n",
      "[180/251] Loss: 2.302459 (2.297951) Accuracy: 0.156250 (0.121202)\n",
      "[190/251] Loss: 2.322842 (2.298006) Accuracy: 0.093750 (0.120746)\n",
      "[200/251] Loss: 2.334548 (2.298629) Accuracy: 0.031250 (0.120336)\n",
      "[210/251] Loss: 2.306683 (2.298700) Accuracy: 0.156250 (0.121001)\n",
      "[220/251] Loss: 2.301709 (2.298282) Accuracy: 0.187500 (0.122738)\n",
      "[230/251] Loss: 2.307786 (2.298446) Accuracy: 0.125000 (0.122159)\n",
      "[240/251] Loss: 2.303599 (2.298738) Accuracy: 0.125000 (0.121629)\n",
      "[250/251] Loss: 2.340156 (2.298663) Accuracy: 0.000000 (0.121887)\n",
      "[0/63] Loss: 2.305233 (2.305233) Accuracy: 0.000000 (0.000000)\n",
      "[10/63] Loss: 2.308091 (2.305436) Accuracy: 0.093750 (0.079545)\n",
      "[20/63] Loss: 2.296141 (2.303928) Accuracy: 0.093750 (0.098214)\n",
      "[30/63] Loss: 2.303002 (2.305972) Accuracy: 0.093750 (0.089718)\n",
      "[40/63] Loss: 2.310670 (2.305167) Accuracy: 0.031250 (0.096799)\n",
      "[50/63] Loss: 2.299211 (2.304417) Accuracy: 0.093750 (0.102328)\n",
      "[60/63] Loss: 2.294580 (2.303642) Accuracy: 0.125000 (0.105020)\n",
      "Epoch: 5/100, Train Loss: 2.2987, Train Acc: 0.1219, Val. Loss: 2.3029, Val. Acc: 0.1068\n",
      "[0/251] Loss: 2.292961 (2.292961) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.324283 (2.296675) Accuracy: 0.093750 (0.113636)\n",
      "[20/251] Loss: 2.324193 (2.294334) Accuracy: 0.062500 (0.132440)\n",
      "[30/251] Loss: 2.254057 (2.294254) Accuracy: 0.250000 (0.139113)\n",
      "[40/251] Loss: 2.284189 (2.293924) Accuracy: 0.156250 (0.142530)\n",
      "[50/251] Loss: 2.302152 (2.294844) Accuracy: 0.125000 (0.139093)\n",
      "[60/251] Loss: 2.327770 (2.296234) Accuracy: 0.031250 (0.132172)\n",
      "[70/251] Loss: 2.326994 (2.297327) Accuracy: 0.062500 (0.128521)\n",
      "[80/251] Loss: 2.307631 (2.296824) Accuracy: 0.093750 (0.128472)\n",
      "[90/251] Loss: 2.318943 (2.297081) Accuracy: 0.062500 (0.125687)\n",
      "[100/251] Loss: 2.303789 (2.297835) Accuracy: 0.062500 (0.121597)\n",
      "[110/251] Loss: 2.294005 (2.298114) Accuracy: 0.093750 (0.119651)\n",
      "[120/251] Loss: 2.307467 (2.297494) Accuracy: 0.156250 (0.123450)\n",
      "[130/251] Loss: 2.297640 (2.296956) Accuracy: 0.093750 (0.125716)\n",
      "[140/251] Loss: 2.247232 (2.297173) Accuracy: 0.218750 (0.125887)\n",
      "[150/251] Loss: 2.317223 (2.296895) Accuracy: 0.031250 (0.125621)\n",
      "[160/251] Loss: 2.287936 (2.297087) Accuracy: 0.218750 (0.125582)\n",
      "[170/251] Loss: 2.314726 (2.297076) Accuracy: 0.062500 (0.125183)\n",
      "[180/251] Loss: 2.309212 (2.297105) Accuracy: 0.093750 (0.125691)\n",
      "[190/251] Loss: 2.325491 (2.297288) Accuracy: 0.093750 (0.126145)\n",
      "[200/251] Loss: 2.281037 (2.297153) Accuracy: 0.125000 (0.127332)\n",
      "[210/251] Loss: 2.293952 (2.296788) Accuracy: 0.125000 (0.128258)\n",
      "[220/251] Loss: 2.293041 (2.296714) Accuracy: 0.156250 (0.128252)\n",
      "[230/251] Loss: 2.289615 (2.296646) Accuracy: 0.125000 (0.128111)\n",
      "[240/251] Loss: 2.295789 (2.296879) Accuracy: 0.187500 (0.127982)\n",
      "[250/251] Loss: 2.359789 (2.297042) Accuracy: 0.000000 (0.127241)\n",
      "[0/63] Loss: 2.301677 (2.301677) Accuracy: 0.031250 (0.031250)\n",
      "[10/63] Loss: 2.313198 (2.302784) Accuracy: 0.093750 (0.105114)\n",
      "[20/63] Loss: 2.291187 (2.301235) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.299499 (2.303415) Accuracy: 0.156250 (0.105847)\n",
      "[40/63] Loss: 2.307425 (2.303294) Accuracy: 0.062500 (0.110518)\n",
      "[50/63] Loss: 2.279069 (2.301810) Accuracy: 0.156250 (0.113971)\n",
      "[60/63] Loss: 2.289752 (2.301509) Accuracy: 0.187500 (0.117316)\n",
      "Epoch: 6/100, Train Loss: 2.2970, Train Acc: 0.1272, Val. Loss: 2.3014, Val. Acc: 0.1192\n",
      "[0/251] Loss: 2.297146 (2.297146) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.296025 (2.293897) Accuracy: 0.125000 (0.119318)\n",
      "[20/251] Loss: 2.306890 (2.292812) Accuracy: 0.125000 (0.136905)\n",
      "[30/251] Loss: 2.281022 (2.291300) Accuracy: 0.156250 (0.137097)\n",
      "[40/251] Loss: 2.311019 (2.293059) Accuracy: 0.125000 (0.132622)\n",
      "[50/251] Loss: 2.284969 (2.291421) Accuracy: 0.187500 (0.137868)\n",
      "[60/251] Loss: 2.300880 (2.291765) Accuracy: 0.125000 (0.138832)\n",
      "[70/251] Loss: 2.321854 (2.292227) Accuracy: 0.093750 (0.138644)\n",
      "[80/251] Loss: 2.300608 (2.292820) Accuracy: 0.125000 (0.136960)\n",
      "[90/251] Loss: 2.308131 (2.293061) Accuracy: 0.062500 (0.134272)\n",
      "[100/251] Loss: 2.315551 (2.293249) Accuracy: 0.031250 (0.133663)\n",
      "[110/251] Loss: 2.314346 (2.293719) Accuracy: 0.125000 (0.134572)\n",
      "[120/251] Loss: 2.275115 (2.293683) Accuracy: 0.093750 (0.133523)\n",
      "[130/251] Loss: 2.307431 (2.293994) Accuracy: 0.125000 (0.132872)\n",
      "[140/251] Loss: 2.290240 (2.294231) Accuracy: 0.125000 (0.132979)\n",
      "[150/251] Loss: 2.291214 (2.294287) Accuracy: 0.093750 (0.133692)\n",
      "[160/251] Loss: 2.312979 (2.294214) Accuracy: 0.062500 (0.133346)\n",
      "[170/251] Loss: 2.254395 (2.294325) Accuracy: 0.218750 (0.133041)\n",
      "[180/251] Loss: 2.301989 (2.294358) Accuracy: 0.125000 (0.132597)\n",
      "[190/251] Loss: 2.296034 (2.294852) Accuracy: 0.093750 (0.131217)\n",
      "[200/251] Loss: 2.294115 (2.294744) Accuracy: 0.093750 (0.131530)\n",
      "[210/251] Loss: 2.297428 (2.294435) Accuracy: 0.125000 (0.132257)\n",
      "[220/251] Loss: 2.307165 (2.294607) Accuracy: 0.093750 (0.130656)\n",
      "[230/251] Loss: 2.306669 (2.294656) Accuracy: 0.062500 (0.130952)\n",
      "[240/251] Loss: 2.329938 (2.295071) Accuracy: 0.031250 (0.129927)\n",
      "[250/251] Loss: 2.342756 (2.295620) Accuracy: 0.000000 (0.128860)\n",
      "[0/63] Loss: 2.300768 (2.300768) Accuracy: 0.062500 (0.062500)\n",
      "[10/63] Loss: 2.307700 (2.300544) Accuracy: 0.093750 (0.110795)\n",
      "[20/63] Loss: 2.289451 (2.299773) Accuracy: 0.156250 (0.116071)\n",
      "[30/63] Loss: 2.293041 (2.302915) Accuracy: 0.156250 (0.106855)\n",
      "[40/63] Loss: 2.299352 (2.302511) Accuracy: 0.093750 (0.108994)\n",
      "[50/63] Loss: 2.291875 (2.301649) Accuracy: 0.156250 (0.115196)\n",
      "[60/63] Loss: 2.290160 (2.301388) Accuracy: 0.125000 (0.117828)\n",
      "Epoch: 7/100, Train Loss: 2.2956, Train Acc: 0.1289, Val. Loss: 2.3011, Val. Acc: 0.1212\n",
      "[0/251] Loss: 2.275816 (2.275816) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.275940 (2.281309) Accuracy: 0.250000 (0.178977)\n",
      "[20/251] Loss: 2.266542 (2.285108) Accuracy: 0.156250 (0.153274)\n",
      "[30/251] Loss: 2.301290 (2.286554) Accuracy: 0.125000 (0.152218)\n",
      "[40/251] Loss: 2.310058 (2.289443) Accuracy: 0.125000 (0.138720)\n",
      "[50/251] Loss: 2.271380 (2.289692) Accuracy: 0.156250 (0.139706)\n",
      "[60/251] Loss: 2.272083 (2.289641) Accuracy: 0.218750 (0.140881)\n",
      "[70/251] Loss: 2.299773 (2.289521) Accuracy: 0.093750 (0.142165)\n",
      "[80/251] Loss: 2.298045 (2.290827) Accuracy: 0.125000 (0.140046)\n",
      "[90/251] Loss: 2.316728 (2.292437) Accuracy: 0.062500 (0.136676)\n",
      "[100/251] Loss: 2.284897 (2.292859) Accuracy: 0.125000 (0.135520)\n",
      "[110/251] Loss: 2.318508 (2.293754) Accuracy: 0.062500 (0.133727)\n",
      "[120/251] Loss: 2.308348 (2.294655) Accuracy: 0.062500 (0.129907)\n",
      "[130/251] Loss: 2.275381 (2.294799) Accuracy: 0.218750 (0.131679)\n",
      "[140/251] Loss: 2.281088 (2.294657) Accuracy: 0.187500 (0.133644)\n",
      "[150/251] Loss: 2.289729 (2.294171) Accuracy: 0.125000 (0.134934)\n",
      "[160/251] Loss: 2.279431 (2.294047) Accuracy: 0.187500 (0.135093)\n",
      "[170/251] Loss: 2.291952 (2.294187) Accuracy: 0.125000 (0.134503)\n",
      "[180/251] Loss: 2.295490 (2.294108) Accuracy: 0.125000 (0.133978)\n",
      "[190/251] Loss: 2.280416 (2.293976) Accuracy: 0.125000 (0.133835)\n",
      "[200/251] Loss: 2.264785 (2.293652) Accuracy: 0.187500 (0.133706)\n",
      "[210/251] Loss: 2.301100 (2.294063) Accuracy: 0.125000 (0.132701)\n",
      "[220/251] Loss: 2.280427 (2.293906) Accuracy: 0.156250 (0.132777)\n",
      "[230/251] Loss: 2.331562 (2.293621) Accuracy: 0.125000 (0.133793)\n",
      "[240/251] Loss: 2.341061 (2.293625) Accuracy: 0.031250 (0.134206)\n",
      "[250/251] Loss: 2.279294 (2.293942) Accuracy: 0.000000 (0.132346)\n",
      "[0/63] Loss: 2.298716 (2.298716) Accuracy: 0.062500 (0.062500)\n",
      "[10/63] Loss: 2.318052 (2.304441) Accuracy: 0.062500 (0.122159)\n",
      "[20/63] Loss: 2.288742 (2.301468) Accuracy: 0.093750 (0.122024)\n",
      "[30/63] Loss: 2.288620 (2.305373) Accuracy: 0.156250 (0.105847)\n",
      "[40/63] Loss: 2.295760 (2.304701) Accuracy: 0.093750 (0.106707)\n",
      "[50/63] Loss: 2.278554 (2.303276) Accuracy: 0.187500 (0.110294)\n",
      "[60/63] Loss: 2.297163 (2.302905) Accuracy: 0.187500 (0.112193)\n",
      "Epoch: 8/100, Train Loss: 2.2939, Train Acc: 0.1323, Val. Loss: 2.3029, Val. Acc: 0.1122\n",
      "[0/251] Loss: 2.279676 (2.279676) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.319151 (2.287893) Accuracy: 0.093750 (0.150568)\n",
      "[20/251] Loss: 2.275513 (2.286098) Accuracy: 0.187500 (0.153274)\n",
      "[30/251] Loss: 2.330860 (2.288467) Accuracy: 0.062500 (0.148185)\n",
      "[40/251] Loss: 2.313055 (2.290834) Accuracy: 0.093750 (0.142530)\n",
      "[50/251] Loss: 2.314461 (2.292218) Accuracy: 0.093750 (0.137868)\n",
      "[60/251] Loss: 2.328210 (2.293418) Accuracy: 0.031250 (0.130635)\n",
      "[70/251] Loss: 2.292201 (2.292945) Accuracy: 0.156250 (0.130722)\n",
      "[80/251] Loss: 2.285766 (2.292976) Accuracy: 0.156250 (0.131944)\n",
      "[90/251] Loss: 2.296254 (2.292197) Accuracy: 0.187500 (0.136676)\n",
      "[100/251] Loss: 2.298149 (2.293496) Accuracy: 0.093750 (0.132735)\n",
      "[110/251] Loss: 2.270798 (2.292979) Accuracy: 0.156250 (0.134009)\n",
      "[120/251] Loss: 2.297640 (2.293161) Accuracy: 0.187500 (0.135331)\n",
      "[130/251] Loss: 2.309726 (2.292943) Accuracy: 0.125000 (0.136212)\n",
      "[140/251] Loss: 2.312657 (2.292949) Accuracy: 0.062500 (0.134973)\n",
      "[150/251] Loss: 2.287145 (2.293172) Accuracy: 0.187500 (0.134934)\n",
      "[160/251] Loss: 2.278289 (2.293258) Accuracy: 0.250000 (0.136064)\n",
      "[170/251] Loss: 2.314366 (2.293278) Accuracy: 0.156250 (0.135599)\n",
      "[180/251] Loss: 2.289708 (2.292995) Accuracy: 0.156250 (0.134841)\n",
      "[190/251] Loss: 2.279983 (2.292853) Accuracy: 0.187500 (0.134326)\n",
      "[200/251] Loss: 2.330324 (2.293024) Accuracy: 0.062500 (0.134328)\n",
      "[210/251] Loss: 2.268693 (2.292568) Accuracy: 0.250000 (0.135960)\n",
      "[220/251] Loss: 2.304037 (2.292872) Accuracy: 0.156250 (0.135747)\n",
      "[230/251] Loss: 2.318561 (2.293094) Accuracy: 0.062500 (0.135552)\n",
      "[240/251] Loss: 2.301494 (2.292848) Accuracy: 0.125000 (0.135892)\n",
      "[250/251] Loss: 2.246345 (2.292774) Accuracy: 0.000000 (0.134711)\n",
      "[0/63] Loss: 2.289494 (2.289494) Accuracy: 0.093750 (0.093750)\n",
      "[10/63] Loss: 2.316598 (2.301215) Accuracy: 0.125000 (0.122159)\n",
      "[20/63] Loss: 2.286091 (2.300120) Accuracy: 0.062500 (0.117560)\n",
      "[30/63] Loss: 2.282895 (2.303461) Accuracy: 0.156250 (0.104839)\n",
      "[40/63] Loss: 2.290306 (2.301853) Accuracy: 0.093750 (0.109756)\n",
      "[50/63] Loss: 2.290150 (2.300440) Accuracy: 0.156250 (0.117034)\n",
      "[60/63] Loss: 2.296349 (2.301194) Accuracy: 0.156250 (0.115266)\n",
      "Epoch: 9/100, Train Loss: 2.2928, Train Acc: 0.1347, Val. Loss: 2.3012, Val. Acc: 0.1142\n",
      "[0/251] Loss: 2.317402 (2.317402) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.297601 (2.294232) Accuracy: 0.187500 (0.144886)\n",
      "[20/251] Loss: 2.314903 (2.291398) Accuracy: 0.125000 (0.154762)\n",
      "[30/251] Loss: 2.280131 (2.290010) Accuracy: 0.125000 (0.152218)\n",
      "[40/251] Loss: 2.261339 (2.291797) Accuracy: 0.218750 (0.144055)\n",
      "[50/251] Loss: 2.302169 (2.291053) Accuracy: 0.031250 (0.142157)\n",
      "[60/251] Loss: 2.282971 (2.291392) Accuracy: 0.156250 (0.142418)\n",
      "[70/251] Loss: 2.258851 (2.291161) Accuracy: 0.250000 (0.145246)\n",
      "[80/251] Loss: 2.265066 (2.290086) Accuracy: 0.218750 (0.148148)\n",
      "[90/251] Loss: 2.302095 (2.290568) Accuracy: 0.156250 (0.146978)\n",
      "[100/251] Loss: 2.305150 (2.290965) Accuracy: 0.031250 (0.143874)\n",
      "[110/251] Loss: 2.303694 (2.291090) Accuracy: 0.093750 (0.143863)\n",
      "[120/251] Loss: 2.289127 (2.291860) Accuracy: 0.156250 (0.141529)\n",
      "[130/251] Loss: 2.301102 (2.292610) Accuracy: 0.156250 (0.138597)\n",
      "[140/251] Loss: 2.282049 (2.292657) Accuracy: 0.156250 (0.137411)\n",
      "[150/251] Loss: 2.280825 (2.292682) Accuracy: 0.156250 (0.137210)\n",
      "[160/251] Loss: 2.329279 (2.292378) Accuracy: 0.000000 (0.138005)\n",
      "[170/251] Loss: 2.278775 (2.291771) Accuracy: 0.125000 (0.139985)\n",
      "[180/251] Loss: 2.285545 (2.291814) Accuracy: 0.156250 (0.138812)\n",
      "[190/251] Loss: 2.285820 (2.291579) Accuracy: 0.187500 (0.139071)\n",
      "[200/251] Loss: 2.265950 (2.291661) Accuracy: 0.218750 (0.138993)\n",
      "[210/251] Loss: 2.305693 (2.291321) Accuracy: 0.093750 (0.140995)\n",
      "[220/251] Loss: 2.273774 (2.291023) Accuracy: 0.250000 (0.141827)\n",
      "[230/251] Loss: 2.301060 (2.291207) Accuracy: 0.125000 (0.141504)\n",
      "[240/251] Loss: 2.281863 (2.291697) Accuracy: 0.156250 (0.140041)\n",
      "[250/251] Loss: 2.347301 (2.292195) Accuracy: 0.000000 (0.138695)\n",
      "[0/63] Loss: 2.305459 (2.305459) Accuracy: 0.062500 (0.062500)\n",
      "[10/63] Loss: 2.312190 (2.302806) Accuracy: 0.093750 (0.099432)\n",
      "[20/63] Loss: 2.282131 (2.301173) Accuracy: 0.125000 (0.108631)\n",
      "[30/63] Loss: 2.292116 (2.303698) Accuracy: 0.156250 (0.105847)\n",
      "[40/63] Loss: 2.296935 (2.303337) Accuracy: 0.093750 (0.109756)\n",
      "[50/63] Loss: 2.279654 (2.301325) Accuracy: 0.187500 (0.121936)\n",
      "[60/63] Loss: 2.289350 (2.301548) Accuracy: 0.156250 (0.120902)\n",
      "Epoch: 10/100, Train Loss: 2.2922, Train Acc: 0.1387, Val. Loss: 2.3013, Val. Acc: 0.1227\n",
      "[0/251] Loss: 2.269832 (2.269832) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.289355 (2.273989) Accuracy: 0.125000 (0.196023)\n",
      "[20/251] Loss: 2.301724 (2.282204) Accuracy: 0.156250 (0.177083)\n",
      "[30/251] Loss: 2.305952 (2.285990) Accuracy: 0.062500 (0.162298)\n",
      "[40/251] Loss: 2.299209 (2.288033) Accuracy: 0.093750 (0.155488)\n",
      "[50/251] Loss: 2.235604 (2.287746) Accuracy: 0.312500 (0.156250)\n",
      "[60/251] Loss: 2.280555 (2.288381) Accuracy: 0.093750 (0.152664)\n",
      "[70/251] Loss: 2.280735 (2.288342) Accuracy: 0.187500 (0.148327)\n",
      "[80/251] Loss: 2.307496 (2.289361) Accuracy: 0.125000 (0.144290)\n",
      "[90/251] Loss: 2.297104 (2.289780) Accuracy: 0.093750 (0.142514)\n",
      "[100/251] Loss: 2.278111 (2.290164) Accuracy: 0.218750 (0.142636)\n",
      "[110/251] Loss: 2.278585 (2.290465) Accuracy: 0.187500 (0.144144)\n",
      "[120/251] Loss: 2.284523 (2.289813) Accuracy: 0.125000 (0.145145)\n",
      "[130/251] Loss: 2.276284 (2.290086) Accuracy: 0.250000 (0.144084)\n",
      "[140/251] Loss: 2.301637 (2.290207) Accuracy: 0.062500 (0.142952)\n",
      "[150/251] Loss: 2.299966 (2.289577) Accuracy: 0.093750 (0.144247)\n",
      "[160/251] Loss: 2.318012 (2.289841) Accuracy: 0.125000 (0.143245)\n",
      "[170/251] Loss: 2.285008 (2.289650) Accuracy: 0.156250 (0.143823)\n",
      "[180/251] Loss: 2.275597 (2.289637) Accuracy: 0.156250 (0.144337)\n",
      "[190/251] Loss: 2.326880 (2.289855) Accuracy: 0.062500 (0.144143)\n",
      "[200/251] Loss: 2.313762 (2.290211) Accuracy: 0.062500 (0.142724)\n",
      "[210/251] Loss: 2.246511 (2.290390) Accuracy: 0.281250 (0.142032)\n",
      "[220/251] Loss: 2.302982 (2.290728) Accuracy: 0.156250 (0.141120)\n",
      "[230/251] Loss: 2.275233 (2.290710) Accuracy: 0.156250 (0.141640)\n",
      "[240/251] Loss: 2.258334 (2.290717) Accuracy: 0.156250 (0.140949)\n",
      "[250/251] Loss: 2.408036 (2.291466) Accuracy: 0.000000 (0.138695)\n",
      "[0/63] Loss: 2.295983 (2.295983) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.313700 (2.302913) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.280714 (2.300001) Accuracy: 0.093750 (0.122024)\n",
      "[30/63] Loss: 2.284400 (2.304115) Accuracy: 0.187500 (0.107863)\n",
      "[40/63] Loss: 2.288199 (2.303385) Accuracy: 0.125000 (0.111280)\n",
      "[50/63] Loss: 2.281529 (2.301618) Accuracy: 0.218750 (0.118873)\n",
      "[60/63] Loss: 2.293366 (2.301733) Accuracy: 0.156250 (0.120902)\n",
      "Epoch: 11/100, Train Loss: 2.2915, Train Acc: 0.1387, Val. Loss: 2.3015, Val. Acc: 0.1222\n",
      "[0/251] Loss: 2.291604 (2.291604) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.287402 (2.289829) Accuracy: 0.156250 (0.136364)\n",
      "[20/251] Loss: 2.263736 (2.291196) Accuracy: 0.218750 (0.132440)\n",
      "[30/251] Loss: 2.322194 (2.293027) Accuracy: 0.125000 (0.133065)\n",
      "[40/251] Loss: 2.292030 (2.295064) Accuracy: 0.156250 (0.133384)\n",
      "[50/251] Loss: 2.274455 (2.293374) Accuracy: 0.218750 (0.140319)\n",
      "[60/251] Loss: 2.293603 (2.293323) Accuracy: 0.156250 (0.141393)\n",
      "[70/251] Loss: 2.302044 (2.292701) Accuracy: 0.093750 (0.143046)\n",
      "[80/251] Loss: 2.290756 (2.291287) Accuracy: 0.093750 (0.145833)\n",
      "[90/251] Loss: 2.286495 (2.290057) Accuracy: 0.156250 (0.149725)\n",
      "[100/251] Loss: 2.304597 (2.290472) Accuracy: 0.125000 (0.147587)\n",
      "[110/251] Loss: 2.306231 (2.290666) Accuracy: 0.125000 (0.144989)\n",
      "[120/251] Loss: 2.298270 (2.290893) Accuracy: 0.125000 (0.145661)\n",
      "[130/251] Loss: 2.233697 (2.290624) Accuracy: 0.343750 (0.145992)\n",
      "[140/251] Loss: 2.259940 (2.290020) Accuracy: 0.156250 (0.145833)\n",
      "[150/251] Loss: 2.300138 (2.290455) Accuracy: 0.093750 (0.144661)\n",
      "[160/251] Loss: 2.282656 (2.290673) Accuracy: 0.187500 (0.144022)\n",
      "[170/251] Loss: 2.278910 (2.290590) Accuracy: 0.156250 (0.144189)\n",
      "[180/251] Loss: 2.297672 (2.291016) Accuracy: 0.125000 (0.142093)\n",
      "[190/251] Loss: 2.252495 (2.291000) Accuracy: 0.218750 (0.141034)\n",
      "[200/251] Loss: 2.267772 (2.290375) Accuracy: 0.156250 (0.142413)\n",
      "[210/251] Loss: 2.294222 (2.290223) Accuracy: 0.062500 (0.143069)\n",
      "[220/251] Loss: 2.269072 (2.289850) Accuracy: 0.187500 (0.143100)\n",
      "[230/251] Loss: 2.289746 (2.289898) Accuracy: 0.093750 (0.142451)\n",
      "[240/251] Loss: 2.293422 (2.290070) Accuracy: 0.093750 (0.141727)\n",
      "[250/251] Loss: 2.127383 (2.289366) Accuracy: 1.000000 (0.145045)\n",
      "[0/63] Loss: 2.305208 (2.305208) Accuracy: 0.062500 (0.062500)\n",
      "[10/63] Loss: 2.312389 (2.301130) Accuracy: 0.156250 (0.133523)\n",
      "[20/63] Loss: 2.283162 (2.300423) Accuracy: 0.156250 (0.117560)\n",
      "[30/63] Loss: 2.283173 (2.304495) Accuracy: 0.218750 (0.104839)\n",
      "[40/63] Loss: 2.289625 (2.303333) Accuracy: 0.093750 (0.109756)\n",
      "[50/63] Loss: 2.276701 (2.301492) Accuracy: 0.187500 (0.113971)\n",
      "[60/63] Loss: 2.294965 (2.301781) Accuracy: 0.187500 (0.117316)\n",
      "Epoch: 12/100, Train Loss: 2.2894, Train Acc: 0.1450, Val. Loss: 2.3016, Val. Acc: 0.1187\n",
      "[0/251] Loss: 2.227323 (2.227323) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.272451 (2.287236) Accuracy: 0.156250 (0.161932)\n",
      "[20/251] Loss: 2.277190 (2.286826) Accuracy: 0.218750 (0.154762)\n",
      "[30/251] Loss: 2.297739 (2.287651) Accuracy: 0.093750 (0.156250)\n",
      "[40/251] Loss: 2.268727 (2.288475) Accuracy: 0.187500 (0.150915)\n",
      "[50/251] Loss: 2.310730 (2.288848) Accuracy: 0.062500 (0.148897)\n",
      "[60/251] Loss: 2.312626 (2.289225) Accuracy: 0.093750 (0.149078)\n",
      "[70/251] Loss: 2.286725 (2.286952) Accuracy: 0.187500 (0.154489)\n",
      "[80/251] Loss: 2.302958 (2.286564) Accuracy: 0.093750 (0.153549)\n",
      "[90/251] Loss: 2.268502 (2.285811) Accuracy: 0.156250 (0.153159)\n",
      "[100/251] Loss: 2.303737 (2.287228) Accuracy: 0.093750 (0.150681)\n",
      "[110/251] Loss: 2.316030 (2.286806) Accuracy: 0.093750 (0.151464)\n",
      "[120/251] Loss: 2.269715 (2.286866) Accuracy: 0.187500 (0.151085)\n",
      "[130/251] Loss: 2.291219 (2.286877) Accuracy: 0.156250 (0.150525)\n",
      "[140/251] Loss: 2.299639 (2.287512) Accuracy: 0.093750 (0.147606)\n",
      "[150/251] Loss: 2.309525 (2.288303) Accuracy: 0.156250 (0.146316)\n",
      "[160/251] Loss: 2.283216 (2.288402) Accuracy: 0.250000 (0.146739)\n",
      "[170/251] Loss: 2.283553 (2.288690) Accuracy: 0.125000 (0.145651)\n",
      "[180/251] Loss: 2.290027 (2.288639) Accuracy: 0.187500 (0.145891)\n",
      "[190/251] Loss: 2.314372 (2.288073) Accuracy: 0.093750 (0.147906)\n",
      "[200/251] Loss: 2.257908 (2.287839) Accuracy: 0.187500 (0.148165)\n",
      "[210/251] Loss: 2.284442 (2.287965) Accuracy: 0.093750 (0.147660)\n",
      "[220/251] Loss: 2.286181 (2.288358) Accuracy: 0.093750 (0.146635)\n",
      "[230/251] Loss: 2.278611 (2.288350) Accuracy: 0.093750 (0.145969)\n",
      "[240/251] Loss: 2.285527 (2.288991) Accuracy: 0.156250 (0.144450)\n",
      "[250/251] Loss: 1.693608 (2.286865) Accuracy: 1.000000 (0.147659)\n",
      "[0/63] Loss: 2.303381 (2.303381) Accuracy: 0.062500 (0.062500)\n",
      "[10/63] Loss: 2.313263 (2.305006) Accuracy: 0.125000 (0.105114)\n",
      "[20/63] Loss: 2.264953 (2.300157) Accuracy: 0.187500 (0.119048)\n",
      "[30/63] Loss: 2.281949 (2.304601) Accuracy: 0.156250 (0.105847)\n",
      "[40/63] Loss: 2.285483 (2.303926) Accuracy: 0.187500 (0.109756)\n",
      "[50/63] Loss: 2.287809 (2.302038) Accuracy: 0.156250 (0.115809)\n",
      "[60/63] Loss: 2.296545 (2.302167) Accuracy: 0.187500 (0.117316)\n",
      "Epoch: 13/100, Train Loss: 2.2869, Train Acc: 0.1477, Val. Loss: 2.3018, Val. Acc: 0.1192\n",
      "[0/251] Loss: 2.290938 (2.290938) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.277429 (2.287219) Accuracy: 0.156250 (0.156250)\n",
      "[20/251] Loss: 2.302946 (2.290715) Accuracy: 0.125000 (0.153274)\n",
      "[30/251] Loss: 2.238536 (2.290100) Accuracy: 0.218750 (0.148185)\n",
      "[40/251] Loss: 2.290715 (2.289507) Accuracy: 0.125000 (0.144055)\n",
      "[50/251] Loss: 2.292344 (2.287426) Accuracy: 0.156250 (0.151348)\n",
      "[60/251] Loss: 2.242080 (2.288266) Accuracy: 0.187500 (0.148053)\n",
      "[70/251] Loss: 2.265939 (2.288679) Accuracy: 0.187500 (0.145687)\n",
      "[80/251] Loss: 2.303424 (2.287325) Accuracy: 0.031250 (0.149691)\n",
      "[90/251] Loss: 2.315426 (2.289071) Accuracy: 0.093750 (0.146635)\n",
      "[100/251] Loss: 2.236960 (2.288120) Accuracy: 0.250000 (0.146349)\n",
      "[110/251] Loss: 2.243770 (2.288711) Accuracy: 0.281250 (0.145270)\n",
      "[120/251] Loss: 2.276234 (2.289366) Accuracy: 0.156250 (0.143079)\n",
      "[130/251] Loss: 2.315006 (2.289308) Accuracy: 0.093750 (0.142891)\n",
      "[140/251] Loss: 2.264893 (2.288326) Accuracy: 0.156250 (0.146498)\n",
      "[150/251] Loss: 2.323208 (2.288175) Accuracy: 0.062500 (0.145281)\n",
      "[160/251] Loss: 2.270664 (2.288506) Accuracy: 0.187500 (0.143439)\n",
      "[170/251] Loss: 2.245446 (2.287783) Accuracy: 0.281250 (0.144554)\n",
      "[180/251] Loss: 2.264131 (2.288162) Accuracy: 0.125000 (0.142783)\n",
      "[190/251] Loss: 2.327285 (2.288147) Accuracy: 0.062500 (0.142997)\n",
      "[200/251] Loss: 2.260440 (2.287852) Accuracy: 0.250000 (0.143812)\n",
      "[210/251] Loss: 2.296203 (2.287920) Accuracy: 0.187500 (0.144254)\n",
      "[220/251] Loss: 2.303396 (2.288086) Accuracy: 0.093750 (0.143948)\n",
      "[230/251] Loss: 2.324517 (2.288496) Accuracy: 0.125000 (0.144751)\n",
      "[240/251] Loss: 2.311110 (2.288859) Accuracy: 0.156250 (0.143932)\n",
      "[250/251] Loss: 2.373295 (2.288866) Accuracy: 0.000000 (0.144173)\n",
      "[0/63] Loss: 2.300858 (2.300858) Accuracy: 0.093750 (0.093750)\n",
      "[10/63] Loss: 2.314035 (2.303158) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.275641 (2.299978) Accuracy: 0.156250 (0.125000)\n",
      "[30/63] Loss: 2.281575 (2.304448) Accuracy: 0.156250 (0.107863)\n",
      "[40/63] Loss: 2.283222 (2.303481) Accuracy: 0.156250 (0.109756)\n",
      "[50/63] Loss: 2.284194 (2.301338) Accuracy: 0.187500 (0.118873)\n",
      "[60/63] Loss: 2.294944 (2.301608) Accuracy: 0.187500 (0.120902)\n",
      "Epoch: 14/100, Train Loss: 2.2889, Train Acc: 0.1442, Val. Loss: 2.3015, Val. Acc: 0.1227\n",
      "[0/251] Loss: 2.292847 (2.292847) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.249044 (2.283599) Accuracy: 0.218750 (0.156250)\n",
      "[20/251] Loss: 2.283634 (2.282510) Accuracy: 0.156250 (0.156250)\n",
      "[30/251] Loss: 2.308620 (2.287138) Accuracy: 0.093750 (0.143145)\n",
      "[40/251] Loss: 2.329462 (2.285231) Accuracy: 0.062500 (0.150152)\n",
      "[50/251] Loss: 2.309070 (2.284920) Accuracy: 0.125000 (0.151961)\n",
      "[60/251] Loss: 2.301872 (2.283750) Accuracy: 0.062500 (0.153176)\n",
      "[70/251] Loss: 2.302615 (2.285786) Accuracy: 0.125000 (0.148768)\n",
      "[80/251] Loss: 2.276633 (2.286286) Accuracy: 0.187500 (0.148534)\n",
      "[90/251] Loss: 2.257304 (2.287300) Accuracy: 0.218750 (0.147321)\n",
      "[100/251] Loss: 2.290521 (2.287689) Accuracy: 0.125000 (0.146349)\n",
      "[110/251] Loss: 2.238754 (2.287259) Accuracy: 0.312500 (0.147241)\n",
      "[120/251] Loss: 2.307227 (2.287085) Accuracy: 0.093750 (0.146694)\n",
      "[130/251] Loss: 2.260918 (2.287300) Accuracy: 0.218750 (0.147662)\n",
      "[140/251] Loss: 2.288446 (2.287176) Accuracy: 0.218750 (0.147828)\n",
      "[150/251] Loss: 2.256229 (2.287787) Accuracy: 0.250000 (0.146109)\n",
      "[160/251] Loss: 2.314542 (2.287443) Accuracy: 0.093750 (0.146739)\n",
      "[170/251] Loss: 2.273972 (2.287201) Accuracy: 0.281250 (0.147478)\n",
      "[180/251] Loss: 2.303326 (2.288057) Accuracy: 0.062500 (0.144682)\n",
      "[190/251] Loss: 2.269281 (2.287807) Accuracy: 0.187500 (0.145452)\n",
      "[200/251] Loss: 2.306169 (2.288018) Accuracy: 0.125000 (0.145678)\n",
      "[210/251] Loss: 2.288842 (2.288015) Accuracy: 0.093750 (0.145735)\n",
      "[220/251] Loss: 2.261156 (2.288049) Accuracy: 0.125000 (0.145079)\n",
      "[230/251] Loss: 2.263117 (2.287722) Accuracy: 0.281250 (0.145833)\n",
      "[240/251] Loss: 2.263153 (2.288182) Accuracy: 0.250000 (0.145488)\n",
      "[250/251] Loss: 2.133785 (2.287190) Accuracy: 0.000000 (0.145418)\n",
      "[0/63] Loss: 2.304204 (2.304204) Accuracy: 0.062500 (0.062500)\n",
      "[10/63] Loss: 2.313064 (2.301618) Accuracy: 0.156250 (0.122159)\n",
      "[20/63] Loss: 2.277329 (2.300425) Accuracy: 0.156250 (0.116071)\n",
      "[30/63] Loss: 2.282268 (2.304809) Accuracy: 0.187500 (0.105847)\n",
      "[40/63] Loss: 2.281358 (2.303893) Accuracy: 0.125000 (0.108232)\n",
      "[50/63] Loss: 2.283124 (2.301877) Accuracy: 0.187500 (0.115809)\n",
      "[60/63] Loss: 2.295405 (2.302297) Accuracy: 0.187500 (0.119877)\n",
      "Epoch: 15/100, Train Loss: 2.2872, Train Acc: 0.1454, Val. Loss: 2.3021, Val. Acc: 0.1212\n",
      "[0/251] Loss: 2.289017 (2.289017) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.272675 (2.290530) Accuracy: 0.187500 (0.142045)\n",
      "[20/251] Loss: 2.283847 (2.289413) Accuracy: 0.156250 (0.145833)\n",
      "[30/251] Loss: 2.289921 (2.288378) Accuracy: 0.156250 (0.144153)\n",
      "[40/251] Loss: 2.287133 (2.288653) Accuracy: 0.125000 (0.143293)\n",
      "[50/251] Loss: 2.297013 (2.289625) Accuracy: 0.093750 (0.139706)\n",
      "[60/251] Loss: 2.255879 (2.289318) Accuracy: 0.218750 (0.139857)\n",
      "[70/251] Loss: 2.276745 (2.288418) Accuracy: 0.187500 (0.142165)\n",
      "[80/251] Loss: 2.306144 (2.288914) Accuracy: 0.093750 (0.141975)\n",
      "[90/251] Loss: 2.272575 (2.288419) Accuracy: 0.156250 (0.143887)\n",
      "[100/251] Loss: 2.280819 (2.288085) Accuracy: 0.187500 (0.146349)\n",
      "[110/251] Loss: 2.289309 (2.287372) Accuracy: 0.125000 (0.149775)\n",
      "[120/251] Loss: 2.253628 (2.287564) Accuracy: 0.250000 (0.150052)\n",
      "[130/251] Loss: 2.286198 (2.287287) Accuracy: 0.218750 (0.151240)\n",
      "[140/251] Loss: 2.303096 (2.287582) Accuracy: 0.062500 (0.149601)\n",
      "[150/251] Loss: 2.282475 (2.287594) Accuracy: 0.187500 (0.150248)\n",
      "[160/251] Loss: 2.259809 (2.287650) Accuracy: 0.187500 (0.149651)\n",
      "[170/251] Loss: 2.275558 (2.288235) Accuracy: 0.218750 (0.148209)\n",
      "[180/251] Loss: 2.247293 (2.288343) Accuracy: 0.250000 (0.147790)\n",
      "[190/251] Loss: 2.276192 (2.288433) Accuracy: 0.125000 (0.147088)\n",
      "[200/251] Loss: 2.307727 (2.288866) Accuracy: 0.093750 (0.145833)\n",
      "[210/251] Loss: 2.284466 (2.288080) Accuracy: 0.218750 (0.148549)\n",
      "[220/251] Loss: 2.301175 (2.287858) Accuracy: 0.156250 (0.149180)\n",
      "[230/251] Loss: 2.277021 (2.287528) Accuracy: 0.187500 (0.149756)\n",
      "[240/251] Loss: 2.322267 (2.287116) Accuracy: 0.000000 (0.149507)\n",
      "[250/251] Loss: 1.969201 (2.285918) Accuracy: 1.000000 (0.152639)\n",
      "[0/63] Loss: 2.297814 (2.297814) Accuracy: 0.062500 (0.062500)\n",
      "[10/63] Loss: 2.312748 (2.300725) Accuracy: 0.156250 (0.122159)\n",
      "[20/63] Loss: 2.277349 (2.299675) Accuracy: 0.156250 (0.119048)\n",
      "[30/63] Loss: 2.277252 (2.304916) Accuracy: 0.187500 (0.102823)\n",
      "[40/63] Loss: 2.278292 (2.303544) Accuracy: 0.156250 (0.105183)\n",
      "[50/63] Loss: 2.281571 (2.301684) Accuracy: 0.218750 (0.113358)\n",
      "[60/63] Loss: 2.292059 (2.302363) Accuracy: 0.187500 (0.117316)\n",
      "Epoch: 16/100, Train Loss: 2.2859, Train Acc: 0.1526, Val. Loss: 2.3022, Val. Acc: 0.1166\n",
      "[0/251] Loss: 2.304020 (2.304020) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.295165 (2.289202) Accuracy: 0.093750 (0.133523)\n",
      "[20/251] Loss: 2.277855 (2.288295) Accuracy: 0.156250 (0.144345)\n",
      "[30/251] Loss: 2.285741 (2.288472) Accuracy: 0.125000 (0.150202)\n",
      "[40/251] Loss: 2.274105 (2.287842) Accuracy: 0.156250 (0.152439)\n",
      "[50/251] Loss: 2.275519 (2.287342) Accuracy: 0.250000 (0.151961)\n",
      "[60/251] Loss: 2.228443 (2.286119) Accuracy: 0.156250 (0.149590)\n",
      "[70/251] Loss: 2.309559 (2.286255) Accuracy: 0.125000 (0.150088)\n",
      "[80/251] Loss: 2.280508 (2.286129) Accuracy: 0.156250 (0.148148)\n",
      "[90/251] Loss: 2.285708 (2.285464) Accuracy: 0.156250 (0.149038)\n",
      "[100/251] Loss: 2.271291 (2.286197) Accuracy: 0.125000 (0.147587)\n",
      "[110/251] Loss: 2.294873 (2.285577) Accuracy: 0.125000 (0.150619)\n",
      "[120/251] Loss: 2.287796 (2.285498) Accuracy: 0.187500 (0.150826)\n",
      "[130/251] Loss: 2.276672 (2.285599) Accuracy: 0.156250 (0.149809)\n",
      "[140/251] Loss: 2.302603 (2.285916) Accuracy: 0.125000 (0.149158)\n",
      "[150/251] Loss: 2.276266 (2.286068) Accuracy: 0.156250 (0.149421)\n",
      "[160/251] Loss: 2.291394 (2.286409) Accuracy: 0.187500 (0.149845)\n",
      "[170/251] Loss: 2.266028 (2.286465) Accuracy: 0.187500 (0.150768)\n",
      "[180/251] Loss: 2.331765 (2.286432) Accuracy: 0.031250 (0.150207)\n",
      "[190/251] Loss: 2.223933 (2.286446) Accuracy: 0.312500 (0.150360)\n",
      "[200/251] Loss: 2.286886 (2.286577) Accuracy: 0.156250 (0.150031)\n",
      "[210/251] Loss: 2.285061 (2.286362) Accuracy: 0.125000 (0.150770)\n",
      "[220/251] Loss: 2.295224 (2.286560) Accuracy: 0.187500 (0.151018)\n",
      "[230/251] Loss: 2.298451 (2.286361) Accuracy: 0.093750 (0.150974)\n",
      "[240/251] Loss: 2.295309 (2.286736) Accuracy: 0.125000 (0.150415)\n",
      "[250/251] Loss: 2.383901 (2.286912) Accuracy: 0.000000 (0.150149)\n",
      "[0/63] Loss: 2.300955 (2.300955) Accuracy: 0.093750 (0.093750)\n",
      "[10/63] Loss: 2.310957 (2.301645) Accuracy: 0.156250 (0.125000)\n",
      "[20/63] Loss: 2.276093 (2.300460) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.281337 (2.305336) Accuracy: 0.187500 (0.104839)\n",
      "[40/63] Loss: 2.281036 (2.304164) Accuracy: 0.156250 (0.109756)\n",
      "[50/63] Loss: 2.278351 (2.302038) Accuracy: 0.187500 (0.116422)\n",
      "[60/63] Loss: 2.290646 (2.302588) Accuracy: 0.187500 (0.116803)\n",
      "Epoch: 17/100, Train Loss: 2.2869, Train Acc: 0.1501, Val. Loss: 2.3024, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.317513 (2.317513) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.323865 (2.280002) Accuracy: 0.062500 (0.156250)\n",
      "[20/251] Loss: 2.314075 (2.281499) Accuracy: 0.156250 (0.160714)\n",
      "[30/251] Loss: 2.290203 (2.286151) Accuracy: 0.187500 (0.157258)\n",
      "[40/251] Loss: 2.270439 (2.287849) Accuracy: 0.093750 (0.149390)\n",
      "[50/251] Loss: 2.287985 (2.284273) Accuracy: 0.187500 (0.156863)\n",
      "[60/251] Loss: 2.302021 (2.283550) Accuracy: 0.093750 (0.157275)\n",
      "[70/251] Loss: 2.285726 (2.283475) Accuracy: 0.093750 (0.152729)\n",
      "[80/251] Loss: 2.303196 (2.281865) Accuracy: 0.218750 (0.158179)\n",
      "[90/251] Loss: 2.278338 (2.283110) Accuracy: 0.156250 (0.153503)\n",
      "[100/251] Loss: 2.310847 (2.284059) Accuracy: 0.062500 (0.153775)\n",
      "[110/251] Loss: 2.279726 (2.284785) Accuracy: 0.187500 (0.152872)\n",
      "[120/251] Loss: 2.272305 (2.284793) Accuracy: 0.156250 (0.152634)\n",
      "[130/251] Loss: 2.259883 (2.284566) Accuracy: 0.281250 (0.153626)\n",
      "[140/251] Loss: 2.263255 (2.284998) Accuracy: 0.187500 (0.152926)\n",
      "[150/251] Loss: 2.270639 (2.285088) Accuracy: 0.218750 (0.153974)\n",
      "[160/251] Loss: 2.220733 (2.284090) Accuracy: 0.375000 (0.157026)\n",
      "[170/251] Loss: 2.262107 (2.284334) Accuracy: 0.218750 (0.155885)\n",
      "[180/251] Loss: 2.292002 (2.284695) Accuracy: 0.156250 (0.154869)\n",
      "[190/251] Loss: 2.277552 (2.285176) Accuracy: 0.125000 (0.153796)\n",
      "[200/251] Loss: 2.268854 (2.284676) Accuracy: 0.125000 (0.154695)\n",
      "[210/251] Loss: 2.307657 (2.284724) Accuracy: 0.125000 (0.154917)\n",
      "[220/251] Loss: 2.299958 (2.284677) Accuracy: 0.156250 (0.156250)\n",
      "[230/251] Loss: 2.236800 (2.284771) Accuracy: 0.281250 (0.156250)\n",
      "[240/251] Loss: 2.281358 (2.285298) Accuracy: 0.156250 (0.154694)\n",
      "[250/251] Loss: 2.375262 (2.286396) Accuracy: 0.000000 (0.152017)\n",
      "[0/63] Loss: 2.299848 (2.299848) Accuracy: 0.062500 (0.062500)\n",
      "[10/63] Loss: 2.310519 (2.301237) Accuracy: 0.156250 (0.125000)\n",
      "[20/63] Loss: 2.275743 (2.299895) Accuracy: 0.125000 (0.122024)\n",
      "[30/63] Loss: 2.280544 (2.304889) Accuracy: 0.156250 (0.105847)\n",
      "[40/63] Loss: 2.278188 (2.303445) Accuracy: 0.156250 (0.110518)\n",
      "[50/63] Loss: 2.282867 (2.301344) Accuracy: 0.187500 (0.117034)\n",
      "[60/63] Loss: 2.292795 (2.302134) Accuracy: 0.187500 (0.117316)\n",
      "Epoch: 18/100, Train Loss: 2.2864, Train Acc: 0.1520, Val. Loss: 2.3020, Val. Acc: 0.1187\n",
      "[0/251] Loss: 2.317071 (2.317071) Accuracy: 0.031250 (0.031250)\n",
      "[10/251] Loss: 2.258217 (2.280194) Accuracy: 0.250000 (0.156250)\n",
      "[20/251] Loss: 2.287810 (2.282455) Accuracy: 0.187500 (0.157738)\n",
      "[30/251] Loss: 2.261784 (2.280054) Accuracy: 0.250000 (0.158266)\n",
      "[40/251] Loss: 2.276344 (2.282447) Accuracy: 0.156250 (0.159299)\n",
      "[50/251] Loss: 2.310104 (2.283787) Accuracy: 0.093750 (0.159314)\n",
      "[60/251] Loss: 2.267053 (2.282110) Accuracy: 0.218750 (0.163934)\n",
      "[70/251] Loss: 2.282624 (2.281684) Accuracy: 0.156250 (0.164173)\n",
      "[80/251] Loss: 2.263568 (2.281574) Accuracy: 0.218750 (0.164738)\n",
      "[90/251] Loss: 2.287035 (2.283389) Accuracy: 0.093750 (0.157967)\n",
      "[100/251] Loss: 2.298162 (2.283511) Accuracy: 0.125000 (0.157488)\n",
      "[110/251] Loss: 2.295409 (2.283263) Accuracy: 0.218750 (0.157939)\n",
      "[120/251] Loss: 2.284623 (2.283259) Accuracy: 0.156250 (0.157541)\n",
      "[130/251] Loss: 2.257341 (2.282164) Accuracy: 0.218750 (0.161498)\n",
      "[140/251] Loss: 2.261849 (2.282595) Accuracy: 0.218750 (0.161569)\n",
      "[150/251] Loss: 2.279831 (2.283008) Accuracy: 0.156250 (0.160182)\n",
      "[160/251] Loss: 2.296839 (2.283732) Accuracy: 0.218750 (0.158967)\n",
      "[170/251] Loss: 2.297899 (2.284399) Accuracy: 0.156250 (0.157529)\n",
      "[180/251] Loss: 2.288224 (2.284928) Accuracy: 0.187500 (0.155732)\n",
      "[190/251] Loss: 2.270113 (2.284830) Accuracy: 0.187500 (0.155759)\n",
      "[200/251] Loss: 2.265399 (2.285083) Accuracy: 0.218750 (0.155006)\n",
      "[210/251] Loss: 2.311164 (2.284912) Accuracy: 0.125000 (0.154177)\n",
      "[220/251] Loss: 2.289777 (2.285085) Accuracy: 0.125000 (0.153422)\n",
      "[230/251] Loss: 2.298482 (2.285160) Accuracy: 0.156250 (0.153680)\n",
      "[240/251] Loss: 2.262826 (2.285680) Accuracy: 0.187500 (0.151841)\n",
      "[250/251] Loss: 2.346453 (2.285700) Accuracy: 0.000000 (0.151270)\n",
      "[0/63] Loss: 2.298948 (2.298948) Accuracy: 0.093750 (0.093750)\n",
      "[10/63] Loss: 2.312834 (2.302909) Accuracy: 0.125000 (0.119318)\n",
      "[20/63] Loss: 2.272475 (2.300373) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.280531 (2.305488) Accuracy: 0.156250 (0.103831)\n",
      "[40/63] Loss: 2.280584 (2.304207) Accuracy: 0.125000 (0.106707)\n",
      "[50/63] Loss: 2.282553 (2.301889) Accuracy: 0.187500 (0.113358)\n",
      "[60/63] Loss: 2.297933 (2.302493) Accuracy: 0.187500 (0.116803)\n",
      "Epoch: 19/100, Train Loss: 2.2857, Train Acc: 0.1513, Val. Loss: 2.3024, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.305314 (2.305314) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.265264 (2.285300) Accuracy: 0.187500 (0.161932)\n",
      "[20/251] Loss: 2.270950 (2.282956) Accuracy: 0.218750 (0.165179)\n",
      "[30/251] Loss: 2.275027 (2.283642) Accuracy: 0.250000 (0.157258)\n",
      "[40/251] Loss: 2.285974 (2.284993) Accuracy: 0.125000 (0.157774)\n",
      "[50/251] Loss: 2.289332 (2.285095) Accuracy: 0.125000 (0.153799)\n",
      "[60/251] Loss: 2.311043 (2.286181) Accuracy: 0.125000 (0.153176)\n",
      "[70/251] Loss: 2.259647 (2.287034) Accuracy: 0.187500 (0.151408)\n",
      "[80/251] Loss: 2.322493 (2.286927) Accuracy: 0.093750 (0.152778)\n",
      "[90/251] Loss: 2.319558 (2.287851) Accuracy: 0.156250 (0.150412)\n",
      "[100/251] Loss: 2.268500 (2.286183) Accuracy: 0.156250 (0.152847)\n",
      "[110/251] Loss: 2.312961 (2.284743) Accuracy: 0.062500 (0.153153)\n",
      "[120/251] Loss: 2.291715 (2.283628) Accuracy: 0.125000 (0.157283)\n",
      "[130/251] Loss: 2.304842 (2.284347) Accuracy: 0.093750 (0.155296)\n",
      "[140/251] Loss: 2.285520 (2.284851) Accuracy: 0.156250 (0.153590)\n",
      "[150/251] Loss: 2.256208 (2.284779) Accuracy: 0.281250 (0.153767)\n",
      "[160/251] Loss: 2.323832 (2.285833) Accuracy: 0.062500 (0.150815)\n",
      "[170/251] Loss: 2.281690 (2.286064) Accuracy: 0.156250 (0.149671)\n",
      "[180/251] Loss: 2.272911 (2.285468) Accuracy: 0.187500 (0.150725)\n",
      "[190/251] Loss: 2.291376 (2.285974) Accuracy: 0.156250 (0.149705)\n",
      "[200/251] Loss: 2.240547 (2.285319) Accuracy: 0.218750 (0.151119)\n",
      "[210/251] Loss: 2.302002 (2.285156) Accuracy: 0.062500 (0.152695)\n",
      "[220/251] Loss: 2.273182 (2.284940) Accuracy: 0.156250 (0.152856)\n",
      "[230/251] Loss: 2.267927 (2.285128) Accuracy: 0.218750 (0.153003)\n",
      "[240/251] Loss: 2.323297 (2.285170) Accuracy: 0.062500 (0.152360)\n",
      "[250/251] Loss: 2.142174 (2.284553) Accuracy: 1.000000 (0.156250)\n",
      "[0/63] Loss: 2.298849 (2.298849) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.313407 (2.303143) Accuracy: 0.156250 (0.122159)\n",
      "[20/63] Loss: 2.276291 (2.301283) Accuracy: 0.093750 (0.116071)\n",
      "[30/63] Loss: 2.280612 (2.305947) Accuracy: 0.156250 (0.108871)\n",
      "[40/63] Loss: 2.278720 (2.304314) Accuracy: 0.156250 (0.110518)\n",
      "[50/63] Loss: 2.281587 (2.301808) Accuracy: 0.187500 (0.116422)\n",
      "[60/63] Loss: 2.296177 (2.302504) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 20/100, Train Loss: 2.2846, Train Acc: 0.1562, Val. Loss: 2.3024, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.286126 (2.286126) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.288455 (2.286378) Accuracy: 0.156250 (0.159091)\n",
      "[20/251] Loss: 2.266556 (2.285372) Accuracy: 0.218750 (0.156250)\n",
      "[30/251] Loss: 2.265757 (2.281137) Accuracy: 0.218750 (0.163306)\n",
      "[40/251] Loss: 2.271706 (2.279625) Accuracy: 0.187500 (0.164634)\n",
      "[50/251] Loss: 2.299310 (2.282163) Accuracy: 0.125000 (0.158088)\n",
      "[60/251] Loss: 2.283910 (2.282936) Accuracy: 0.218750 (0.161885)\n",
      "[70/251] Loss: 2.304420 (2.282733) Accuracy: 0.062500 (0.159771)\n",
      "[80/251] Loss: 2.313724 (2.285017) Accuracy: 0.062500 (0.153549)\n",
      "[90/251] Loss: 2.306608 (2.286775) Accuracy: 0.093750 (0.148352)\n",
      "[100/251] Loss: 2.300822 (2.287693) Accuracy: 0.125000 (0.146040)\n",
      "[110/251] Loss: 2.292232 (2.286959) Accuracy: 0.156250 (0.147523)\n",
      "[120/251] Loss: 2.291330 (2.287191) Accuracy: 0.125000 (0.145919)\n",
      "[130/251] Loss: 2.292468 (2.286623) Accuracy: 0.125000 (0.147901)\n",
      "[140/251] Loss: 2.304900 (2.286139) Accuracy: 0.093750 (0.148936)\n",
      "[150/251] Loss: 2.274082 (2.285549) Accuracy: 0.187500 (0.152111)\n",
      "[160/251] Loss: 2.256901 (2.284923) Accuracy: 0.281250 (0.153921)\n",
      "[170/251] Loss: 2.298023 (2.284902) Accuracy: 0.062500 (0.154423)\n",
      "[180/251] Loss: 2.268618 (2.285330) Accuracy: 0.250000 (0.153660)\n",
      "[190/251] Loss: 2.307979 (2.284888) Accuracy: 0.062500 (0.154450)\n",
      "[200/251] Loss: 2.259045 (2.284807) Accuracy: 0.218750 (0.154540)\n",
      "[210/251] Loss: 2.318832 (2.285246) Accuracy: 0.062500 (0.154177)\n",
      "[220/251] Loss: 2.303493 (2.285228) Accuracy: 0.125000 (0.153705)\n",
      "[230/251] Loss: 2.274501 (2.285064) Accuracy: 0.187500 (0.154356)\n",
      "[240/251] Loss: 2.298266 (2.284978) Accuracy: 0.125000 (0.154175)\n",
      "[250/251] Loss: 2.262282 (2.284527) Accuracy: 0.000000 (0.154507)\n",
      "[0/63] Loss: 2.301136 (2.301136) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312117 (2.302363) Accuracy: 0.156250 (0.125000)\n",
      "[20/63] Loss: 2.275131 (2.300730) Accuracy: 0.125000 (0.122024)\n",
      "[30/63] Loss: 2.280516 (2.305461) Accuracy: 0.156250 (0.109879)\n",
      "[40/63] Loss: 2.278161 (2.303910) Accuracy: 0.187500 (0.112043)\n",
      "[50/63] Loss: 2.282652 (2.301456) Accuracy: 0.187500 (0.118260)\n",
      "[60/63] Loss: 2.295891 (2.302248) Accuracy: 0.187500 (0.119365)\n",
      "Epoch: 21/100, Train Loss: 2.2845, Train Acc: 0.1545, Val. Loss: 2.3021, Val. Acc: 0.1202\n",
      "[0/251] Loss: 2.303611 (2.303611) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.285156 (2.288073) Accuracy: 0.156250 (0.136364)\n",
      "[20/251] Loss: 2.274494 (2.288636) Accuracy: 0.187500 (0.139881)\n",
      "[30/251] Loss: 2.278243 (2.286043) Accuracy: 0.187500 (0.140121)\n",
      "[40/251] Loss: 2.277822 (2.283220) Accuracy: 0.187500 (0.152439)\n",
      "[50/251] Loss: 2.309962 (2.282495) Accuracy: 0.062500 (0.158701)\n",
      "[60/251] Loss: 2.274614 (2.282246) Accuracy: 0.093750 (0.156250)\n",
      "[70/251] Loss: 2.259223 (2.282615) Accuracy: 0.187500 (0.154930)\n",
      "[80/251] Loss: 2.271470 (2.283424) Accuracy: 0.156250 (0.155864)\n",
      "[90/251] Loss: 2.242151 (2.282760) Accuracy: 0.281250 (0.158310)\n",
      "[100/251] Loss: 2.299277 (2.282937) Accuracy: 0.062500 (0.157488)\n",
      "[110/251] Loss: 2.294725 (2.283612) Accuracy: 0.093750 (0.154561)\n",
      "[120/251] Loss: 2.321557 (2.283924) Accuracy: 0.093750 (0.155217)\n",
      "[130/251] Loss: 2.278924 (2.283575) Accuracy: 0.156250 (0.155773)\n",
      "[140/251] Loss: 2.276949 (2.284157) Accuracy: 0.187500 (0.155142)\n",
      "[150/251] Loss: 2.290762 (2.283571) Accuracy: 0.156250 (0.156250)\n",
      "[160/251] Loss: 2.311847 (2.283304) Accuracy: 0.125000 (0.157803)\n",
      "[170/251] Loss: 2.270829 (2.283438) Accuracy: 0.125000 (0.158808)\n",
      "[180/251] Loss: 2.301235 (2.283870) Accuracy: 0.156250 (0.157631)\n",
      "[190/251] Loss: 2.306581 (2.284143) Accuracy: 0.062500 (0.155923)\n",
      "[200/251] Loss: 2.265581 (2.283808) Accuracy: 0.125000 (0.155784)\n",
      "[210/251] Loss: 2.268258 (2.284764) Accuracy: 0.187500 (0.154621)\n",
      "[220/251] Loss: 2.257221 (2.284271) Accuracy: 0.218750 (0.154977)\n",
      "[230/251] Loss: 2.294913 (2.284221) Accuracy: 0.156250 (0.155574)\n",
      "[240/251] Loss: 2.256641 (2.284130) Accuracy: 0.250000 (0.155342)\n",
      "[250/251] Loss: 2.340288 (2.284468) Accuracy: 0.000000 (0.154507)\n",
      "[0/63] Loss: 2.299843 (2.299843) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312447 (2.302507) Accuracy: 0.156250 (0.119318)\n",
      "[20/63] Loss: 2.271960 (2.300401) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.279694 (2.305400) Accuracy: 0.156250 (0.103831)\n",
      "[40/63] Loss: 2.277181 (2.303816) Accuracy: 0.156250 (0.105945)\n",
      "[50/63] Loss: 2.285496 (2.301415) Accuracy: 0.187500 (0.113358)\n",
      "[60/63] Loss: 2.298310 (2.302289) Accuracy: 0.187500 (0.114754)\n",
      "Epoch: 22/100, Train Loss: 2.2845, Train Acc: 0.1545, Val. Loss: 2.3022, Val. Acc: 0.1152\n",
      "[0/251] Loss: 2.260443 (2.260443) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.245952 (2.281607) Accuracy: 0.218750 (0.167614)\n",
      "[20/251] Loss: 2.322530 (2.289154) Accuracy: 0.062500 (0.145833)\n",
      "[30/251] Loss: 2.214335 (2.286136) Accuracy: 0.281250 (0.147177)\n",
      "[40/251] Loss: 2.306691 (2.288573) Accuracy: 0.093750 (0.144055)\n",
      "[50/251] Loss: 2.292348 (2.288433) Accuracy: 0.187500 (0.150123)\n",
      "[60/251] Loss: 2.251182 (2.287379) Accuracy: 0.281250 (0.153176)\n",
      "[70/251] Loss: 2.271134 (2.286740) Accuracy: 0.156250 (0.151849)\n",
      "[80/251] Loss: 2.291946 (2.286875) Accuracy: 0.156250 (0.153549)\n",
      "[90/251] Loss: 2.282981 (2.286222) Accuracy: 0.156250 (0.155563)\n",
      "[100/251] Loss: 2.287690 (2.285240) Accuracy: 0.156250 (0.158106)\n",
      "[110/251] Loss: 2.280730 (2.285201) Accuracy: 0.218750 (0.159628)\n",
      "[120/251] Loss: 2.248632 (2.284807) Accuracy: 0.281250 (0.161157)\n",
      "[130/251] Loss: 2.292616 (2.284008) Accuracy: 0.156250 (0.162452)\n",
      "[140/251] Loss: 2.316985 (2.284482) Accuracy: 0.156250 (0.160904)\n",
      "[150/251] Loss: 2.268769 (2.285064) Accuracy: 0.156250 (0.158940)\n",
      "[160/251] Loss: 2.301893 (2.284263) Accuracy: 0.062500 (0.160326)\n",
      "[170/251] Loss: 2.308264 (2.283495) Accuracy: 0.062500 (0.161550)\n",
      "[180/251] Loss: 2.315435 (2.284457) Accuracy: 0.062500 (0.159358)\n",
      "[190/251] Loss: 2.289755 (2.283650) Accuracy: 0.093750 (0.160340)\n",
      "[200/251] Loss: 2.308976 (2.284044) Accuracy: 0.062500 (0.159204)\n",
      "[210/251] Loss: 2.286668 (2.283972) Accuracy: 0.156250 (0.158916)\n",
      "[220/251] Loss: 2.280008 (2.283533) Accuracy: 0.187500 (0.159785)\n",
      "[230/251] Loss: 2.288248 (2.283876) Accuracy: 0.125000 (0.158144)\n",
      "[240/251] Loss: 2.251004 (2.283563) Accuracy: 0.250000 (0.158584)\n",
      "[250/251] Loss: 2.416078 (2.284415) Accuracy: 0.000000 (0.157246)\n",
      "[0/63] Loss: 2.301970 (2.301970) Accuracy: 0.093750 (0.093750)\n",
      "[10/63] Loss: 2.312888 (2.302934) Accuracy: 0.125000 (0.116477)\n",
      "[20/63] Loss: 2.273176 (2.300709) Accuracy: 0.093750 (0.116071)\n",
      "[30/63] Loss: 2.280441 (2.305854) Accuracy: 0.156250 (0.103831)\n",
      "[40/63] Loss: 2.277497 (2.304299) Accuracy: 0.156250 (0.108232)\n",
      "[50/63] Loss: 2.284580 (2.301842) Accuracy: 0.187500 (0.115809)\n",
      "[60/63] Loss: 2.299694 (2.302538) Accuracy: 0.187500 (0.118852)\n",
      "Epoch: 23/100, Train Loss: 2.2844, Train Acc: 0.1572, Val. Loss: 2.3024, Val. Acc: 0.1197\n",
      "[0/251] Loss: 2.266691 (2.266691) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.299903 (2.288089) Accuracy: 0.062500 (0.139205)\n",
      "[20/251] Loss: 2.299753 (2.289615) Accuracy: 0.093750 (0.144345)\n",
      "[30/251] Loss: 2.299553 (2.289521) Accuracy: 0.093750 (0.148185)\n",
      "[40/251] Loss: 2.261532 (2.289006) Accuracy: 0.187500 (0.146341)\n",
      "[50/251] Loss: 2.285602 (2.287850) Accuracy: 0.125000 (0.150123)\n",
      "[60/251] Loss: 2.237723 (2.284338) Accuracy: 0.312500 (0.158299)\n",
      "[70/251] Loss: 2.327738 (2.284962) Accuracy: 0.031250 (0.155810)\n",
      "[80/251] Loss: 2.244100 (2.283431) Accuracy: 0.281250 (0.158951)\n",
      "[90/251] Loss: 2.269029 (2.284465) Accuracy: 0.187500 (0.158310)\n",
      "[100/251] Loss: 2.298271 (2.284502) Accuracy: 0.062500 (0.157797)\n",
      "[110/251] Loss: 2.301840 (2.284211) Accuracy: 0.125000 (0.156813)\n",
      "[120/251] Loss: 2.280524 (2.283633) Accuracy: 0.187500 (0.156508)\n",
      "[130/251] Loss: 2.313244 (2.283093) Accuracy: 0.156250 (0.158635)\n",
      "[140/251] Loss: 2.291845 (2.282752) Accuracy: 0.125000 (0.158688)\n",
      "[150/251] Loss: 2.309262 (2.283419) Accuracy: 0.125000 (0.156250)\n",
      "[160/251] Loss: 2.252797 (2.283700) Accuracy: 0.218750 (0.155085)\n",
      "[170/251] Loss: 2.266939 (2.284033) Accuracy: 0.187500 (0.155885)\n",
      "[180/251] Loss: 2.327921 (2.284157) Accuracy: 0.031250 (0.155214)\n",
      "[190/251] Loss: 2.287239 (2.283699) Accuracy: 0.125000 (0.156741)\n",
      "[200/251] Loss: 2.267521 (2.283177) Accuracy: 0.156250 (0.157805)\n",
      "[210/251] Loss: 2.299164 (2.283580) Accuracy: 0.125000 (0.157139)\n",
      "[220/251] Loss: 2.302211 (2.283359) Accuracy: 0.093750 (0.157240)\n",
      "[230/251] Loss: 2.301922 (2.283700) Accuracy: 0.156250 (0.156115)\n",
      "[240/251] Loss: 2.266586 (2.283839) Accuracy: 0.187500 (0.156120)\n",
      "[250/251] Loss: 2.237144 (2.283414) Accuracy: 0.000000 (0.155503)\n",
      "[0/63] Loss: 2.301843 (2.301843) Accuracy: 0.093750 (0.093750)\n",
      "[10/63] Loss: 2.312080 (2.302832) Accuracy: 0.125000 (0.113636)\n",
      "[20/63] Loss: 2.272020 (2.300370) Accuracy: 0.125000 (0.114583)\n",
      "[30/63] Loss: 2.279543 (2.305496) Accuracy: 0.156250 (0.102823)\n",
      "[40/63] Loss: 2.278440 (2.304124) Accuracy: 0.156250 (0.107470)\n",
      "[50/63] Loss: 2.283763 (2.301664) Accuracy: 0.187500 (0.115809)\n",
      "[60/63] Loss: 2.299467 (2.302447) Accuracy: 0.156250 (0.115779)\n",
      "Epoch: 24/100, Train Loss: 2.2834, Train Acc: 0.1555, Val. Loss: 2.3023, Val. Acc: 0.1167\n",
      "[0/251] Loss: 2.303083 (2.303083) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.311288 (2.296379) Accuracy: 0.093750 (0.130682)\n",
      "[20/251] Loss: 2.255961 (2.289909) Accuracy: 0.250000 (0.151786)\n",
      "[30/251] Loss: 2.249532 (2.283980) Accuracy: 0.250000 (0.159274)\n",
      "[40/251] Loss: 2.282946 (2.284543) Accuracy: 0.093750 (0.153963)\n",
      "[50/251] Loss: 2.299699 (2.286521) Accuracy: 0.125000 (0.148897)\n",
      "[60/251] Loss: 2.271311 (2.286154) Accuracy: 0.187500 (0.152152)\n",
      "[70/251] Loss: 2.310012 (2.286853) Accuracy: 0.125000 (0.150528)\n",
      "[80/251] Loss: 2.263909 (2.284850) Accuracy: 0.218750 (0.155864)\n",
      "[90/251] Loss: 2.277092 (2.283515) Accuracy: 0.187500 (0.157280)\n",
      "[100/251] Loss: 2.293108 (2.284153) Accuracy: 0.187500 (0.156869)\n",
      "[110/251] Loss: 2.284014 (2.284588) Accuracy: 0.218750 (0.156250)\n",
      "[120/251] Loss: 2.290704 (2.282917) Accuracy: 0.093750 (0.159866)\n",
      "[130/251] Loss: 2.255653 (2.282638) Accuracy: 0.218750 (0.159590)\n",
      "[140/251] Loss: 2.200875 (2.281411) Accuracy: 0.281250 (0.162677)\n",
      "[150/251] Loss: 2.313264 (2.282465) Accuracy: 0.093750 (0.160389)\n",
      "[160/251] Loss: 2.299917 (2.282610) Accuracy: 0.125000 (0.158385)\n",
      "[170/251] Loss: 2.249823 (2.281858) Accuracy: 0.250000 (0.159905)\n",
      "[180/251] Loss: 2.312393 (2.281926) Accuracy: 0.156250 (0.159012)\n",
      "[190/251] Loss: 2.297586 (2.282151) Accuracy: 0.125000 (0.158704)\n",
      "[200/251] Loss: 2.267604 (2.282663) Accuracy: 0.218750 (0.157805)\n",
      "[210/251] Loss: 2.288373 (2.283057) Accuracy: 0.156250 (0.157583)\n",
      "[220/251] Loss: 2.296726 (2.282889) Accuracy: 0.093750 (0.157523)\n",
      "[230/251] Loss: 2.255429 (2.282644) Accuracy: 0.250000 (0.157873)\n",
      "[240/251] Loss: 2.269534 (2.282862) Accuracy: 0.156250 (0.157676)\n",
      "[250/251] Loss: 2.278591 (2.283253) Accuracy: 0.000000 (0.155503)\n",
      "[0/63] Loss: 2.301406 (2.301406) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.311136 (2.302672) Accuracy: 0.156250 (0.122159)\n",
      "[20/63] Loss: 2.274221 (2.300885) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.281171 (2.306131) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.276190 (2.304526) Accuracy: 0.125000 (0.108232)\n",
      "[50/63] Loss: 2.281653 (2.302002) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.297149 (2.302845) Accuracy: 0.218750 (0.118852)\n",
      "Epoch: 25/100, Train Loss: 2.2833, Train Acc: 0.1555, Val. Loss: 2.3027, Val. Acc: 0.1197\n",
      "[0/251] Loss: 2.244196 (2.244196) Accuracy: 0.281250 (0.281250)\n",
      "[10/251] Loss: 2.298908 (2.290554) Accuracy: 0.093750 (0.161932)\n",
      "[20/251] Loss: 2.316368 (2.290290) Accuracy: 0.156250 (0.159226)\n",
      "[30/251] Loss: 2.283296 (2.287350) Accuracy: 0.093750 (0.153226)\n",
      "[40/251] Loss: 2.325502 (2.285464) Accuracy: 0.062500 (0.153963)\n",
      "[50/251] Loss: 2.276029 (2.286896) Accuracy: 0.125000 (0.148284)\n",
      "[60/251] Loss: 2.245222 (2.285753) Accuracy: 0.218750 (0.150615)\n",
      "[70/251] Loss: 2.281054 (2.285760) Accuracy: 0.125000 (0.152289)\n",
      "[80/251] Loss: 2.218025 (2.282444) Accuracy: 0.281250 (0.159336)\n",
      "[90/251] Loss: 2.323961 (2.283054) Accuracy: 0.125000 (0.157967)\n",
      "[100/251] Loss: 2.300987 (2.282921) Accuracy: 0.125000 (0.157488)\n",
      "[110/251] Loss: 2.228532 (2.282627) Accuracy: 0.312500 (0.157658)\n",
      "[120/251] Loss: 2.316541 (2.283664) Accuracy: 0.156250 (0.155992)\n",
      "[130/251] Loss: 2.240608 (2.283362) Accuracy: 0.218750 (0.156727)\n",
      "[140/251] Loss: 2.305715 (2.283703) Accuracy: 0.093750 (0.156472)\n",
      "[150/251] Loss: 2.268564 (2.283830) Accuracy: 0.218750 (0.155836)\n",
      "[160/251] Loss: 2.282361 (2.284346) Accuracy: 0.125000 (0.154891)\n",
      "[170/251] Loss: 2.273737 (2.283528) Accuracy: 0.187500 (0.156798)\n",
      "[180/251] Loss: 2.278055 (2.284009) Accuracy: 0.156250 (0.155559)\n",
      "[190/251] Loss: 2.308701 (2.283270) Accuracy: 0.125000 (0.156741)\n",
      "[200/251] Loss: 2.294755 (2.283420) Accuracy: 0.125000 (0.157027)\n",
      "[210/251] Loss: 2.285784 (2.283457) Accuracy: 0.187500 (0.156842)\n",
      "[220/251] Loss: 2.303814 (2.283340) Accuracy: 0.125000 (0.157381)\n",
      "[230/251] Loss: 2.318541 (2.283443) Accuracy: 0.062500 (0.157603)\n",
      "[240/251] Loss: 2.250534 (2.283219) Accuracy: 0.343750 (0.158714)\n",
      "[250/251] Loss: 2.392277 (2.283432) Accuracy: 0.000000 (0.158242)\n",
      "[0/63] Loss: 2.300779 (2.300779) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.311462 (2.302685) Accuracy: 0.156250 (0.125000)\n",
      "[20/63] Loss: 2.274016 (2.300913) Accuracy: 0.125000 (0.120536)\n",
      "[30/63] Loss: 2.281294 (2.306282) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274936 (2.304583) Accuracy: 0.125000 (0.108232)\n",
      "[50/63] Loss: 2.282813 (2.302038) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.298745 (2.302928) Accuracy: 0.250000 (0.119365)\n",
      "Epoch: 26/100, Train Loss: 2.2834, Train Acc: 0.1582, Val. Loss: 2.3029, Val. Acc: 0.1202\n",
      "[0/251] Loss: 2.290553 (2.290553) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.325584 (2.302412) Accuracy: 0.093750 (0.133523)\n",
      "[20/251] Loss: 2.288194 (2.293113) Accuracy: 0.187500 (0.147321)\n",
      "[30/251] Loss: 2.301280 (2.289338) Accuracy: 0.156250 (0.147177)\n",
      "[40/251] Loss: 2.303506 (2.288418) Accuracy: 0.062500 (0.153963)\n",
      "[50/251] Loss: 2.328373 (2.286434) Accuracy: 0.031250 (0.154412)\n",
      "[60/251] Loss: 2.328237 (2.286864) Accuracy: 0.062500 (0.154201)\n",
      "[70/251] Loss: 2.235769 (2.285659) Accuracy: 0.187500 (0.156690)\n",
      "[80/251] Loss: 2.234966 (2.284506) Accuracy: 0.218750 (0.156636)\n",
      "[90/251] Loss: 2.239405 (2.283311) Accuracy: 0.250000 (0.159684)\n",
      "[100/251] Loss: 2.258563 (2.283717) Accuracy: 0.250000 (0.157488)\n",
      "[110/251] Loss: 2.295673 (2.283522) Accuracy: 0.156250 (0.158784)\n",
      "[120/251] Loss: 2.272675 (2.282595) Accuracy: 0.187500 (0.158833)\n",
      "[130/251] Loss: 2.294525 (2.282202) Accuracy: 0.125000 (0.160544)\n",
      "[140/251] Loss: 2.320732 (2.282526) Accuracy: 0.093750 (0.160461)\n",
      "[150/251] Loss: 2.315976 (2.282623) Accuracy: 0.062500 (0.160182)\n",
      "[160/251] Loss: 2.324321 (2.283369) Accuracy: 0.031250 (0.157609)\n",
      "[170/251] Loss: 2.341077 (2.283190) Accuracy: 0.093750 (0.159174)\n",
      "[180/251] Loss: 2.272634 (2.283862) Accuracy: 0.156250 (0.157286)\n",
      "[190/251] Loss: 2.258871 (2.283678) Accuracy: 0.218750 (0.157395)\n",
      "[200/251] Loss: 2.296959 (2.284382) Accuracy: 0.125000 (0.155473)\n",
      "[210/251] Loss: 2.320174 (2.283865) Accuracy: 0.125000 (0.156842)\n",
      "[220/251] Loss: 2.277704 (2.283192) Accuracy: 0.125000 (0.157523)\n",
      "[230/251] Loss: 2.295853 (2.283131) Accuracy: 0.093750 (0.157873)\n",
      "[240/251] Loss: 2.308535 (2.283218) Accuracy: 0.093750 (0.158195)\n",
      "[250/251] Loss: 2.389875 (2.283305) Accuracy: 0.000000 (0.158118)\n",
      "[0/63] Loss: 2.300786 (2.300786) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.311895 (2.302832) Accuracy: 0.125000 (0.122159)\n",
      "[20/63] Loss: 2.272755 (2.300736) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.280464 (2.306160) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.275731 (2.304507) Accuracy: 0.125000 (0.108232)\n",
      "[50/63] Loss: 2.284839 (2.302011) Accuracy: 0.187500 (0.116422)\n",
      "[60/63] Loss: 2.299767 (2.302870) Accuracy: 0.187500 (0.116803)\n",
      "Epoch: 27/100, Train Loss: 2.2833, Train Acc: 0.1581, Val. Loss: 2.3027, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.304151 (2.304151) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.319919 (2.273710) Accuracy: 0.093750 (0.176136)\n",
      "[20/251] Loss: 2.299808 (2.280289) Accuracy: 0.093750 (0.163690)\n",
      "[30/251] Loss: 2.282856 (2.277155) Accuracy: 0.093750 (0.173387)\n",
      "[40/251] Loss: 2.282615 (2.278601) Accuracy: 0.218750 (0.167683)\n",
      "[50/251] Loss: 2.276558 (2.279338) Accuracy: 0.187500 (0.162990)\n",
      "[60/251] Loss: 2.273882 (2.280397) Accuracy: 0.093750 (0.159836)\n",
      "[70/251] Loss: 2.288830 (2.280869) Accuracy: 0.125000 (0.160651)\n",
      "[80/251] Loss: 2.272120 (2.283645) Accuracy: 0.218750 (0.154707)\n",
      "[90/251] Loss: 2.308180 (2.281380) Accuracy: 0.125000 (0.162088)\n",
      "[100/251] Loss: 2.294734 (2.280935) Accuracy: 0.187500 (0.162438)\n",
      "[110/251] Loss: 2.293696 (2.280971) Accuracy: 0.218750 (0.162725)\n",
      "[120/251] Loss: 2.268354 (2.281364) Accuracy: 0.156250 (0.161674)\n",
      "[130/251] Loss: 2.327594 (2.281105) Accuracy: 0.031250 (0.161260)\n",
      "[140/251] Loss: 2.262035 (2.280764) Accuracy: 0.218750 (0.162234)\n",
      "[150/251] Loss: 2.223783 (2.280540) Accuracy: 0.312500 (0.163700)\n",
      "[160/251] Loss: 2.265666 (2.280505) Accuracy: 0.156250 (0.165179)\n",
      "[170/251] Loss: 2.304303 (2.280316) Accuracy: 0.125000 (0.166301)\n",
      "[180/251] Loss: 2.284828 (2.281166) Accuracy: 0.156250 (0.163501)\n",
      "[190/251] Loss: 2.293830 (2.281310) Accuracy: 0.125000 (0.162467)\n",
      "[200/251] Loss: 2.329554 (2.281957) Accuracy: 0.031250 (0.160603)\n",
      "[210/251] Loss: 2.304960 (2.282718) Accuracy: 0.093750 (0.158916)\n",
      "[220/251] Loss: 2.334589 (2.282726) Accuracy: 0.062500 (0.158371)\n",
      "[230/251] Loss: 2.280070 (2.281944) Accuracy: 0.218750 (0.159632)\n",
      "[240/251] Loss: 2.296931 (2.282580) Accuracy: 0.062500 (0.158195)\n",
      "[250/251] Loss: 2.340346 (2.282847) Accuracy: 0.000000 (0.157744)\n",
      "[0/63] Loss: 2.300829 (2.300829) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.311910 (2.303026) Accuracy: 0.125000 (0.122159)\n",
      "[20/63] Loss: 2.271669 (2.300770) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.280322 (2.306118) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.276521 (2.304501) Accuracy: 0.125000 (0.108994)\n",
      "[50/63] Loss: 2.285016 (2.301991) Accuracy: 0.187500 (0.117034)\n",
      "[60/63] Loss: 2.299745 (2.302876) Accuracy: 0.187500 (0.117316)\n",
      "Epoch: 28/100, Train Loss: 2.2828, Train Acc: 0.1577, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.239529 (2.239529) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.225778 (2.270637) Accuracy: 0.343750 (0.190341)\n",
      "[20/251] Loss: 2.262325 (2.275342) Accuracy: 0.218750 (0.180060)\n",
      "[30/251] Loss: 2.256632 (2.278297) Accuracy: 0.218750 (0.169355)\n",
      "[40/251] Loss: 2.299684 (2.279127) Accuracy: 0.156250 (0.165396)\n",
      "[50/251] Loss: 2.293801 (2.278614) Accuracy: 0.093750 (0.164216)\n",
      "[60/251] Loss: 2.305709 (2.277425) Accuracy: 0.093750 (0.165984)\n",
      "[70/251] Loss: 2.245450 (2.276645) Accuracy: 0.250000 (0.167254)\n",
      "[80/251] Loss: 2.312876 (2.278243) Accuracy: 0.125000 (0.163194)\n",
      "[90/251] Loss: 2.279119 (2.280136) Accuracy: 0.156250 (0.161401)\n",
      "[100/251] Loss: 2.279866 (2.281008) Accuracy: 0.187500 (0.159344)\n",
      "[110/251] Loss: 2.275565 (2.281987) Accuracy: 0.156250 (0.155968)\n",
      "[120/251] Loss: 2.268262 (2.280993) Accuracy: 0.187500 (0.159866)\n",
      "[130/251] Loss: 2.280009 (2.281006) Accuracy: 0.187500 (0.160067)\n",
      "[140/251] Loss: 2.261815 (2.281629) Accuracy: 0.218750 (0.158910)\n",
      "[150/251] Loss: 2.237388 (2.281115) Accuracy: 0.281250 (0.159561)\n",
      "[160/251] Loss: 2.275892 (2.281225) Accuracy: 0.156250 (0.159744)\n",
      "[170/251] Loss: 2.258332 (2.280818) Accuracy: 0.218750 (0.161001)\n",
      "[180/251] Loss: 2.239872 (2.281267) Accuracy: 0.281250 (0.160739)\n",
      "[190/251] Loss: 2.305463 (2.281818) Accuracy: 0.093750 (0.159522)\n",
      "[200/251] Loss: 2.235571 (2.281478) Accuracy: 0.281250 (0.160137)\n",
      "[210/251] Loss: 2.301687 (2.282496) Accuracy: 0.093750 (0.157583)\n",
      "[220/251] Loss: 2.282363 (2.282950) Accuracy: 0.156250 (0.157098)\n",
      "[230/251] Loss: 2.287781 (2.282800) Accuracy: 0.156250 (0.157332)\n",
      "[240/251] Loss: 2.251598 (2.282238) Accuracy: 0.250000 (0.158973)\n",
      "[250/251] Loss: 2.322221 (2.282580) Accuracy: 0.000000 (0.157744)\n",
      "[0/63] Loss: 2.301151 (2.301151) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312050 (2.302750) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271725 (2.300557) Accuracy: 0.125000 (0.120536)\n",
      "[30/63] Loss: 2.279699 (2.305945) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.276169 (2.304340) Accuracy: 0.125000 (0.108232)\n",
      "[50/63] Loss: 2.284694 (2.301807) Accuracy: 0.187500 (0.116422)\n",
      "[60/63] Loss: 2.299717 (2.302718) Accuracy: 0.187500 (0.116291)\n",
      "Epoch: 29/100, Train Loss: 2.2826, Train Acc: 0.1577, Val. Loss: 2.3026, Val. Acc: 0.1172\n",
      "[0/251] Loss: 2.276190 (2.276190) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.311543 (2.279206) Accuracy: 0.093750 (0.181818)\n",
      "[20/251] Loss: 2.272764 (2.280108) Accuracy: 0.125000 (0.163690)\n",
      "[30/251] Loss: 2.295312 (2.275908) Accuracy: 0.125000 (0.170363)\n",
      "[40/251] Loss: 2.255445 (2.278116) Accuracy: 0.187500 (0.163110)\n",
      "[50/251] Loss: 2.276039 (2.279041) Accuracy: 0.156250 (0.164828)\n",
      "[60/251] Loss: 2.242811 (2.279868) Accuracy: 0.218750 (0.164447)\n",
      "[70/251] Loss: 2.287405 (2.279976) Accuracy: 0.218750 (0.167254)\n",
      "[80/251] Loss: 2.295471 (2.277035) Accuracy: 0.187500 (0.172454)\n",
      "[90/251] Loss: 2.305147 (2.277642) Accuracy: 0.093750 (0.172734)\n",
      "[100/251] Loss: 2.321311 (2.278376) Accuracy: 0.062500 (0.171720)\n",
      "[110/251] Loss: 2.300926 (2.278703) Accuracy: 0.093750 (0.171171)\n",
      "[120/251] Loss: 2.274531 (2.278248) Accuracy: 0.125000 (0.170713)\n",
      "[130/251] Loss: 2.303340 (2.278434) Accuracy: 0.125000 (0.170086)\n",
      "[140/251] Loss: 2.242853 (2.278307) Accuracy: 0.187500 (0.169105)\n",
      "[150/251] Loss: 2.290193 (2.279064) Accuracy: 0.125000 (0.166805)\n",
      "[160/251] Loss: 2.314382 (2.279933) Accuracy: 0.031250 (0.166731)\n",
      "[170/251] Loss: 2.274194 (2.280470) Accuracy: 0.125000 (0.164474)\n",
      "[180/251] Loss: 2.309105 (2.280894) Accuracy: 0.093750 (0.163847)\n",
      "[190/251] Loss: 2.302676 (2.281495) Accuracy: 0.156250 (0.161976)\n",
      "[200/251] Loss: 2.272497 (2.281696) Accuracy: 0.125000 (0.160914)\n",
      "[210/251] Loss: 2.283286 (2.281827) Accuracy: 0.156250 (0.161730)\n",
      "[220/251] Loss: 2.256189 (2.281370) Accuracy: 0.281250 (0.164027)\n",
      "[230/251] Loss: 2.302168 (2.282269) Accuracy: 0.062500 (0.161255)\n",
      "[240/251] Loss: 2.263374 (2.282210) Accuracy: 0.187500 (0.160399)\n",
      "[250/251] Loss: 2.135930 (2.281667) Accuracy: 1.000000 (0.163098)\n",
      "[0/63] Loss: 2.301911 (2.301911) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.311807 (2.303001) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271183 (2.300600) Accuracy: 0.125000 (0.122024)\n",
      "[30/63] Loss: 2.279039 (2.305884) Accuracy: 0.156250 (0.107863)\n",
      "[40/63] Loss: 2.275313 (2.304399) Accuracy: 0.156250 (0.112043)\n",
      "[50/63] Loss: 2.285194 (2.301831) Accuracy: 0.187500 (0.119485)\n",
      "[60/63] Loss: 2.299037 (2.302685) Accuracy: 0.187500 (0.119365)\n",
      "Epoch: 30/100, Train Loss: 2.2817, Train Acc: 0.1631, Val. Loss: 2.3025, Val. Acc: 0.1197\n",
      "[0/251] Loss: 2.297487 (2.297487) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.287530 (2.287415) Accuracy: 0.156250 (0.142045)\n",
      "[20/251] Loss: 2.262428 (2.279727) Accuracy: 0.156250 (0.165179)\n",
      "[30/251] Loss: 2.252580 (2.280317) Accuracy: 0.218750 (0.170363)\n",
      "[40/251] Loss: 2.296099 (2.281891) Accuracy: 0.062500 (0.163872)\n",
      "[50/251] Loss: 2.320205 (2.282596) Accuracy: 0.125000 (0.161152)\n",
      "[60/251] Loss: 2.271493 (2.283034) Accuracy: 0.218750 (0.161885)\n",
      "[70/251] Loss: 2.267710 (2.281139) Accuracy: 0.156250 (0.166373)\n",
      "[80/251] Loss: 2.255484 (2.279858) Accuracy: 0.250000 (0.170139)\n",
      "[90/251] Loss: 2.310990 (2.281368) Accuracy: 0.062500 (0.165522)\n",
      "[100/251] Loss: 2.285125 (2.283077) Accuracy: 0.156250 (0.160582)\n",
      "[110/251] Loss: 2.246676 (2.283435) Accuracy: 0.187500 (0.158784)\n",
      "[120/251] Loss: 2.260772 (2.282392) Accuracy: 0.218750 (0.160124)\n",
      "[130/251] Loss: 2.274576 (2.282110) Accuracy: 0.125000 (0.161737)\n",
      "[140/251] Loss: 2.296327 (2.282549) Accuracy: 0.125000 (0.161126)\n",
      "[150/251] Loss: 2.286221 (2.282871) Accuracy: 0.156250 (0.160803)\n",
      "[160/251] Loss: 2.300289 (2.282719) Accuracy: 0.156250 (0.161685)\n",
      "[170/251] Loss: 2.232082 (2.283044) Accuracy: 0.250000 (0.159539)\n",
      "[180/251] Loss: 2.240755 (2.282235) Accuracy: 0.281250 (0.160912)\n",
      "[190/251] Loss: 2.277546 (2.282067) Accuracy: 0.156250 (0.160831)\n",
      "[200/251] Loss: 2.286991 (2.282666) Accuracy: 0.187500 (0.158893)\n",
      "[210/251] Loss: 2.301259 (2.282467) Accuracy: 0.125000 (0.159656)\n",
      "[220/251] Loss: 2.310797 (2.282374) Accuracy: 0.062500 (0.159785)\n",
      "[230/251] Loss: 2.272494 (2.282083) Accuracy: 0.218750 (0.160173)\n",
      "[240/251] Loss: 2.242230 (2.282113) Accuracy: 0.250000 (0.160010)\n",
      "[250/251] Loss: 2.386282 (2.282453) Accuracy: 0.000000 (0.159363)\n",
      "[0/63] Loss: 2.301504 (2.301504) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.311901 (2.302917) Accuracy: 0.125000 (0.122159)\n",
      "[20/63] Loss: 2.271684 (2.300608) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278731 (2.305917) Accuracy: 0.156250 (0.105847)\n",
      "[40/63] Loss: 2.275522 (2.304372) Accuracy: 0.156250 (0.108994)\n",
      "[50/63] Loss: 2.284731 (2.301791) Accuracy: 0.187500 (0.117034)\n",
      "[60/63] Loss: 2.299356 (2.302664) Accuracy: 0.187500 (0.116803)\n",
      "Epoch: 31/100, Train Loss: 2.2825, Train Acc: 0.1594, Val. Loss: 2.3025, Val. Acc: 0.1172\n",
      "[0/251] Loss: 2.318236 (2.318236) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.266443 (2.291527) Accuracy: 0.218750 (0.144886)\n",
      "[20/251] Loss: 2.308844 (2.284170) Accuracy: 0.062500 (0.157738)\n",
      "[30/251] Loss: 2.232089 (2.281462) Accuracy: 0.281250 (0.164315)\n",
      "[40/251] Loss: 2.288733 (2.283294) Accuracy: 0.125000 (0.157774)\n",
      "[50/251] Loss: 2.291512 (2.283411) Accuracy: 0.125000 (0.156863)\n",
      "[60/251] Loss: 2.295986 (2.283833) Accuracy: 0.093750 (0.157275)\n",
      "[70/251] Loss: 2.232567 (2.279371) Accuracy: 0.218750 (0.166813)\n",
      "[80/251] Loss: 2.275315 (2.280246) Accuracy: 0.156250 (0.164352)\n",
      "[90/251] Loss: 2.181100 (2.278361) Accuracy: 0.406250 (0.167582)\n",
      "[100/251] Loss: 2.285189 (2.278381) Accuracy: 0.156250 (0.167698)\n",
      "[110/251] Loss: 2.297849 (2.279334) Accuracy: 0.062500 (0.165541)\n",
      "[120/251] Loss: 2.287361 (2.280022) Accuracy: 0.156250 (0.162965)\n",
      "[130/251] Loss: 2.293381 (2.280243) Accuracy: 0.187500 (0.162214)\n",
      "[140/251] Loss: 2.301831 (2.281490) Accuracy: 0.093750 (0.159574)\n",
      "[150/251] Loss: 2.276786 (2.281553) Accuracy: 0.187500 (0.160389)\n",
      "[160/251] Loss: 2.309920 (2.281723) Accuracy: 0.156250 (0.161297)\n",
      "[170/251] Loss: 2.247732 (2.282178) Accuracy: 0.218750 (0.160636)\n",
      "[180/251] Loss: 2.243394 (2.282075) Accuracy: 0.281250 (0.160912)\n",
      "[190/251] Loss: 2.229789 (2.281589) Accuracy: 0.250000 (0.161813)\n",
      "[200/251] Loss: 2.285465 (2.281062) Accuracy: 0.187500 (0.162469)\n",
      "[210/251] Loss: 2.240178 (2.280864) Accuracy: 0.281250 (0.163211)\n",
      "[220/251] Loss: 2.310393 (2.281516) Accuracy: 0.062500 (0.161765)\n",
      "[230/251] Loss: 2.296241 (2.281536) Accuracy: 0.156250 (0.162608)\n",
      "[240/251] Loss: 2.316745 (2.281720) Accuracy: 0.031250 (0.162085)\n",
      "[250/251] Loss: 2.393485 (2.282359) Accuracy: 0.000000 (0.160981)\n",
      "[0/63] Loss: 2.301264 (2.301264) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312078 (2.302984) Accuracy: 0.156250 (0.130682)\n",
      "[20/63] Loss: 2.272240 (2.300684) Accuracy: 0.125000 (0.123512)\n",
      "[30/63] Loss: 2.278895 (2.306018) Accuracy: 0.156250 (0.108871)\n",
      "[40/63] Loss: 2.275267 (2.304434) Accuracy: 0.125000 (0.112043)\n",
      "[50/63] Loss: 2.284539 (2.301817) Accuracy: 0.187500 (0.119485)\n",
      "[60/63] Loss: 2.299599 (2.302687) Accuracy: 0.156250 (0.119365)\n",
      "Epoch: 32/100, Train Loss: 2.2824, Train Acc: 0.1610, Val. Loss: 2.3026, Val. Acc: 0.1197\n",
      "[0/251] Loss: 2.268858 (2.268858) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.292645 (2.278343) Accuracy: 0.218750 (0.173295)\n",
      "[20/251] Loss: 2.297024 (2.278812) Accuracy: 0.093750 (0.174107)\n",
      "[30/251] Loss: 2.334543 (2.280223) Accuracy: 0.062500 (0.170363)\n",
      "[40/251] Loss: 2.243860 (2.280715) Accuracy: 0.250000 (0.169970)\n",
      "[50/251] Loss: 2.255518 (2.281452) Accuracy: 0.125000 (0.163603)\n",
      "[60/251] Loss: 2.301705 (2.280930) Accuracy: 0.156250 (0.166496)\n",
      "[70/251] Loss: 2.295061 (2.281056) Accuracy: 0.093750 (0.163732)\n",
      "[80/251] Loss: 2.277231 (2.280891) Accuracy: 0.156250 (0.163966)\n",
      "[90/251] Loss: 2.284338 (2.279762) Accuracy: 0.125000 (0.163805)\n",
      "[100/251] Loss: 2.270893 (2.280578) Accuracy: 0.156250 (0.160582)\n",
      "[110/251] Loss: 2.296278 (2.280643) Accuracy: 0.125000 (0.162162)\n",
      "[120/251] Loss: 2.251788 (2.280018) Accuracy: 0.250000 (0.163740)\n",
      "[130/251] Loss: 2.239339 (2.280337) Accuracy: 0.281250 (0.163884)\n",
      "[140/251] Loss: 2.256443 (2.280038) Accuracy: 0.218750 (0.163785)\n",
      "[150/251] Loss: 2.298556 (2.280734) Accuracy: 0.125000 (0.162459)\n",
      "[160/251] Loss: 2.274214 (2.281079) Accuracy: 0.125000 (0.161297)\n",
      "[170/251] Loss: 2.280802 (2.280857) Accuracy: 0.156250 (0.161184)\n",
      "[180/251] Loss: 2.277974 (2.281104) Accuracy: 0.062500 (0.160566)\n",
      "[190/251] Loss: 2.298694 (2.281064) Accuracy: 0.125000 (0.161486)\n",
      "[200/251] Loss: 2.325159 (2.281474) Accuracy: 0.093750 (0.160759)\n",
      "[210/251] Loss: 2.303825 (2.282363) Accuracy: 0.062500 (0.159212)\n",
      "[220/251] Loss: 2.285174 (2.282491) Accuracy: 0.125000 (0.159219)\n",
      "[230/251] Loss: 2.297153 (2.282506) Accuracy: 0.156250 (0.159632)\n",
      "[240/251] Loss: 2.288199 (2.281792) Accuracy: 0.187500 (0.161307)\n",
      "[250/251] Loss: 2.299653 (2.281877) Accuracy: 0.000000 (0.160483)\n",
      "[0/63] Loss: 2.300755 (2.300755) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312123 (2.302974) Accuracy: 0.156250 (0.127841)\n",
      "[20/63] Loss: 2.272030 (2.300693) Accuracy: 0.125000 (0.122024)\n",
      "[30/63] Loss: 2.278890 (2.306002) Accuracy: 0.156250 (0.107863)\n",
      "[40/63] Loss: 2.275627 (2.304407) Accuracy: 0.125000 (0.111280)\n",
      "[50/63] Loss: 2.284690 (2.301771) Accuracy: 0.187500 (0.119485)\n",
      "[60/63] Loss: 2.300111 (2.302655) Accuracy: 0.156250 (0.119877)\n",
      "Epoch: 33/100, Train Loss: 2.2819, Train Acc: 0.1605, Val. Loss: 2.3025, Val. Acc: 0.1202\n",
      "[0/251] Loss: 2.285786 (2.285786) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.297004 (2.283374) Accuracy: 0.156250 (0.153409)\n",
      "[20/251] Loss: 2.257562 (2.283922) Accuracy: 0.125000 (0.151786)\n",
      "[30/251] Loss: 2.280438 (2.281958) Accuracy: 0.156250 (0.159274)\n",
      "[40/251] Loss: 2.305316 (2.281799) Accuracy: 0.062500 (0.155488)\n",
      "[50/251] Loss: 2.285233 (2.281359) Accuracy: 0.156250 (0.154412)\n",
      "[60/251] Loss: 2.301705 (2.281131) Accuracy: 0.093750 (0.154713)\n",
      "[70/251] Loss: 2.263940 (2.281859) Accuracy: 0.187500 (0.155810)\n",
      "[80/251] Loss: 2.319378 (2.282473) Accuracy: 0.093750 (0.155093)\n",
      "[90/251] Loss: 2.290816 (2.282085) Accuracy: 0.125000 (0.155220)\n",
      "[100/251] Loss: 2.259357 (2.280563) Accuracy: 0.187500 (0.159035)\n",
      "[110/251] Loss: 2.267056 (2.280445) Accuracy: 0.187500 (0.159347)\n",
      "[120/251] Loss: 2.298879 (2.281078) Accuracy: 0.156250 (0.159607)\n",
      "[130/251] Loss: 2.272892 (2.281157) Accuracy: 0.156250 (0.159113)\n",
      "[140/251] Loss: 2.265877 (2.281652) Accuracy: 0.187500 (0.157801)\n",
      "[150/251] Loss: 2.262786 (2.282278) Accuracy: 0.187500 (0.157078)\n",
      "[160/251] Loss: 2.292958 (2.281356) Accuracy: 0.156250 (0.159356)\n",
      "[170/251] Loss: 2.286527 (2.281883) Accuracy: 0.218750 (0.158260)\n",
      "[180/251] Loss: 2.308019 (2.282012) Accuracy: 0.031250 (0.158149)\n",
      "[190/251] Loss: 2.265115 (2.281536) Accuracy: 0.156250 (0.159686)\n",
      "[200/251] Loss: 2.256621 (2.281689) Accuracy: 0.250000 (0.159826)\n",
      "[210/251] Loss: 2.278625 (2.281552) Accuracy: 0.156250 (0.160397)\n",
      "[220/251] Loss: 2.259485 (2.281809) Accuracy: 0.218750 (0.159219)\n",
      "[230/251] Loss: 2.307788 (2.281607) Accuracy: 0.156250 (0.159632)\n",
      "[240/251] Loss: 2.312917 (2.281526) Accuracy: 0.062500 (0.160140)\n",
      "[250/251] Loss: 2.310605 (2.281788) Accuracy: 0.000000 (0.159736)\n",
      "[0/63] Loss: 2.301034 (2.301034) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.311850 (2.302804) Accuracy: 0.156250 (0.130682)\n",
      "[20/63] Loss: 2.271951 (2.300668) Accuracy: 0.125000 (0.123512)\n",
      "[30/63] Loss: 2.278394 (2.305944) Accuracy: 0.156250 (0.108871)\n",
      "[40/63] Loss: 2.275554 (2.304355) Accuracy: 0.156250 (0.112043)\n",
      "[50/63] Loss: 2.284037 (2.301729) Accuracy: 0.187500 (0.119485)\n",
      "[60/63] Loss: 2.299736 (2.302631) Accuracy: 0.156250 (0.119365)\n",
      "Epoch: 34/100, Train Loss: 2.2818, Train Acc: 0.1597, Val. Loss: 2.3025, Val. Acc: 0.1197\n",
      "[0/251] Loss: 2.272827 (2.272827) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.321867 (2.295326) Accuracy: 0.062500 (0.130682)\n",
      "[20/251] Loss: 2.275377 (2.289374) Accuracy: 0.187500 (0.145833)\n",
      "[30/251] Loss: 2.273298 (2.283073) Accuracy: 0.125000 (0.150202)\n",
      "[40/251] Loss: 2.305797 (2.280348) Accuracy: 0.062500 (0.156250)\n",
      "[50/251] Loss: 2.207674 (2.277521) Accuracy: 0.312500 (0.168505)\n",
      "[60/251] Loss: 2.295823 (2.276912) Accuracy: 0.156250 (0.173156)\n",
      "[70/251] Loss: 2.288907 (2.279190) Accuracy: 0.125000 (0.165053)\n",
      "[80/251] Loss: 2.244532 (2.279106) Accuracy: 0.156250 (0.163194)\n",
      "[90/251] Loss: 2.235829 (2.279263) Accuracy: 0.250000 (0.163805)\n",
      "[100/251] Loss: 2.311994 (2.279917) Accuracy: 0.125000 (0.163985)\n",
      "[110/251] Loss: 2.275016 (2.280057) Accuracy: 0.156250 (0.163288)\n",
      "[120/251] Loss: 2.344728 (2.281581) Accuracy: 0.062500 (0.159866)\n",
      "[130/251] Loss: 2.266115 (2.281857) Accuracy: 0.187500 (0.159828)\n",
      "[140/251] Loss: 2.258507 (2.281400) Accuracy: 0.156250 (0.160683)\n",
      "[150/251] Loss: 2.249867 (2.281723) Accuracy: 0.250000 (0.159975)\n",
      "[160/251] Loss: 2.271112 (2.282034) Accuracy: 0.156250 (0.158191)\n",
      "[170/251] Loss: 2.262574 (2.282010) Accuracy: 0.250000 (0.158260)\n",
      "[180/251] Loss: 2.251561 (2.281799) Accuracy: 0.218750 (0.159012)\n",
      "[190/251] Loss: 2.292331 (2.282019) Accuracy: 0.156250 (0.157723)\n",
      "[200/251] Loss: 2.278774 (2.282250) Accuracy: 0.218750 (0.157805)\n",
      "[210/251] Loss: 2.288008 (2.282197) Accuracy: 0.156250 (0.158620)\n",
      "[220/251] Loss: 2.271717 (2.282448) Accuracy: 0.156250 (0.158371)\n",
      "[230/251] Loss: 2.307490 (2.282354) Accuracy: 0.125000 (0.159091)\n",
      "[240/251] Loss: 2.241177 (2.281755) Accuracy: 0.312500 (0.160918)\n",
      "[250/251] Loss: 2.369600 (2.281909) Accuracy: 0.000000 (0.160359)\n",
      "[0/63] Loss: 2.301098 (2.301098) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.311902 (2.302841) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271913 (2.300663) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278542 (2.305964) Accuracy: 0.156250 (0.105847)\n",
      "[40/63] Loss: 2.275372 (2.304358) Accuracy: 0.156250 (0.111280)\n",
      "[50/63] Loss: 2.284527 (2.301734) Accuracy: 0.187500 (0.118873)\n",
      "[60/63] Loss: 2.300078 (2.302649) Accuracy: 0.156250 (0.118852)\n",
      "Epoch: 35/100, Train Loss: 2.2819, Train Acc: 0.1604, Val. Loss: 2.3025, Val. Acc: 0.1192\n",
      "[0/251] Loss: 2.311792 (2.311792) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.237576 (2.281723) Accuracy: 0.218750 (0.164773)\n",
      "[20/251] Loss: 2.277321 (2.274695) Accuracy: 0.187500 (0.175595)\n",
      "[30/251] Loss: 2.276384 (2.278870) Accuracy: 0.156250 (0.168347)\n",
      "[40/251] Loss: 2.284734 (2.279290) Accuracy: 0.093750 (0.169970)\n",
      "[50/251] Loss: 2.295026 (2.282906) Accuracy: 0.093750 (0.162990)\n",
      "[60/251] Loss: 2.236662 (2.282646) Accuracy: 0.312500 (0.164447)\n",
      "[70/251] Loss: 2.327307 (2.284285) Accuracy: 0.062500 (0.161972)\n",
      "[80/251] Loss: 2.268342 (2.283974) Accuracy: 0.187500 (0.163580)\n",
      "[90/251] Loss: 2.292106 (2.284577) Accuracy: 0.125000 (0.162775)\n",
      "[100/251] Loss: 2.264529 (2.284404) Accuracy: 0.156250 (0.163057)\n",
      "[110/251] Loss: 2.283805 (2.284883) Accuracy: 0.187500 (0.160755)\n",
      "[120/251] Loss: 2.301315 (2.284943) Accuracy: 0.062500 (0.160124)\n",
      "[130/251] Loss: 2.254709 (2.284558) Accuracy: 0.187500 (0.159351)\n",
      "[140/251] Loss: 2.311824 (2.283294) Accuracy: 0.093750 (0.161791)\n",
      "[150/251] Loss: 2.214293 (2.282852) Accuracy: 0.312500 (0.161838)\n",
      "[160/251] Loss: 2.267009 (2.283078) Accuracy: 0.187500 (0.160714)\n",
      "[170/251] Loss: 2.302911 (2.282606) Accuracy: 0.125000 (0.159905)\n",
      "[180/251] Loss: 2.290954 (2.282394) Accuracy: 0.093750 (0.159876)\n",
      "[190/251] Loss: 2.288494 (2.281504) Accuracy: 0.156250 (0.161486)\n",
      "[200/251] Loss: 2.280235 (2.281650) Accuracy: 0.218750 (0.162780)\n",
      "[210/251] Loss: 2.278921 (2.281819) Accuracy: 0.187500 (0.161286)\n",
      "[220/251] Loss: 2.280411 (2.281661) Accuracy: 0.125000 (0.161623)\n",
      "[230/251] Loss: 2.273322 (2.281794) Accuracy: 0.218750 (0.161526)\n",
      "[240/251] Loss: 2.303204 (2.281222) Accuracy: 0.093750 (0.162344)\n",
      "[250/251] Loss: 2.352817 (2.281754) Accuracy: 0.000000 (0.161230)\n",
      "[0/63] Loss: 2.300880 (2.300880) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312003 (2.302852) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271996 (2.300687) Accuracy: 0.125000 (0.120536)\n",
      "[30/63] Loss: 2.278479 (2.305989) Accuracy: 0.156250 (0.106855)\n",
      "[40/63] Loss: 2.275256 (2.304363) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284440 (2.301716) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300181 (2.302646) Accuracy: 0.156250 (0.117828)\n",
      "Epoch: 36/100, Train Loss: 2.2818, Train Acc: 0.1612, Val. Loss: 2.3025, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.285972 (2.285972) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.295033 (2.288781) Accuracy: 0.156250 (0.150568)\n",
      "[20/251] Loss: 2.270893 (2.282044) Accuracy: 0.187500 (0.171131)\n",
      "[30/251] Loss: 2.302674 (2.278146) Accuracy: 0.125000 (0.172379)\n",
      "[40/251] Loss: 2.314921 (2.278812) Accuracy: 0.187500 (0.166921)\n",
      "[50/251] Loss: 2.299307 (2.279663) Accuracy: 0.125000 (0.163603)\n",
      "[60/251] Loss: 2.302044 (2.282134) Accuracy: 0.093750 (0.160348)\n",
      "[70/251] Loss: 2.235469 (2.280335) Accuracy: 0.156250 (0.158891)\n",
      "[80/251] Loss: 2.346298 (2.282151) Accuracy: 0.031250 (0.158179)\n",
      "[90/251] Loss: 2.264308 (2.282369) Accuracy: 0.187500 (0.158654)\n",
      "[100/251] Loss: 2.312748 (2.282785) Accuracy: 0.093750 (0.159035)\n",
      "[110/251] Loss: 2.319900 (2.283369) Accuracy: 0.031250 (0.157376)\n",
      "[120/251] Loss: 2.234428 (2.283713) Accuracy: 0.312500 (0.157800)\n",
      "[130/251] Loss: 2.304815 (2.282899) Accuracy: 0.156250 (0.159113)\n",
      "[140/251] Loss: 2.270093 (2.281403) Accuracy: 0.187500 (0.162234)\n",
      "[150/251] Loss: 2.299839 (2.280972) Accuracy: 0.125000 (0.162873)\n",
      "[160/251] Loss: 2.267443 (2.280828) Accuracy: 0.156250 (0.163626)\n",
      "[170/251] Loss: 2.262574 (2.280726) Accuracy: 0.125000 (0.164839)\n",
      "[180/251] Loss: 2.291962 (2.281224) Accuracy: 0.156250 (0.163847)\n",
      "[190/251] Loss: 2.300680 (2.280860) Accuracy: 0.156250 (0.164431)\n",
      "[200/251] Loss: 2.272160 (2.280331) Accuracy: 0.156250 (0.164335)\n",
      "[210/251] Loss: 2.306060 (2.280543) Accuracy: 0.093750 (0.163803)\n",
      "[220/251] Loss: 2.293296 (2.280789) Accuracy: 0.156250 (0.163320)\n",
      "[230/251] Loss: 2.246996 (2.280882) Accuracy: 0.187500 (0.163014)\n",
      "[240/251] Loss: 2.267885 (2.281098) Accuracy: 0.187500 (0.162733)\n",
      "[250/251] Loss: 2.385031 (2.281779) Accuracy: 0.000000 (0.161106)\n",
      "[0/63] Loss: 2.300849 (2.300849) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312043 (2.302870) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271706 (2.300655) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278437 (2.305964) Accuracy: 0.156250 (0.105847)\n",
      "[40/63] Loss: 2.275310 (2.304341) Accuracy: 0.125000 (0.108994)\n",
      "[50/63] Loss: 2.284682 (2.301690) Accuracy: 0.187500 (0.117034)\n",
      "[60/63] Loss: 2.300372 (2.302638) Accuracy: 0.156250 (0.116291)\n",
      "Epoch: 37/100, Train Loss: 2.2818, Train Acc: 0.1611, Val. Loss: 2.3025, Val. Acc: 0.1167\n",
      "[0/251] Loss: 2.294563 (2.294563) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.299383 (2.291105) Accuracy: 0.093750 (0.144886)\n",
      "[20/251] Loss: 2.264616 (2.283857) Accuracy: 0.218750 (0.151786)\n",
      "[30/251] Loss: 2.293694 (2.281253) Accuracy: 0.156250 (0.158266)\n",
      "[40/251] Loss: 2.247138 (2.279785) Accuracy: 0.187500 (0.163110)\n",
      "[50/251] Loss: 2.269024 (2.278553) Accuracy: 0.125000 (0.166054)\n",
      "[60/251] Loss: 2.276887 (2.278218) Accuracy: 0.156250 (0.167008)\n",
      "[70/251] Loss: 2.302776 (2.279365) Accuracy: 0.093750 (0.163732)\n",
      "[80/251] Loss: 2.265499 (2.278192) Accuracy: 0.187500 (0.166281)\n",
      "[90/251] Loss: 2.304887 (2.279193) Accuracy: 0.093750 (0.168269)\n",
      "[100/251] Loss: 2.320275 (2.280292) Accuracy: 0.125000 (0.167079)\n",
      "[110/251] Loss: 2.265060 (2.279705) Accuracy: 0.156250 (0.166948)\n",
      "[120/251] Loss: 2.321207 (2.279976) Accuracy: 0.062500 (0.166322)\n",
      "[130/251] Loss: 2.285657 (2.279643) Accuracy: 0.156250 (0.166269)\n",
      "[140/251] Loss: 2.298410 (2.280582) Accuracy: 0.125000 (0.165559)\n",
      "[150/251] Loss: 2.250450 (2.280196) Accuracy: 0.187500 (0.165563)\n",
      "[160/251] Loss: 2.306923 (2.280111) Accuracy: 0.093750 (0.165955)\n",
      "[170/251] Loss: 2.283002 (2.280493) Accuracy: 0.218750 (0.164474)\n",
      "[180/251] Loss: 2.324610 (2.280682) Accuracy: 0.031250 (0.163501)\n",
      "[190/251] Loss: 2.278700 (2.280588) Accuracy: 0.156250 (0.163613)\n",
      "[200/251] Loss: 2.288594 (2.280681) Accuracy: 0.125000 (0.164024)\n",
      "[210/251] Loss: 2.234973 (2.281133) Accuracy: 0.218750 (0.163211)\n",
      "[220/251] Loss: 2.261450 (2.281655) Accuracy: 0.250000 (0.162613)\n",
      "[230/251] Loss: 2.284263 (2.281449) Accuracy: 0.125000 (0.163014)\n",
      "[240/251] Loss: 2.285470 (2.281499) Accuracy: 0.125000 (0.161955)\n",
      "[250/251] Loss: 2.326100 (2.281476) Accuracy: 0.000000 (0.161977)\n",
      "[0/63] Loss: 2.300973 (2.300973) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312001 (2.302899) Accuracy: 0.125000 (0.130682)\n",
      "[20/63] Loss: 2.271264 (2.300631) Accuracy: 0.125000 (0.122024)\n",
      "[30/63] Loss: 2.278397 (2.305916) Accuracy: 0.156250 (0.107863)\n",
      "[40/63] Loss: 2.275462 (2.304315) Accuracy: 0.125000 (0.110518)\n",
      "[50/63] Loss: 2.284805 (2.301666) Accuracy: 0.187500 (0.118873)\n",
      "[60/63] Loss: 2.300390 (2.302633) Accuracy: 0.156250 (0.118340)\n",
      "Epoch: 38/100, Train Loss: 2.2815, Train Acc: 0.1620, Val. Loss: 2.3025, Val. Acc: 0.1187\n",
      "[0/251] Loss: 2.283315 (2.283315) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.276500 (2.280384) Accuracy: 0.187500 (0.187500)\n",
      "[20/251] Loss: 2.295202 (2.284188) Accuracy: 0.156250 (0.165179)\n",
      "[30/251] Loss: 2.313632 (2.286393) Accuracy: 0.062500 (0.157258)\n",
      "[40/251] Loss: 2.279571 (2.282478) Accuracy: 0.218750 (0.161585)\n",
      "[50/251] Loss: 2.316408 (2.282789) Accuracy: 0.062500 (0.159926)\n",
      "[60/251] Loss: 2.258394 (2.282657) Accuracy: 0.218750 (0.159836)\n",
      "[70/251] Loss: 2.309570 (2.282113) Accuracy: 0.125000 (0.162412)\n",
      "[80/251] Loss: 2.225330 (2.281384) Accuracy: 0.281250 (0.161651)\n",
      "[90/251] Loss: 2.284140 (2.282480) Accuracy: 0.187500 (0.159341)\n",
      "[100/251] Loss: 2.303011 (2.282154) Accuracy: 0.187500 (0.160582)\n",
      "[110/251] Loss: 2.283526 (2.281726) Accuracy: 0.187500 (0.161036)\n",
      "[120/251] Loss: 2.258785 (2.281432) Accuracy: 0.281250 (0.161932)\n",
      "[130/251] Loss: 2.286611 (2.281843) Accuracy: 0.062500 (0.159351)\n",
      "[140/251] Loss: 2.275868 (2.281311) Accuracy: 0.187500 (0.162012)\n",
      "[150/251] Loss: 2.320223 (2.282503) Accuracy: 0.156250 (0.160596)\n",
      "[160/251] Loss: 2.269944 (2.282300) Accuracy: 0.187500 (0.160908)\n",
      "[170/251] Loss: 2.345472 (2.282483) Accuracy: 0.000000 (0.161001)\n",
      "[180/251] Loss: 2.259605 (2.282480) Accuracy: 0.187500 (0.160048)\n",
      "[190/251] Loss: 2.201506 (2.281956) Accuracy: 0.343750 (0.161813)\n",
      "[200/251] Loss: 2.289461 (2.281978) Accuracy: 0.187500 (0.161536)\n",
      "[210/251] Loss: 2.231014 (2.281122) Accuracy: 0.250000 (0.162767)\n",
      "[220/251] Loss: 2.312980 (2.281388) Accuracy: 0.125000 (0.162330)\n",
      "[230/251] Loss: 2.244516 (2.280866) Accuracy: 0.218750 (0.163285)\n",
      "[240/251] Loss: 2.317228 (2.281188) Accuracy: 0.031250 (0.162085)\n",
      "[250/251] Loss: 2.310956 (2.281346) Accuracy: 0.000000 (0.161853)\n",
      "[0/63] Loss: 2.301052 (2.301052) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.311966 (2.302826) Accuracy: 0.156250 (0.133523)\n",
      "[20/63] Loss: 2.271466 (2.300630) Accuracy: 0.125000 (0.120536)\n",
      "[30/63] Loss: 2.278604 (2.305924) Accuracy: 0.156250 (0.106855)\n",
      "[40/63] Loss: 2.275384 (2.304312) Accuracy: 0.125000 (0.110518)\n",
      "[50/63] Loss: 2.284707 (2.301653) Accuracy: 0.187500 (0.118873)\n",
      "[60/63] Loss: 2.300109 (2.302630) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 39/100, Train Loss: 2.2813, Train Acc: 0.1619, Val. Loss: 2.3025, Val. Acc: 0.1172\n",
      "[0/251] Loss: 2.248824 (2.248824) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.271511 (2.280466) Accuracy: 0.156250 (0.153409)\n",
      "[20/251] Loss: 2.331915 (2.279861) Accuracy: 0.062500 (0.166667)\n",
      "[30/251] Loss: 2.274746 (2.279174) Accuracy: 0.218750 (0.163306)\n",
      "[40/251] Loss: 2.305383 (2.280303) Accuracy: 0.062500 (0.162348)\n",
      "[50/251] Loss: 2.323996 (2.278745) Accuracy: 0.062500 (0.164216)\n",
      "[60/251] Loss: 2.281890 (2.279272) Accuracy: 0.250000 (0.160861)\n",
      "[70/251] Loss: 2.280750 (2.279898) Accuracy: 0.093750 (0.161092)\n",
      "[80/251] Loss: 2.290318 (2.279459) Accuracy: 0.187500 (0.164352)\n",
      "[90/251] Loss: 2.223719 (2.278982) Accuracy: 0.343750 (0.164492)\n",
      "[100/251] Loss: 2.255204 (2.279782) Accuracy: 0.281250 (0.164295)\n",
      "[110/251] Loss: 2.253388 (2.279864) Accuracy: 0.187500 (0.164414)\n",
      "[120/251] Loss: 2.286664 (2.279572) Accuracy: 0.156250 (0.166322)\n",
      "[130/251] Loss: 2.347938 (2.280148) Accuracy: 0.031250 (0.166508)\n",
      "[140/251] Loss: 2.257190 (2.279167) Accuracy: 0.218750 (0.167332)\n",
      "[150/251] Loss: 2.218966 (2.279713) Accuracy: 0.218750 (0.164321)\n",
      "[160/251] Loss: 2.277753 (2.279864) Accuracy: 0.218750 (0.164596)\n",
      "[170/251] Loss: 2.296207 (2.279932) Accuracy: 0.093750 (0.163925)\n",
      "[180/251] Loss: 2.250680 (2.279652) Accuracy: 0.156250 (0.165055)\n",
      "[190/251] Loss: 2.287972 (2.280211) Accuracy: 0.156250 (0.164594)\n",
      "[200/251] Loss: 2.289318 (2.280172) Accuracy: 0.156250 (0.164646)\n",
      "[210/251] Loss: 2.270916 (2.280121) Accuracy: 0.187500 (0.164988)\n",
      "[220/251] Loss: 2.308837 (2.280153) Accuracy: 0.156250 (0.164876)\n",
      "[230/251] Loss: 2.290792 (2.280775) Accuracy: 0.156250 (0.164637)\n",
      "[240/251] Loss: 2.281801 (2.281049) Accuracy: 0.093750 (0.163511)\n",
      "[250/251] Loss: 2.395399 (2.281608) Accuracy: 0.000000 (0.162102)\n",
      "[0/63] Loss: 2.301077 (2.301077) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.311990 (2.302839) Accuracy: 0.156250 (0.130682)\n",
      "[20/63] Loss: 2.271547 (2.300652) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278654 (2.305967) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.275242 (2.304342) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284640 (2.301673) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300186 (2.302647) Accuracy: 0.156250 (0.115779)\n",
      "Epoch: 40/100, Train Loss: 2.2816, Train Acc: 0.1621, Val. Loss: 2.3025, Val. Acc: 0.1162\n",
      "[0/251] Loss: 2.290798 (2.290798) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.272598 (2.285611) Accuracy: 0.187500 (0.173295)\n",
      "[20/251] Loss: 2.264650 (2.283471) Accuracy: 0.218750 (0.166667)\n",
      "[30/251] Loss: 2.281567 (2.282656) Accuracy: 0.093750 (0.167339)\n",
      "[40/251] Loss: 2.242315 (2.279808) Accuracy: 0.250000 (0.173780)\n",
      "[50/251] Loss: 2.283273 (2.277943) Accuracy: 0.156250 (0.175858)\n",
      "[60/251] Loss: 2.290653 (2.279574) Accuracy: 0.093750 (0.171107)\n",
      "[70/251] Loss: 2.299891 (2.278220) Accuracy: 0.093750 (0.175176)\n",
      "[80/251] Loss: 2.284920 (2.279991) Accuracy: 0.093750 (0.168981)\n",
      "[90/251] Loss: 2.282411 (2.279438) Accuracy: 0.156250 (0.171703)\n",
      "[100/251] Loss: 2.302371 (2.280887) Accuracy: 0.093750 (0.167079)\n",
      "[110/251] Loss: 2.248292 (2.281021) Accuracy: 0.218750 (0.168074)\n",
      "[120/251] Loss: 2.293187 (2.281129) Accuracy: 0.093750 (0.166581)\n",
      "[130/251] Loss: 2.276870 (2.281382) Accuracy: 0.156250 (0.164838)\n",
      "[140/251] Loss: 2.298588 (2.281644) Accuracy: 0.187500 (0.164672)\n",
      "[150/251] Loss: 2.303286 (2.281863) Accuracy: 0.093750 (0.164942)\n",
      "[160/251] Loss: 2.313409 (2.281730) Accuracy: 0.093750 (0.165567)\n",
      "[170/251] Loss: 2.276653 (2.281991) Accuracy: 0.218750 (0.165387)\n",
      "[180/251] Loss: 2.263214 (2.281590) Accuracy: 0.187500 (0.165401)\n",
      "[190/251] Loss: 2.287387 (2.281451) Accuracy: 0.125000 (0.164594)\n",
      "[200/251] Loss: 2.286610 (2.281366) Accuracy: 0.187500 (0.164335)\n",
      "[210/251] Loss: 2.225385 (2.280641) Accuracy: 0.250000 (0.164544)\n",
      "[220/251] Loss: 2.269380 (2.280778) Accuracy: 0.187500 (0.163886)\n",
      "[230/251] Loss: 2.281796 (2.281517) Accuracy: 0.125000 (0.162067)\n",
      "[240/251] Loss: 2.310950 (2.281150) Accuracy: 0.156250 (0.162604)\n",
      "[250/251] Loss: 2.289450 (2.281139) Accuracy: 0.000000 (0.161728)\n",
      "[0/63] Loss: 2.301059 (2.301059) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.312209 (2.302956) Accuracy: 0.125000 (0.130682)\n",
      "[20/63] Loss: 2.271597 (2.300720) Accuracy: 0.125000 (0.120536)\n",
      "[30/63] Loss: 2.278798 (2.306055) Accuracy: 0.125000 (0.105847)\n",
      "[40/63] Loss: 2.275280 (2.304424) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284496 (2.301741) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300334 (2.302697) Accuracy: 0.156250 (0.115779)\n",
      "Epoch: 41/100, Train Loss: 2.2811, Train Acc: 0.1617, Val. Loss: 2.3026, Val. Acc: 0.1162\n",
      "[0/251] Loss: 2.320021 (2.320021) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.258317 (2.287891) Accuracy: 0.250000 (0.139205)\n",
      "[20/251] Loss: 2.292111 (2.288364) Accuracy: 0.125000 (0.144345)\n",
      "[30/251] Loss: 2.309965 (2.281098) Accuracy: 0.062500 (0.156250)\n",
      "[40/251] Loss: 2.271769 (2.281779) Accuracy: 0.125000 (0.154726)\n",
      "[50/251] Loss: 2.292741 (2.281314) Accuracy: 0.093750 (0.157475)\n",
      "[60/251] Loss: 2.263742 (2.282671) Accuracy: 0.187500 (0.157275)\n",
      "[70/251] Loss: 2.217123 (2.282509) Accuracy: 0.250000 (0.160651)\n",
      "[80/251] Loss: 2.306204 (2.281744) Accuracy: 0.156250 (0.162037)\n",
      "[90/251] Loss: 2.282071 (2.281313) Accuracy: 0.156250 (0.161058)\n",
      "[100/251] Loss: 2.335201 (2.280481) Accuracy: 0.031250 (0.161819)\n",
      "[110/251] Loss: 2.239964 (2.278892) Accuracy: 0.281250 (0.163570)\n",
      "[120/251] Loss: 2.287894 (2.278878) Accuracy: 0.218750 (0.166322)\n",
      "[130/251] Loss: 2.310706 (2.279065) Accuracy: 0.093750 (0.164838)\n",
      "[140/251] Loss: 2.310134 (2.279289) Accuracy: 0.093750 (0.166002)\n",
      "[150/251] Loss: 2.287650 (2.279175) Accuracy: 0.125000 (0.165977)\n",
      "[160/251] Loss: 2.259437 (2.279644) Accuracy: 0.156250 (0.163820)\n",
      "[170/251] Loss: 2.265044 (2.280077) Accuracy: 0.187500 (0.163377)\n",
      "[180/251] Loss: 2.312126 (2.279980) Accuracy: 0.125000 (0.164019)\n",
      "[190/251] Loss: 2.303963 (2.280532) Accuracy: 0.156250 (0.162795)\n",
      "[200/251] Loss: 2.273796 (2.280402) Accuracy: 0.156250 (0.163713)\n",
      "[210/251] Loss: 2.291019 (2.280467) Accuracy: 0.156250 (0.163211)\n",
      "[220/251] Loss: 2.261235 (2.280910) Accuracy: 0.156250 (0.161765)\n",
      "[230/251] Loss: 2.250957 (2.280857) Accuracy: 0.281250 (0.163014)\n",
      "[240/251] Loss: 2.308399 (2.280881) Accuracy: 0.093750 (0.162215)\n",
      "[250/251] Loss: 2.309241 (2.281158) Accuracy: 0.000000 (0.161355)\n",
      "[0/63] Loss: 2.301065 (2.301065) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.312198 (2.302976) Accuracy: 0.156250 (0.133523)\n",
      "[20/63] Loss: 2.271574 (2.300730) Accuracy: 0.125000 (0.123512)\n",
      "[30/63] Loss: 2.278738 (2.306084) Accuracy: 0.125000 (0.107863)\n",
      "[40/63] Loss: 2.275117 (2.304446) Accuracy: 0.125000 (0.112805)\n",
      "[50/63] Loss: 2.284478 (2.301760) Accuracy: 0.187500 (0.120098)\n",
      "[60/63] Loss: 2.300271 (2.302712) Accuracy: 0.156250 (0.118852)\n",
      "Epoch: 42/100, Train Loss: 2.2812, Train Acc: 0.1614, Val. Loss: 2.3026, Val. Acc: 0.1197\n",
      "[0/251] Loss: 2.288913 (2.288913) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.260604 (2.277638) Accuracy: 0.187500 (0.156250)\n",
      "[20/251] Loss: 2.345308 (2.279646) Accuracy: 0.062500 (0.153274)\n",
      "[30/251] Loss: 2.317190 (2.281239) Accuracy: 0.093750 (0.158266)\n",
      "[40/251] Loss: 2.269806 (2.279584) Accuracy: 0.187500 (0.157774)\n",
      "[50/251] Loss: 2.342878 (2.277323) Accuracy: 0.031250 (0.161765)\n",
      "[60/251] Loss: 2.256112 (2.278173) Accuracy: 0.218750 (0.160861)\n",
      "[70/251] Loss: 2.275436 (2.278897) Accuracy: 0.187500 (0.162412)\n",
      "[80/251] Loss: 2.262632 (2.277469) Accuracy: 0.250000 (0.166667)\n",
      "[90/251] Loss: 2.307579 (2.278265) Accuracy: 0.125000 (0.165522)\n",
      "[100/251] Loss: 2.312321 (2.278450) Accuracy: 0.093750 (0.163985)\n",
      "[110/251] Loss: 2.274093 (2.277448) Accuracy: 0.156250 (0.166104)\n",
      "[120/251] Loss: 2.328754 (2.278223) Accuracy: 0.062500 (0.166322)\n",
      "[130/251] Loss: 2.268686 (2.277802) Accuracy: 0.187500 (0.167462)\n",
      "[140/251] Loss: 2.286810 (2.277275) Accuracy: 0.187500 (0.169770)\n",
      "[150/251] Loss: 2.316007 (2.277529) Accuracy: 0.125000 (0.169702)\n",
      "[160/251] Loss: 2.254755 (2.278014) Accuracy: 0.218750 (0.170031)\n",
      "[170/251] Loss: 2.290381 (2.278916) Accuracy: 0.093750 (0.167398)\n",
      "[180/251] Loss: 2.302933 (2.279751) Accuracy: 0.093750 (0.166264)\n",
      "[190/251] Loss: 2.318274 (2.279718) Accuracy: 0.093750 (0.165740)\n",
      "[200/251] Loss: 2.273344 (2.279860) Accuracy: 0.156250 (0.166045)\n",
      "[210/251] Loss: 2.266325 (2.279904) Accuracy: 0.187500 (0.166025)\n",
      "[220/251] Loss: 2.306447 (2.280503) Accuracy: 0.031250 (0.164593)\n",
      "[230/251] Loss: 2.291863 (2.280689) Accuracy: 0.093750 (0.163961)\n",
      "[240/251] Loss: 2.280080 (2.280852) Accuracy: 0.156250 (0.163771)\n",
      "[250/251] Loss: 2.243534 (2.280856) Accuracy: 0.000000 (0.162724)\n",
      "[0/63] Loss: 2.301129 (2.301129) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.312256 (2.302989) Accuracy: 0.156250 (0.133523)\n",
      "[20/63] Loss: 2.271666 (2.300746) Accuracy: 0.125000 (0.122024)\n",
      "[30/63] Loss: 2.278694 (2.306105) Accuracy: 0.125000 (0.106855)\n",
      "[40/63] Loss: 2.275067 (2.304460) Accuracy: 0.125000 (0.112043)\n",
      "[50/63] Loss: 2.284507 (2.301775) Accuracy: 0.187500 (0.119485)\n",
      "[60/63] Loss: 2.300299 (2.302726) Accuracy: 0.156250 (0.118852)\n",
      "Epoch: 43/100, Train Loss: 2.2809, Train Acc: 0.1627, Val. Loss: 2.3026, Val. Acc: 0.1197\n",
      "[0/251] Loss: 2.269396 (2.269396) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.292261 (2.282672) Accuracy: 0.156250 (0.161932)\n",
      "[20/251] Loss: 2.269185 (2.280258) Accuracy: 0.281250 (0.168155)\n",
      "[30/251] Loss: 2.299526 (2.280935) Accuracy: 0.093750 (0.167339)\n",
      "[40/251] Loss: 2.308267 (2.280167) Accuracy: 0.062500 (0.169970)\n",
      "[50/251] Loss: 2.293232 (2.281781) Accuracy: 0.125000 (0.162990)\n",
      "[60/251] Loss: 2.273066 (2.281474) Accuracy: 0.218750 (0.163422)\n",
      "[70/251] Loss: 2.312125 (2.280609) Accuracy: 0.062500 (0.164173)\n",
      "[80/251] Loss: 2.281184 (2.279538) Accuracy: 0.187500 (0.168981)\n",
      "[90/251] Loss: 2.241260 (2.280349) Accuracy: 0.187500 (0.166209)\n",
      "[100/251] Loss: 2.271561 (2.280036) Accuracy: 0.250000 (0.168007)\n",
      "[110/251] Loss: 2.263340 (2.279886) Accuracy: 0.156250 (0.168074)\n",
      "[120/251] Loss: 2.239145 (2.278742) Accuracy: 0.218750 (0.170713)\n",
      "[130/251] Loss: 2.286592 (2.278809) Accuracy: 0.093750 (0.169370)\n",
      "[140/251] Loss: 2.275626 (2.279918) Accuracy: 0.156250 (0.167332)\n",
      "[150/251] Loss: 2.300299 (2.280745) Accuracy: 0.156250 (0.165770)\n",
      "[160/251] Loss: 2.304266 (2.280977) Accuracy: 0.093750 (0.164984)\n",
      "[170/251] Loss: 2.288673 (2.280940) Accuracy: 0.093750 (0.164108)\n",
      "[180/251] Loss: 2.295159 (2.281042) Accuracy: 0.093750 (0.164192)\n",
      "[190/251] Loss: 2.254621 (2.280403) Accuracy: 0.218750 (0.166230)\n",
      "[200/251] Loss: 2.295844 (2.280514) Accuracy: 0.062500 (0.164956)\n",
      "[210/251] Loss: 2.276363 (2.280777) Accuracy: 0.187500 (0.163951)\n",
      "[220/251] Loss: 2.307543 (2.281212) Accuracy: 0.093750 (0.162613)\n",
      "[230/251] Loss: 2.294631 (2.280972) Accuracy: 0.125000 (0.163690)\n",
      "[240/251] Loss: 2.286206 (2.281305) Accuracy: 0.062500 (0.162733)\n",
      "[250/251] Loss: 2.375191 (2.281321) Accuracy: 0.000000 (0.162600)\n",
      "[0/63] Loss: 2.301083 (2.301083) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.312266 (2.302991) Accuracy: 0.156250 (0.133523)\n",
      "[20/63] Loss: 2.271664 (2.300756) Accuracy: 0.125000 (0.122024)\n",
      "[30/63] Loss: 2.278692 (2.306122) Accuracy: 0.125000 (0.106855)\n",
      "[40/63] Loss: 2.274988 (2.304467) Accuracy: 0.125000 (0.112043)\n",
      "[50/63] Loss: 2.284476 (2.301777) Accuracy: 0.187500 (0.119485)\n",
      "[60/63] Loss: 2.300346 (2.302733) Accuracy: 0.156250 (0.118852)\n",
      "Epoch: 44/100, Train Loss: 2.2813, Train Acc: 0.1626, Val. Loss: 2.3026, Val. Acc: 0.1197\n",
      "[0/251] Loss: 2.304718 (2.304718) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.281519 (2.280509) Accuracy: 0.093750 (0.159091)\n",
      "[20/251] Loss: 2.275095 (2.279807) Accuracy: 0.187500 (0.147321)\n",
      "[30/251] Loss: 2.293775 (2.281778) Accuracy: 0.093750 (0.147177)\n",
      "[40/251] Loss: 2.253927 (2.278171) Accuracy: 0.187500 (0.158537)\n",
      "[50/251] Loss: 2.288134 (2.279933) Accuracy: 0.093750 (0.154412)\n",
      "[60/251] Loss: 2.230320 (2.277318) Accuracy: 0.281250 (0.161885)\n",
      "[70/251] Loss: 2.315400 (2.279619) Accuracy: 0.125000 (0.158011)\n",
      "[80/251] Loss: 2.291026 (2.279306) Accuracy: 0.125000 (0.160880)\n",
      "[90/251] Loss: 2.329269 (2.279012) Accuracy: 0.031250 (0.160371)\n",
      "[100/251] Loss: 2.276689 (2.278307) Accuracy: 0.156250 (0.163676)\n",
      "[110/251] Loss: 2.272526 (2.280412) Accuracy: 0.156250 (0.161036)\n",
      "[120/251] Loss: 2.276793 (2.279757) Accuracy: 0.218750 (0.162965)\n",
      "[130/251] Loss: 2.289797 (2.280159) Accuracy: 0.062500 (0.162929)\n",
      "[140/251] Loss: 2.273612 (2.279824) Accuracy: 0.125000 (0.162234)\n",
      "[150/251] Loss: 2.300799 (2.279472) Accuracy: 0.156250 (0.162252)\n",
      "[160/251] Loss: 2.268219 (2.280168) Accuracy: 0.218750 (0.161102)\n",
      "[170/251] Loss: 2.264024 (2.281100) Accuracy: 0.187500 (0.159905)\n",
      "[180/251] Loss: 2.266034 (2.280729) Accuracy: 0.250000 (0.161257)\n",
      "[190/251] Loss: 2.274104 (2.280844) Accuracy: 0.250000 (0.160995)\n",
      "[200/251] Loss: 2.296199 (2.281148) Accuracy: 0.125000 (0.161381)\n",
      "[210/251] Loss: 2.260587 (2.281468) Accuracy: 0.281250 (0.161286)\n",
      "[220/251] Loss: 2.269363 (2.281697) Accuracy: 0.187500 (0.161058)\n",
      "[230/251] Loss: 2.180278 (2.281103) Accuracy: 0.250000 (0.162067)\n",
      "[240/251] Loss: 2.317748 (2.280972) Accuracy: 0.031250 (0.162733)\n",
      "[250/251] Loss: 2.084947 (2.280166) Accuracy: 1.000000 (0.166335)\n",
      "[0/63] Loss: 2.301177 (2.301177) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312153 (2.302989) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271681 (2.300760) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278643 (2.306127) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274925 (2.304468) Accuracy: 0.125000 (0.110518)\n",
      "[50/63] Loss: 2.284468 (2.301779) Accuracy: 0.187500 (0.118260)\n",
      "[60/63] Loss: 2.300436 (2.302739) Accuracy: 0.156250 (0.117828)\n",
      "Epoch: 45/100, Train Loss: 2.2802, Train Acc: 0.1663, Val. Loss: 2.3026, Val. Acc: 0.1187\n",
      "[0/251] Loss: 2.318001 (2.318001) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.291546 (2.277887) Accuracy: 0.156250 (0.176136)\n",
      "[20/251] Loss: 2.254514 (2.285721) Accuracy: 0.250000 (0.154762)\n",
      "[30/251] Loss: 2.245257 (2.282242) Accuracy: 0.218750 (0.160282)\n",
      "[40/251] Loss: 2.256289 (2.281921) Accuracy: 0.187500 (0.160823)\n",
      "[50/251] Loss: 2.285916 (2.282251) Accuracy: 0.187500 (0.159926)\n",
      "[60/251] Loss: 2.271616 (2.281907) Accuracy: 0.125000 (0.161373)\n",
      "[70/251] Loss: 2.292258 (2.282360) Accuracy: 0.125000 (0.159771)\n",
      "[80/251] Loss: 2.288734 (2.283384) Accuracy: 0.156250 (0.158565)\n",
      "[90/251] Loss: 2.297484 (2.283138) Accuracy: 0.156250 (0.158310)\n",
      "[100/251] Loss: 2.243316 (2.283614) Accuracy: 0.250000 (0.157178)\n",
      "[110/251] Loss: 2.301511 (2.282934) Accuracy: 0.125000 (0.158502)\n",
      "[120/251] Loss: 2.270761 (2.282776) Accuracy: 0.187500 (0.158574)\n",
      "[130/251] Loss: 2.247196 (2.282651) Accuracy: 0.281250 (0.159351)\n",
      "[140/251] Loss: 2.294938 (2.282181) Accuracy: 0.187500 (0.160018)\n",
      "[150/251] Loss: 2.267861 (2.281948) Accuracy: 0.218750 (0.161010)\n",
      "[160/251] Loss: 2.274432 (2.282367) Accuracy: 0.187500 (0.159550)\n",
      "[170/251] Loss: 2.290600 (2.281918) Accuracy: 0.156250 (0.160636)\n",
      "[180/251] Loss: 2.260160 (2.281829) Accuracy: 0.218750 (0.160048)\n",
      "[190/251] Loss: 2.307079 (2.281564) Accuracy: 0.062500 (0.160013)\n",
      "[200/251] Loss: 2.299468 (2.281357) Accuracy: 0.187500 (0.161070)\n",
      "[210/251] Loss: 2.309007 (2.281434) Accuracy: 0.125000 (0.161582)\n",
      "[220/251] Loss: 2.273128 (2.280867) Accuracy: 0.156250 (0.162330)\n",
      "[230/251] Loss: 2.267464 (2.281091) Accuracy: 0.250000 (0.161797)\n",
      "[240/251] Loss: 2.287013 (2.281076) Accuracy: 0.156250 (0.162733)\n",
      "[250/251] Loss: 2.385484 (2.281284) Accuracy: 0.000000 (0.162475)\n",
      "[0/63] Loss: 2.301149 (2.301149) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312160 (2.302989) Accuracy: 0.125000 (0.130682)\n",
      "[20/63] Loss: 2.271649 (2.300740) Accuracy: 0.125000 (0.120536)\n",
      "[30/63] Loss: 2.278604 (2.306128) Accuracy: 0.125000 (0.105847)\n",
      "[40/63] Loss: 2.274806 (2.304461) Accuracy: 0.125000 (0.111280)\n",
      "[50/63] Loss: 2.284622 (2.301774) Accuracy: 0.187500 (0.118873)\n",
      "[60/63] Loss: 2.300574 (2.302742) Accuracy: 0.156250 (0.118340)\n",
      "Epoch: 46/100, Train Loss: 2.2813, Train Acc: 0.1625, Val. Loss: 2.3026, Val. Acc: 0.1192\n",
      "[0/251] Loss: 2.269573 (2.269573) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.265364 (2.280208) Accuracy: 0.187500 (0.150568)\n",
      "[20/251] Loss: 2.250402 (2.283257) Accuracy: 0.218750 (0.150298)\n",
      "[30/251] Loss: 2.218192 (2.282277) Accuracy: 0.281250 (0.158266)\n",
      "[40/251] Loss: 2.284368 (2.281430) Accuracy: 0.156250 (0.160823)\n",
      "[50/251] Loss: 2.275305 (2.282425) Accuracy: 0.218750 (0.162990)\n",
      "[60/251] Loss: 2.255845 (2.282171) Accuracy: 0.156250 (0.163422)\n",
      "[70/251] Loss: 2.283722 (2.279810) Accuracy: 0.156250 (0.167694)\n",
      "[80/251] Loss: 2.265834 (2.279666) Accuracy: 0.125000 (0.165123)\n",
      "[90/251] Loss: 2.272956 (2.279748) Accuracy: 0.218750 (0.166896)\n",
      "[100/251] Loss: 2.291271 (2.279568) Accuracy: 0.093750 (0.165532)\n",
      "[110/251] Loss: 2.298705 (2.281221) Accuracy: 0.187500 (0.161599)\n",
      "[120/251] Loss: 2.310020 (2.280962) Accuracy: 0.125000 (0.161932)\n",
      "[130/251] Loss: 2.279924 (2.279976) Accuracy: 0.125000 (0.164361)\n",
      "[140/251] Loss: 2.271996 (2.279753) Accuracy: 0.125000 (0.164450)\n",
      "[150/251] Loss: 2.272074 (2.279884) Accuracy: 0.156250 (0.164528)\n",
      "[160/251] Loss: 2.274179 (2.280520) Accuracy: 0.187500 (0.164014)\n",
      "[170/251] Loss: 2.274735 (2.279777) Accuracy: 0.187500 (0.165022)\n",
      "[180/251] Loss: 2.290808 (2.280580) Accuracy: 0.187500 (0.164537)\n",
      "[190/251] Loss: 2.293504 (2.280882) Accuracy: 0.125000 (0.163940)\n",
      "[200/251] Loss: 2.277333 (2.279795) Accuracy: 0.125000 (0.165267)\n",
      "[210/251] Loss: 2.263899 (2.280123) Accuracy: 0.218750 (0.164988)\n",
      "[220/251] Loss: 2.314374 (2.280979) Accuracy: 0.093750 (0.163886)\n",
      "[230/251] Loss: 2.269973 (2.281120) Accuracy: 0.156250 (0.162879)\n",
      "[240/251] Loss: 2.277896 (2.281076) Accuracy: 0.156250 (0.163122)\n",
      "[250/251] Loss: 2.371994 (2.281206) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301116 (2.301116) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.312173 (2.303000) Accuracy: 0.125000 (0.133523)\n",
      "[20/63] Loss: 2.271609 (2.300749) Accuracy: 0.125000 (0.122024)\n",
      "[30/63] Loss: 2.278610 (2.306137) Accuracy: 0.125000 (0.106855)\n",
      "[40/63] Loss: 2.274807 (2.304471) Accuracy: 0.125000 (0.111280)\n",
      "[50/63] Loss: 2.284623 (2.301780) Accuracy: 0.187500 (0.118873)\n",
      "[60/63] Loss: 2.300594 (2.302749) Accuracy: 0.156250 (0.118340)\n",
      "Epoch: 47/100, Train Loss: 2.2812, Train Acc: 0.1630, Val. Loss: 2.3026, Val. Acc: 0.1192\n",
      "[0/251] Loss: 2.313603 (2.313603) Accuracy: 0.000000 (0.000000)\n",
      "[10/251] Loss: 2.279933 (2.278973) Accuracy: 0.218750 (0.164773)\n",
      "[20/251] Loss: 2.311560 (2.276539) Accuracy: 0.156250 (0.165179)\n",
      "[30/251] Loss: 2.278075 (2.278075) Accuracy: 0.187500 (0.164315)\n",
      "[40/251] Loss: 2.289198 (2.276939) Accuracy: 0.156250 (0.162348)\n",
      "[50/251] Loss: 2.255212 (2.278460) Accuracy: 0.250000 (0.164216)\n",
      "[60/251] Loss: 2.314279 (2.279983) Accuracy: 0.093750 (0.161885)\n",
      "[70/251] Loss: 2.304431 (2.282649) Accuracy: 0.125000 (0.158891)\n",
      "[80/251] Loss: 2.281961 (2.279947) Accuracy: 0.156250 (0.163580)\n",
      "[90/251] Loss: 2.300491 (2.279626) Accuracy: 0.093750 (0.163462)\n",
      "[100/251] Loss: 2.290278 (2.279232) Accuracy: 0.125000 (0.164295)\n",
      "[110/251] Loss: 2.277522 (2.279538) Accuracy: 0.156250 (0.163007)\n",
      "[120/251] Loss: 2.269379 (2.280347) Accuracy: 0.187500 (0.164256)\n",
      "[130/251] Loss: 2.287017 (2.281080) Accuracy: 0.093750 (0.162691)\n",
      "[140/251] Loss: 2.291403 (2.281499) Accuracy: 0.156250 (0.161569)\n",
      "[150/251] Loss: 2.337902 (2.281420) Accuracy: 0.031250 (0.160803)\n",
      "[160/251] Loss: 2.271930 (2.281641) Accuracy: 0.187500 (0.161102)\n",
      "[170/251] Loss: 2.313021 (2.281555) Accuracy: 0.125000 (0.160636)\n",
      "[180/251] Loss: 2.263754 (2.281222) Accuracy: 0.218750 (0.161430)\n",
      "[190/251] Loss: 2.313962 (2.282099) Accuracy: 0.156250 (0.159195)\n",
      "[200/251] Loss: 2.297795 (2.282453) Accuracy: 0.156250 (0.157805)\n",
      "[210/251] Loss: 2.282133 (2.282370) Accuracy: 0.156250 (0.158175)\n",
      "[220/251] Loss: 2.329474 (2.281210) Accuracy: 0.093750 (0.161765)\n",
      "[230/251] Loss: 2.202768 (2.280949) Accuracy: 0.250000 (0.161526)\n",
      "[240/251] Loss: 2.239552 (2.280405) Accuracy: 0.281250 (0.162863)\n",
      "[250/251] Loss: 2.076776 (2.280041) Accuracy: 0.000000 (0.162102)\n",
      "[0/63] Loss: 2.301238 (2.301238) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312150 (2.303053) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271502 (2.300778) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278638 (2.306160) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274894 (2.304497) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284634 (2.301803) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300694 (2.302772) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 48/100, Train Loss: 2.2800, Train Acc: 0.1621, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.260280 (2.260280) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.197021 (2.265062) Accuracy: 0.375000 (0.196023)\n",
      "[20/251] Loss: 2.295832 (2.271406) Accuracy: 0.187500 (0.187500)\n",
      "[30/251] Loss: 2.276984 (2.272867) Accuracy: 0.187500 (0.179435)\n",
      "[40/251] Loss: 2.322013 (2.277206) Accuracy: 0.125000 (0.176067)\n",
      "[50/251] Loss: 2.252796 (2.280212) Accuracy: 0.125000 (0.169118)\n",
      "[60/251] Loss: 2.293888 (2.277897) Accuracy: 0.125000 (0.172643)\n",
      "[70/251] Loss: 2.275897 (2.278181) Accuracy: 0.156250 (0.172095)\n",
      "[80/251] Loss: 2.304371 (2.279157) Accuracy: 0.125000 (0.170139)\n",
      "[90/251] Loss: 2.316581 (2.279494) Accuracy: 0.093750 (0.167582)\n",
      "[100/251] Loss: 2.209202 (2.278570) Accuracy: 0.312500 (0.169554)\n",
      "[110/251] Loss: 2.290828 (2.278982) Accuracy: 0.187500 (0.168356)\n",
      "[120/251] Loss: 2.294005 (2.280073) Accuracy: 0.156250 (0.166064)\n",
      "[130/251] Loss: 2.267679 (2.280034) Accuracy: 0.156250 (0.164838)\n",
      "[140/251] Loss: 2.284471 (2.280684) Accuracy: 0.218750 (0.163342)\n",
      "[150/251] Loss: 2.266380 (2.279702) Accuracy: 0.250000 (0.166184)\n",
      "[160/251] Loss: 2.299006 (2.279506) Accuracy: 0.125000 (0.165955)\n",
      "[170/251] Loss: 2.272963 (2.279825) Accuracy: 0.125000 (0.164291)\n",
      "[180/251] Loss: 2.273904 (2.280055) Accuracy: 0.187500 (0.164019)\n",
      "[190/251] Loss: 2.272164 (2.280285) Accuracy: 0.187500 (0.162795)\n",
      "[200/251] Loss: 2.243039 (2.280283) Accuracy: 0.250000 (0.162624)\n",
      "[210/251] Loss: 2.317551 (2.280408) Accuracy: 0.125000 (0.162322)\n",
      "[220/251] Loss: 2.265114 (2.280004) Accuracy: 0.187500 (0.163462)\n",
      "[230/251] Loss: 2.294796 (2.280049) Accuracy: 0.125000 (0.163690)\n",
      "[240/251] Loss: 2.317238 (2.280656) Accuracy: 0.062500 (0.162733)\n",
      "[250/251] Loss: 2.327734 (2.280976) Accuracy: 0.000000 (0.162600)\n",
      "[0/63] Loss: 2.301191 (2.301191) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312119 (2.303013) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271527 (2.300768) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278545 (2.306147) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274831 (2.304481) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284575 (2.301787) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300605 (2.302762) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 49/100, Train Loss: 2.2810, Train Acc: 0.1626, Val. Loss: 2.3026, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.302363 (2.302363) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.241566 (2.274734) Accuracy: 0.250000 (0.170455)\n",
      "[20/251] Loss: 2.209640 (2.279599) Accuracy: 0.312500 (0.160714)\n",
      "[30/251] Loss: 2.286474 (2.277811) Accuracy: 0.125000 (0.164315)\n",
      "[40/251] Loss: 2.273459 (2.276713) Accuracy: 0.156250 (0.168445)\n",
      "[50/251] Loss: 2.285283 (2.278694) Accuracy: 0.156250 (0.164828)\n",
      "[60/251] Loss: 2.252894 (2.278345) Accuracy: 0.218750 (0.164959)\n",
      "[70/251] Loss: 2.298855 (2.278091) Accuracy: 0.125000 (0.167694)\n",
      "[80/251] Loss: 2.305351 (2.278715) Accuracy: 0.125000 (0.167052)\n",
      "[90/251] Loss: 2.270637 (2.279287) Accuracy: 0.250000 (0.166209)\n",
      "[100/251] Loss: 2.269898 (2.280266) Accuracy: 0.218750 (0.163057)\n",
      "[110/251] Loss: 2.288727 (2.279724) Accuracy: 0.156250 (0.165259)\n",
      "[120/251] Loss: 2.256408 (2.279701) Accuracy: 0.218750 (0.166839)\n",
      "[130/251] Loss: 2.255393 (2.279395) Accuracy: 0.250000 (0.168177)\n",
      "[140/251] Loss: 2.280228 (2.279934) Accuracy: 0.093750 (0.167110)\n",
      "[150/251] Loss: 2.298783 (2.279955) Accuracy: 0.218750 (0.166184)\n",
      "[160/251] Loss: 2.293905 (2.280322) Accuracy: 0.125000 (0.165955)\n",
      "[170/251] Loss: 2.276025 (2.279756) Accuracy: 0.218750 (0.166667)\n",
      "[180/251] Loss: 2.267212 (2.279558) Accuracy: 0.156250 (0.167472)\n",
      "[190/251] Loss: 2.285971 (2.279296) Accuracy: 0.218750 (0.166885)\n",
      "[200/251] Loss: 2.274255 (2.279533) Accuracy: 0.187500 (0.165889)\n",
      "[210/251] Loss: 2.251339 (2.279688) Accuracy: 0.218750 (0.165136)\n",
      "[220/251] Loss: 2.331605 (2.280030) Accuracy: 0.093750 (0.164876)\n",
      "[230/251] Loss: 2.311393 (2.280403) Accuracy: 0.062500 (0.163420)\n",
      "[240/251] Loss: 2.286223 (2.280687) Accuracy: 0.156250 (0.163252)\n",
      "[250/251] Loss: 2.406283 (2.281262) Accuracy: 0.000000 (0.162475)\n",
      "[0/63] Loss: 2.301195 (2.301195) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312138 (2.303020) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271522 (2.300773) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278539 (2.306154) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274829 (2.304488) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284556 (2.301791) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300625 (2.302765) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 50/100, Train Loss: 2.2813, Train Acc: 0.1625, Val. Loss: 2.3026, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.309654 (2.309654) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.320662 (2.294732) Accuracy: 0.031250 (0.127841)\n",
      "[20/251] Loss: 2.300120 (2.292550) Accuracy: 0.125000 (0.130952)\n",
      "[30/251] Loss: 2.300989 (2.287432) Accuracy: 0.093750 (0.145161)\n",
      "[40/251] Loss: 2.283301 (2.286328) Accuracy: 0.093750 (0.147104)\n",
      "[50/251] Loss: 2.292064 (2.282541) Accuracy: 0.187500 (0.156863)\n",
      "[60/251] Loss: 2.261497 (2.281598) Accuracy: 0.187500 (0.161885)\n",
      "[70/251] Loss: 2.299515 (2.281318) Accuracy: 0.093750 (0.164173)\n",
      "[80/251] Loss: 2.295062 (2.281399) Accuracy: 0.125000 (0.166667)\n",
      "[90/251] Loss: 2.247379 (2.281743) Accuracy: 0.250000 (0.164492)\n",
      "[100/251] Loss: 2.278926 (2.281447) Accuracy: 0.125000 (0.163985)\n",
      "[110/251] Loss: 2.260559 (2.281327) Accuracy: 0.156250 (0.163007)\n",
      "[120/251] Loss: 2.303886 (2.281490) Accuracy: 0.125000 (0.162707)\n",
      "[130/251] Loss: 2.273789 (2.282578) Accuracy: 0.156250 (0.161260)\n",
      "[140/251] Loss: 2.255828 (2.281685) Accuracy: 0.156250 (0.162456)\n",
      "[150/251] Loss: 2.276168 (2.281620) Accuracy: 0.187500 (0.163286)\n",
      "[160/251] Loss: 2.315259 (2.280769) Accuracy: 0.125000 (0.166343)\n",
      "[170/251] Loss: 2.294530 (2.280799) Accuracy: 0.156250 (0.166849)\n",
      "[180/251] Loss: 2.260266 (2.280622) Accuracy: 0.281250 (0.167300)\n",
      "[190/251] Loss: 2.265369 (2.280793) Accuracy: 0.093750 (0.165576)\n",
      "[200/251] Loss: 2.287179 (2.280437) Accuracy: 0.187500 (0.165734)\n",
      "[210/251] Loss: 2.261655 (2.280739) Accuracy: 0.156250 (0.164396)\n",
      "[220/251] Loss: 2.280056 (2.280044) Accuracy: 0.187500 (0.165724)\n",
      "[230/251] Loss: 2.269275 (2.280822) Accuracy: 0.218750 (0.163826)\n",
      "[240/251] Loss: 2.284576 (2.280876) Accuracy: 0.156250 (0.163641)\n",
      "[250/251] Loss: 2.336335 (2.280974) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301224 (2.301224) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312143 (2.303011) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271499 (2.300772) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278510 (2.306155) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274825 (2.304487) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284542 (2.301789) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300607 (2.302764) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 51/100, Train Loss: 2.2810, Train Acc: 0.1628, Val. Loss: 2.3026, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.295442 (2.295442) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.298701 (2.285468) Accuracy: 0.125000 (0.142045)\n",
      "[20/251] Loss: 2.264057 (2.282813) Accuracy: 0.250000 (0.165179)\n",
      "[30/251] Loss: 2.290715 (2.281985) Accuracy: 0.156250 (0.168347)\n",
      "[40/251] Loss: 2.301999 (2.279454) Accuracy: 0.093750 (0.169970)\n",
      "[50/251] Loss: 2.277848 (2.277555) Accuracy: 0.218750 (0.171569)\n",
      "[60/251] Loss: 2.305553 (2.278093) Accuracy: 0.125000 (0.171107)\n",
      "[70/251] Loss: 2.290376 (2.278786) Accuracy: 0.156250 (0.168574)\n",
      "[80/251] Loss: 2.308038 (2.280068) Accuracy: 0.156250 (0.167824)\n",
      "[90/251] Loss: 2.277851 (2.279613) Accuracy: 0.218750 (0.168269)\n",
      "[100/251] Loss: 2.321604 (2.280395) Accuracy: 0.093750 (0.165223)\n",
      "[110/251] Loss: 2.287481 (2.280850) Accuracy: 0.125000 (0.162162)\n",
      "[120/251] Loss: 2.288291 (2.281564) Accuracy: 0.125000 (0.160382)\n",
      "[130/251] Loss: 2.303386 (2.281027) Accuracy: 0.031250 (0.161260)\n",
      "[140/251] Loss: 2.282729 (2.280638) Accuracy: 0.156250 (0.162012)\n",
      "[150/251] Loss: 2.301915 (2.281217) Accuracy: 0.093750 (0.160389)\n",
      "[160/251] Loss: 2.254979 (2.280874) Accuracy: 0.250000 (0.160908)\n",
      "[170/251] Loss: 2.313643 (2.280424) Accuracy: 0.125000 (0.162646)\n",
      "[180/251] Loss: 2.267466 (2.280375) Accuracy: 0.218750 (0.162983)\n",
      "[190/251] Loss: 2.253890 (2.280433) Accuracy: 0.156250 (0.163285)\n",
      "[200/251] Loss: 2.238032 (2.279874) Accuracy: 0.281250 (0.165889)\n",
      "[210/251] Loss: 2.265306 (2.279968) Accuracy: 0.218750 (0.166321)\n",
      "[220/251] Loss: 2.308783 (2.280076) Accuracy: 0.062500 (0.165158)\n",
      "[230/251] Loss: 2.260630 (2.280179) Accuracy: 0.218750 (0.165043)\n",
      "[240/251] Loss: 2.249306 (2.280361) Accuracy: 0.218750 (0.165067)\n",
      "[250/251] Loss: 2.397044 (2.281188) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301207 (2.301207) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312141 (2.303003) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271511 (2.300773) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278495 (2.306157) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274792 (2.304486) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284528 (2.301787) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300599 (2.302764) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 52/100, Train Loss: 2.2812, Train Acc: 0.1630, Val. Loss: 2.3026, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.257423 (2.257423) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.279757 (2.279637) Accuracy: 0.156250 (0.159091)\n",
      "[20/251] Loss: 2.263306 (2.269195) Accuracy: 0.218750 (0.177083)\n",
      "[30/251] Loss: 2.295393 (2.268878) Accuracy: 0.156250 (0.185484)\n",
      "[40/251] Loss: 2.193474 (2.267591) Accuracy: 0.375000 (0.189024)\n",
      "[50/251] Loss: 2.289839 (2.271241) Accuracy: 0.125000 (0.178922)\n",
      "[60/251] Loss: 2.280587 (2.274691) Accuracy: 0.125000 (0.170082)\n",
      "[70/251] Loss: 2.253616 (2.275617) Accuracy: 0.125000 (0.167254)\n",
      "[80/251] Loss: 2.225716 (2.276296) Accuracy: 0.437500 (0.169367)\n",
      "[90/251] Loss: 2.273271 (2.278068) Accuracy: 0.250000 (0.166552)\n",
      "[100/251] Loss: 2.292706 (2.278202) Accuracy: 0.156250 (0.166151)\n",
      "[110/251] Loss: 2.250924 (2.277757) Accuracy: 0.250000 (0.166667)\n",
      "[120/251] Loss: 2.253057 (2.277939) Accuracy: 0.187500 (0.166322)\n",
      "[130/251] Loss: 2.306952 (2.278429) Accuracy: 0.125000 (0.165076)\n",
      "[140/251] Loss: 2.285284 (2.277882) Accuracy: 0.125000 (0.167110)\n",
      "[150/251] Loss: 2.241178 (2.277334) Accuracy: 0.375000 (0.169081)\n",
      "[160/251] Loss: 2.287751 (2.278351) Accuracy: 0.093750 (0.166537)\n",
      "[170/251] Loss: 2.335372 (2.279044) Accuracy: 0.031250 (0.165022)\n",
      "[180/251] Loss: 2.295312 (2.278853) Accuracy: 0.187500 (0.166091)\n",
      "[190/251] Loss: 2.275021 (2.278852) Accuracy: 0.156250 (0.167376)\n",
      "[200/251] Loss: 2.291834 (2.279295) Accuracy: 0.187500 (0.167133)\n",
      "[210/251] Loss: 2.241647 (2.279424) Accuracy: 0.218750 (0.166321)\n",
      "[220/251] Loss: 2.255709 (2.279564) Accuracy: 0.218750 (0.165583)\n",
      "[230/251] Loss: 2.293842 (2.280276) Accuracy: 0.156250 (0.164096)\n",
      "[240/251] Loss: 2.341289 (2.280949) Accuracy: 0.031250 (0.162474)\n",
      "[250/251] Loss: 2.379925 (2.281106) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301210 (2.301210) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312137 (2.303001) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271519 (2.300774) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278486 (2.306158) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274774 (2.304486) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284517 (2.301785) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300610 (2.302765) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 53/100, Train Loss: 2.2811, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.240199 (2.240199) Accuracy: 0.281250 (0.281250)\n",
      "[10/251] Loss: 2.284492 (2.270407) Accuracy: 0.187500 (0.196023)\n",
      "[20/251] Loss: 2.296534 (2.276577) Accuracy: 0.093750 (0.172619)\n",
      "[30/251] Loss: 2.325310 (2.276201) Accuracy: 0.062500 (0.173387)\n",
      "[40/251] Loss: 2.342249 (2.278397) Accuracy: 0.062500 (0.171494)\n",
      "[50/251] Loss: 2.265249 (2.278031) Accuracy: 0.187500 (0.170956)\n",
      "[60/251] Loss: 2.243477 (2.276484) Accuracy: 0.218750 (0.172131)\n",
      "[70/251] Loss: 2.340074 (2.278300) Accuracy: 0.000000 (0.168134)\n",
      "[80/251] Loss: 2.308022 (2.278524) Accuracy: 0.156250 (0.167438)\n",
      "[90/251] Loss: 2.298582 (2.275793) Accuracy: 0.156250 (0.173420)\n",
      "[100/251] Loss: 2.239922 (2.276773) Accuracy: 0.250000 (0.171411)\n",
      "[110/251] Loss: 2.256988 (2.277802) Accuracy: 0.218750 (0.170890)\n",
      "[120/251] Loss: 2.283084 (2.277972) Accuracy: 0.156250 (0.170196)\n",
      "[130/251] Loss: 2.268643 (2.278287) Accuracy: 0.218750 (0.169370)\n",
      "[140/251] Loss: 2.302652 (2.278965) Accuracy: 0.062500 (0.168661)\n",
      "[150/251] Loss: 2.284935 (2.278678) Accuracy: 0.125000 (0.169081)\n",
      "[160/251] Loss: 2.284797 (2.278086) Accuracy: 0.156250 (0.169837)\n",
      "[170/251] Loss: 2.298598 (2.279020) Accuracy: 0.125000 (0.167763)\n",
      "[180/251] Loss: 2.305533 (2.279512) Accuracy: 0.062500 (0.166609)\n",
      "[190/251] Loss: 2.268124 (2.279556) Accuracy: 0.156250 (0.166394)\n",
      "[200/251] Loss: 2.265401 (2.280178) Accuracy: 0.218750 (0.165423)\n",
      "[210/251] Loss: 2.252751 (2.280000) Accuracy: 0.187500 (0.165432)\n",
      "[220/251] Loss: 2.286555 (2.279996) Accuracy: 0.156250 (0.165865)\n",
      "[230/251] Loss: 2.249201 (2.280587) Accuracy: 0.187500 (0.164232)\n",
      "[240/251] Loss: 2.319010 (2.280451) Accuracy: 0.000000 (0.164289)\n",
      "[250/251] Loss: 2.297076 (2.280771) Accuracy: 0.000000 (0.162724)\n",
      "[0/63] Loss: 2.301157 (2.301157) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312150 (2.303000) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271485 (2.300770) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278492 (2.306155) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274798 (2.304483) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284561 (2.301780) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300666 (2.302760) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 54/100, Train Loss: 2.2808, Train Acc: 0.1627, Val. Loss: 2.3026, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.278299 (2.278299) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.270389 (2.285744) Accuracy: 0.218750 (0.159091)\n",
      "[20/251] Loss: 2.244781 (2.280030) Accuracy: 0.156250 (0.163690)\n",
      "[30/251] Loss: 2.252784 (2.281422) Accuracy: 0.250000 (0.163306)\n",
      "[40/251] Loss: 2.177254 (2.276769) Accuracy: 0.312500 (0.173018)\n",
      "[50/251] Loss: 2.281371 (2.276197) Accuracy: 0.187500 (0.177696)\n",
      "[60/251] Loss: 2.299805 (2.277327) Accuracy: 0.062500 (0.173668)\n",
      "[70/251] Loss: 2.287558 (2.279350) Accuracy: 0.187500 (0.169014)\n",
      "[80/251] Loss: 2.295341 (2.279878) Accuracy: 0.187500 (0.168210)\n",
      "[90/251] Loss: 2.268002 (2.279398) Accuracy: 0.187500 (0.168269)\n",
      "[100/251] Loss: 2.287201 (2.279105) Accuracy: 0.062500 (0.166770)\n",
      "[110/251] Loss: 2.270681 (2.278817) Accuracy: 0.156250 (0.166385)\n",
      "[120/251] Loss: 2.246968 (2.278054) Accuracy: 0.187500 (0.167614)\n",
      "[130/251] Loss: 2.308672 (2.278566) Accuracy: 0.093750 (0.168416)\n",
      "[140/251] Loss: 2.280823 (2.278655) Accuracy: 0.156250 (0.168218)\n",
      "[150/251] Loss: 2.259506 (2.278360) Accuracy: 0.218750 (0.168046)\n",
      "[160/251] Loss: 2.259663 (2.278505) Accuracy: 0.218750 (0.167896)\n",
      "[170/251] Loss: 2.288627 (2.279143) Accuracy: 0.093750 (0.166484)\n",
      "[180/251] Loss: 2.311762 (2.279363) Accuracy: 0.062500 (0.165919)\n",
      "[190/251] Loss: 2.331161 (2.279570) Accuracy: 0.031250 (0.165085)\n",
      "[200/251] Loss: 2.277706 (2.279669) Accuracy: 0.218750 (0.165267)\n",
      "[210/251] Loss: 2.297915 (2.280311) Accuracy: 0.125000 (0.163803)\n",
      "[220/251] Loss: 2.234715 (2.280767) Accuracy: 0.281250 (0.162896)\n",
      "[230/251] Loss: 2.250638 (2.280874) Accuracy: 0.187500 (0.163285)\n",
      "[240/251] Loss: 2.241975 (2.280789) Accuracy: 0.218750 (0.162993)\n",
      "[250/251] Loss: 2.235496 (2.280518) Accuracy: 0.000000 (0.163098)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312152 (2.303008) Accuracy: 0.125000 (0.125000)\n",
      "[20/63] Loss: 2.271471 (2.300779) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278517 (2.306167) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274755 (2.304495) Accuracy: 0.125000 (0.108994)\n",
      "[50/63] Loss: 2.284528 (2.301790) Accuracy: 0.187500 (0.117034)\n",
      "[60/63] Loss: 2.300675 (2.302771) Accuracy: 0.156250 (0.116291)\n",
      "Epoch: 55/100, Train Loss: 2.2805, Train Acc: 0.1631, Val. Loss: 2.3027, Val. Acc: 0.1172\n",
      "[0/251] Loss: 2.311033 (2.311033) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.286832 (2.283944) Accuracy: 0.156250 (0.147727)\n",
      "[20/251] Loss: 2.261521 (2.277843) Accuracy: 0.281250 (0.171131)\n",
      "[30/251] Loss: 2.284944 (2.277581) Accuracy: 0.156250 (0.170363)\n",
      "[40/251] Loss: 2.249220 (2.279268) Accuracy: 0.218750 (0.164634)\n",
      "[50/251] Loss: 2.285987 (2.280207) Accuracy: 0.218750 (0.164828)\n",
      "[60/251] Loss: 2.301339 (2.279886) Accuracy: 0.093750 (0.164447)\n",
      "[70/251] Loss: 2.286718 (2.281041) Accuracy: 0.093750 (0.159331)\n",
      "[80/251] Loss: 2.273234 (2.281291) Accuracy: 0.156250 (0.158565)\n",
      "[90/251] Loss: 2.272110 (2.281054) Accuracy: 0.281250 (0.160027)\n",
      "[100/251] Loss: 2.257473 (2.279814) Accuracy: 0.187500 (0.162748)\n",
      "[110/251] Loss: 2.199754 (2.280420) Accuracy: 0.312500 (0.163570)\n",
      "[120/251] Loss: 2.320216 (2.280181) Accuracy: 0.031250 (0.164256)\n",
      "[130/251] Loss: 2.259094 (2.280094) Accuracy: 0.218750 (0.168177)\n",
      "[140/251] Loss: 2.281909 (2.279821) Accuracy: 0.187500 (0.167110)\n",
      "[150/251] Loss: 2.272604 (2.280304) Accuracy: 0.187500 (0.166598)\n",
      "[160/251] Loss: 2.220897 (2.281088) Accuracy: 0.250000 (0.164402)\n",
      "[170/251] Loss: 2.270911 (2.281363) Accuracy: 0.218750 (0.164474)\n",
      "[180/251] Loss: 2.271636 (2.281476) Accuracy: 0.156250 (0.163674)\n",
      "[190/251] Loss: 2.259730 (2.281906) Accuracy: 0.187500 (0.161976)\n",
      "[200/251] Loss: 2.269287 (2.281590) Accuracy: 0.156250 (0.162624)\n",
      "[210/251] Loss: 2.278535 (2.281878) Accuracy: 0.156250 (0.161878)\n",
      "[220/251] Loss: 2.235531 (2.281325) Accuracy: 0.187500 (0.163037)\n",
      "[230/251] Loss: 2.297068 (2.281618) Accuracy: 0.156250 (0.162473)\n",
      "[240/251] Loss: 2.286552 (2.281048) Accuracy: 0.093750 (0.163382)\n",
      "[250/251] Loss: 2.127115 (2.280087) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301169 (2.301169) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312155 (2.303000) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271445 (2.300778) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278494 (2.306169) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274749 (2.304494) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284521 (2.301789) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300633 (2.302771) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 56/100, Train Loss: 2.2801, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.248029 (2.248029) Accuracy: 0.281250 (0.281250)\n",
      "[10/251] Loss: 2.297956 (2.280693) Accuracy: 0.093750 (0.167614)\n",
      "[20/251] Loss: 2.203088 (2.278970) Accuracy: 0.281250 (0.166667)\n",
      "[30/251] Loss: 2.268138 (2.282078) Accuracy: 0.187500 (0.161290)\n",
      "[40/251] Loss: 2.274662 (2.278972) Accuracy: 0.156250 (0.170732)\n",
      "[50/251] Loss: 2.318120 (2.280414) Accuracy: 0.062500 (0.165441)\n",
      "[60/251] Loss: 2.300638 (2.281043) Accuracy: 0.125000 (0.161885)\n",
      "[70/251] Loss: 2.302709 (2.279177) Accuracy: 0.187500 (0.167254)\n",
      "[80/251] Loss: 2.288401 (2.279959) Accuracy: 0.093750 (0.164738)\n",
      "[90/251] Loss: 2.276126 (2.280688) Accuracy: 0.156250 (0.163462)\n",
      "[100/251] Loss: 2.265570 (2.281677) Accuracy: 0.187500 (0.162129)\n",
      "[110/251] Loss: 2.289487 (2.281718) Accuracy: 0.156250 (0.163007)\n",
      "[120/251] Loss: 2.238622 (2.281514) Accuracy: 0.218750 (0.163223)\n",
      "[130/251] Loss: 2.265447 (2.281515) Accuracy: 0.218750 (0.164122)\n",
      "[140/251] Loss: 2.231252 (2.280901) Accuracy: 0.218750 (0.164229)\n",
      "[150/251] Loss: 2.274752 (2.281227) Accuracy: 0.250000 (0.163079)\n",
      "[160/251] Loss: 2.285664 (2.281114) Accuracy: 0.156250 (0.163432)\n",
      "[170/251] Loss: 2.268964 (2.280772) Accuracy: 0.187500 (0.163743)\n",
      "[180/251] Loss: 2.290084 (2.281585) Accuracy: 0.187500 (0.161775)\n",
      "[190/251] Loss: 2.303793 (2.281196) Accuracy: 0.187500 (0.163122)\n",
      "[200/251] Loss: 2.322360 (2.281209) Accuracy: 0.062500 (0.163402)\n",
      "[210/251] Loss: 2.293780 (2.281386) Accuracy: 0.156250 (0.164100)\n",
      "[220/251] Loss: 2.293059 (2.281080) Accuracy: 0.125000 (0.164027)\n",
      "[230/251] Loss: 2.297314 (2.280621) Accuracy: 0.093750 (0.163285)\n",
      "[240/251] Loss: 2.258597 (2.280802) Accuracy: 0.187500 (0.163252)\n",
      "[250/251] Loss: 2.368055 (2.281006) Accuracy: 0.000000 (0.162724)\n",
      "[0/63] Loss: 2.301169 (2.301169) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312157 (2.302999) Accuracy: 0.125000 (0.130682)\n",
      "[20/63] Loss: 2.271439 (2.300778) Accuracy: 0.125000 (0.120536)\n",
      "[30/63] Loss: 2.278487 (2.306168) Accuracy: 0.125000 (0.105847)\n",
      "[40/63] Loss: 2.274734 (2.304493) Accuracy: 0.125000 (0.110518)\n",
      "[50/63] Loss: 2.284515 (2.301787) Accuracy: 0.187500 (0.118260)\n",
      "[60/63] Loss: 2.300648 (2.302771) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 57/100, Train Loss: 2.2810, Train Acc: 0.1627, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.308566 (2.308566) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.292982 (2.281939) Accuracy: 0.156250 (0.144886)\n",
      "[20/251] Loss: 2.272555 (2.276320) Accuracy: 0.125000 (0.172619)\n",
      "[30/251] Loss: 2.292669 (2.276170) Accuracy: 0.093750 (0.172379)\n",
      "[40/251] Loss: 2.255396 (2.275752) Accuracy: 0.312500 (0.176067)\n",
      "[50/251] Loss: 2.253736 (2.277989) Accuracy: 0.218750 (0.168505)\n",
      "[60/251] Loss: 2.293369 (2.278994) Accuracy: 0.125000 (0.164447)\n",
      "[70/251] Loss: 2.309462 (2.280277) Accuracy: 0.062500 (0.160211)\n",
      "[80/251] Loss: 2.305666 (2.280596) Accuracy: 0.093750 (0.159722)\n",
      "[90/251] Loss: 2.263531 (2.281198) Accuracy: 0.187500 (0.158310)\n",
      "[100/251] Loss: 2.328403 (2.281298) Accuracy: 0.125000 (0.159035)\n",
      "[110/251] Loss: 2.285804 (2.281569) Accuracy: 0.156250 (0.159628)\n",
      "[120/251] Loss: 2.236681 (2.280781) Accuracy: 0.250000 (0.161157)\n",
      "[130/251] Loss: 2.239120 (2.280889) Accuracy: 0.218750 (0.161021)\n",
      "[140/251] Loss: 2.335901 (2.281379) Accuracy: 0.093750 (0.160239)\n",
      "[150/251] Loss: 2.284189 (2.281428) Accuracy: 0.187500 (0.159354)\n",
      "[160/251] Loss: 2.274299 (2.281328) Accuracy: 0.218750 (0.160520)\n",
      "[170/251] Loss: 2.250787 (2.281749) Accuracy: 0.250000 (0.160453)\n",
      "[180/251] Loss: 2.261692 (2.281665) Accuracy: 0.187500 (0.161775)\n",
      "[190/251] Loss: 2.306827 (2.281981) Accuracy: 0.156250 (0.160831)\n",
      "[200/251] Loss: 2.287851 (2.281514) Accuracy: 0.156250 (0.162002)\n",
      "[210/251] Loss: 2.263074 (2.281469) Accuracy: 0.156250 (0.162322)\n",
      "[220/251] Loss: 2.323094 (2.280591) Accuracy: 0.062500 (0.163744)\n",
      "[230/251] Loss: 2.257426 (2.280440) Accuracy: 0.218750 (0.163961)\n",
      "[240/251] Loss: 2.293108 (2.280687) Accuracy: 0.093750 (0.163641)\n",
      "[250/251] Loss: 2.330143 (2.280851) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301161 (2.301161) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312147 (2.302989) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271448 (2.300775) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278508 (2.306166) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274712 (2.304488) Accuracy: 0.125000 (0.108994)\n",
      "[50/63] Loss: 2.284523 (2.301781) Accuracy: 0.187500 (0.117034)\n",
      "[60/63] Loss: 2.300630 (2.302769) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 58/100, Train Loss: 2.2809, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.238531 (2.238531) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.310991 (2.276133) Accuracy: 0.125000 (0.167614)\n",
      "[20/251] Loss: 2.293709 (2.277534) Accuracy: 0.125000 (0.159226)\n",
      "[30/251] Loss: 2.269907 (2.278036) Accuracy: 0.156250 (0.159274)\n",
      "[40/251] Loss: 2.277444 (2.279738) Accuracy: 0.250000 (0.157012)\n",
      "[50/251] Loss: 2.285108 (2.280979) Accuracy: 0.156250 (0.153186)\n",
      "[60/251] Loss: 2.313228 (2.280750) Accuracy: 0.031250 (0.151127)\n",
      "[70/251] Loss: 2.225382 (2.279510) Accuracy: 0.156250 (0.154049)\n",
      "[80/251] Loss: 2.314968 (2.278936) Accuracy: 0.125000 (0.157407)\n",
      "[90/251] Loss: 2.274751 (2.279376) Accuracy: 0.156250 (0.156937)\n",
      "[100/251] Loss: 2.307642 (2.279583) Accuracy: 0.093750 (0.157488)\n",
      "[110/251] Loss: 2.297699 (2.280687) Accuracy: 0.187500 (0.156813)\n",
      "[120/251] Loss: 2.310659 (2.281307) Accuracy: 0.156250 (0.157283)\n",
      "[130/251] Loss: 2.258540 (2.280906) Accuracy: 0.125000 (0.159113)\n",
      "[140/251] Loss: 2.271923 (2.281435) Accuracy: 0.187500 (0.157358)\n",
      "[150/251] Loss: 2.293338 (2.281208) Accuracy: 0.187500 (0.159561)\n",
      "[160/251] Loss: 2.326745 (2.281960) Accuracy: 0.031250 (0.158579)\n",
      "[170/251] Loss: 2.323812 (2.281936) Accuracy: 0.093750 (0.158443)\n",
      "[180/251] Loss: 2.296483 (2.281262) Accuracy: 0.125000 (0.160394)\n",
      "[190/251] Loss: 2.270319 (2.280875) Accuracy: 0.250000 (0.161813)\n",
      "[200/251] Loss: 2.268373 (2.280655) Accuracy: 0.218750 (0.162158)\n",
      "[210/251] Loss: 2.294783 (2.280722) Accuracy: 0.125000 (0.162470)\n",
      "[220/251] Loss: 2.306841 (2.280528) Accuracy: 0.125000 (0.163179)\n",
      "[230/251] Loss: 2.272500 (2.280262) Accuracy: 0.156250 (0.163690)\n",
      "[240/251] Loss: 2.295494 (2.280457) Accuracy: 0.187500 (0.163511)\n",
      "[250/251] Loss: 2.032258 (2.279692) Accuracy: 1.000000 (0.166708)\n",
      "[0/63] Loss: 2.301193 (2.301193) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312149 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271378 (2.300779) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278516 (2.306170) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274734 (2.304492) Accuracy: 0.125000 (0.108994)\n",
      "[50/63] Loss: 2.284532 (2.301783) Accuracy: 0.187500 (0.117034)\n",
      "[60/63] Loss: 2.300655 (2.302770) Accuracy: 0.156250 (0.116291)\n",
      "Epoch: 59/100, Train Loss: 2.2797, Train Acc: 0.1667, Val. Loss: 2.3027, Val. Acc: 0.1172\n",
      "[0/251] Loss: 2.270114 (2.270114) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.311579 (2.267523) Accuracy: 0.093750 (0.187500)\n",
      "[20/251] Loss: 2.331249 (2.274972) Accuracy: 0.031250 (0.172619)\n",
      "[30/251] Loss: 2.316808 (2.278799) Accuracy: 0.093750 (0.166331)\n",
      "[40/251] Loss: 2.300183 (2.279987) Accuracy: 0.218750 (0.170732)\n",
      "[50/251] Loss: 2.264600 (2.279353) Accuracy: 0.156250 (0.169730)\n",
      "[60/251] Loss: 2.294758 (2.279048) Accuracy: 0.156250 (0.172131)\n",
      "[70/251] Loss: 2.308711 (2.280350) Accuracy: 0.125000 (0.168574)\n",
      "[80/251] Loss: 2.262838 (2.278192) Accuracy: 0.218750 (0.175154)\n",
      "[90/251] Loss: 2.315813 (2.279073) Accuracy: 0.093750 (0.172734)\n",
      "[100/251] Loss: 2.294551 (2.278891) Accuracy: 0.156250 (0.173577)\n",
      "[110/251] Loss: 2.243802 (2.278993) Accuracy: 0.218750 (0.172579)\n",
      "[120/251] Loss: 2.296324 (2.278875) Accuracy: 0.093750 (0.172004)\n",
      "[130/251] Loss: 2.249686 (2.279231) Accuracy: 0.125000 (0.170324)\n",
      "[140/251] Loss: 2.240914 (2.279068) Accuracy: 0.375000 (0.170656)\n",
      "[150/251] Loss: 2.343600 (2.280111) Accuracy: 0.031250 (0.168460)\n",
      "[160/251] Loss: 2.294945 (2.280770) Accuracy: 0.187500 (0.166731)\n",
      "[170/251] Loss: 2.274009 (2.280668) Accuracy: 0.187500 (0.166118)\n",
      "[180/251] Loss: 2.233634 (2.280182) Accuracy: 0.281250 (0.166954)\n",
      "[190/251] Loss: 2.258937 (2.280531) Accuracy: 0.218750 (0.165740)\n",
      "[200/251] Loss: 2.305395 (2.281054) Accuracy: 0.125000 (0.164024)\n",
      "[210/251] Loss: 2.285949 (2.280524) Accuracy: 0.125000 (0.164544)\n",
      "[220/251] Loss: 2.253599 (2.280355) Accuracy: 0.250000 (0.165017)\n",
      "[230/251] Loss: 2.241615 (2.280909) Accuracy: 0.187500 (0.162744)\n",
      "[240/251] Loss: 2.221192 (2.280695) Accuracy: 0.218750 (0.163771)\n",
      "[250/251] Loss: 2.186130 (2.280275) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301199 (2.301199) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312130 (2.302993) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271445 (2.300790) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278553 (2.306185) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274654 (2.304499) Accuracy: 0.125000 (0.108994)\n",
      "[50/63] Loss: 2.284498 (2.301790) Accuracy: 0.187500 (0.117034)\n",
      "[60/63] Loss: 2.300627 (2.302777) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 60/100, Train Loss: 2.2803, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.267205 (2.267205) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.294516 (2.275898) Accuracy: 0.156250 (0.173295)\n",
      "[20/251] Loss: 2.339225 (2.278051) Accuracy: 0.062500 (0.171131)\n",
      "[30/251] Loss: 2.236033 (2.282286) Accuracy: 0.218750 (0.160282)\n",
      "[40/251] Loss: 2.305262 (2.284321) Accuracy: 0.093750 (0.151677)\n",
      "[50/251] Loss: 2.307917 (2.282769) Accuracy: 0.156250 (0.157475)\n",
      "[60/251] Loss: 2.312623 (2.282790) Accuracy: 0.125000 (0.161885)\n",
      "[70/251] Loss: 2.260107 (2.282239) Accuracy: 0.156250 (0.165053)\n",
      "[80/251] Loss: 2.269875 (2.281416) Accuracy: 0.218750 (0.165895)\n",
      "[90/251] Loss: 2.306685 (2.282030) Accuracy: 0.125000 (0.163462)\n",
      "[100/251] Loss: 2.227522 (2.281861) Accuracy: 0.312500 (0.163985)\n",
      "[110/251] Loss: 2.265668 (2.281829) Accuracy: 0.218750 (0.163851)\n",
      "[120/251] Loss: 2.243797 (2.281578) Accuracy: 0.218750 (0.163223)\n",
      "[130/251] Loss: 2.285263 (2.281868) Accuracy: 0.125000 (0.162452)\n",
      "[140/251] Loss: 2.298637 (2.281056) Accuracy: 0.062500 (0.163785)\n",
      "[150/251] Loss: 2.265116 (2.281145) Accuracy: 0.187500 (0.162666)\n",
      "[160/251] Loss: 2.263930 (2.281244) Accuracy: 0.125000 (0.162655)\n",
      "[170/251] Loss: 2.275236 (2.281320) Accuracy: 0.156250 (0.161732)\n",
      "[180/251] Loss: 2.307644 (2.281480) Accuracy: 0.125000 (0.162638)\n",
      "[190/251] Loss: 2.207078 (2.281108) Accuracy: 0.312500 (0.163613)\n",
      "[200/251] Loss: 2.269973 (2.280929) Accuracy: 0.218750 (0.162780)\n",
      "[210/251] Loss: 2.254944 (2.280477) Accuracy: 0.187500 (0.163655)\n",
      "[220/251] Loss: 2.307858 (2.280260) Accuracy: 0.093750 (0.164876)\n",
      "[230/251] Loss: 2.290454 (2.280441) Accuracy: 0.125000 (0.164232)\n",
      "[240/251] Loss: 2.304535 (2.280582) Accuracy: 0.093750 (0.163771)\n",
      "[250/251] Loss: 2.081203 (2.279863) Accuracy: 1.000000 (0.166708)\n",
      "[0/63] Loss: 2.301164 (2.301164) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312138 (2.302989) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271419 (2.300786) Accuracy: 0.125000 (0.117560)\n",
      "[30/63] Loss: 2.278544 (2.306181) Accuracy: 0.125000 (0.103831)\n",
      "[40/63] Loss: 2.274666 (2.304496) Accuracy: 0.125000 (0.108994)\n",
      "[50/63] Loss: 2.284520 (2.301787) Accuracy: 0.187500 (0.117034)\n",
      "[60/63] Loss: 2.300666 (2.302775) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 61/100, Train Loss: 2.2799, Train Acc: 0.1667, Val. Loss: 2.3027, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.285654 (2.285654) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.270320 (2.279181) Accuracy: 0.218750 (0.173295)\n",
      "[20/251] Loss: 2.232943 (2.276060) Accuracy: 0.281250 (0.180060)\n",
      "[30/251] Loss: 2.305080 (2.275811) Accuracy: 0.125000 (0.180444)\n",
      "[40/251] Loss: 2.315239 (2.275874) Accuracy: 0.156250 (0.184451)\n",
      "[50/251] Loss: 2.282275 (2.275390) Accuracy: 0.156250 (0.182598)\n",
      "[60/251] Loss: 2.265971 (2.276025) Accuracy: 0.218750 (0.179816)\n",
      "[70/251] Loss: 2.271477 (2.275738) Accuracy: 0.156250 (0.179577)\n",
      "[80/251] Loss: 2.299932 (2.278909) Accuracy: 0.125000 (0.170910)\n",
      "[90/251] Loss: 2.239121 (2.280276) Accuracy: 0.156250 (0.166552)\n",
      "[100/251] Loss: 2.322729 (2.280725) Accuracy: 0.031250 (0.165842)\n",
      "[110/251] Loss: 2.254412 (2.280688) Accuracy: 0.218750 (0.164696)\n",
      "[120/251] Loss: 2.310474 (2.280312) Accuracy: 0.093750 (0.164514)\n",
      "[130/251] Loss: 2.283852 (2.281795) Accuracy: 0.062500 (0.161737)\n",
      "[140/251] Loss: 2.253804 (2.281896) Accuracy: 0.218750 (0.160904)\n",
      "[150/251] Loss: 2.279133 (2.281157) Accuracy: 0.156250 (0.162873)\n",
      "[160/251] Loss: 2.303758 (2.280529) Accuracy: 0.156250 (0.164208)\n",
      "[170/251] Loss: 2.303451 (2.280178) Accuracy: 0.093750 (0.164839)\n",
      "[180/251] Loss: 2.307624 (2.280072) Accuracy: 0.062500 (0.163847)\n",
      "[190/251] Loss: 2.223389 (2.279424) Accuracy: 0.312500 (0.165740)\n",
      "[200/251] Loss: 2.274238 (2.279767) Accuracy: 0.187500 (0.165423)\n",
      "[210/251] Loss: 2.256720 (2.280005) Accuracy: 0.187500 (0.164544)\n",
      "[220/251] Loss: 2.319422 (2.279863) Accuracy: 0.031250 (0.164593)\n",
      "[230/251] Loss: 2.278413 (2.280202) Accuracy: 0.156250 (0.163690)\n",
      "[240/251] Loss: 2.268688 (2.280240) Accuracy: 0.187500 (0.164030)\n",
      "[250/251] Loss: 2.114433 (2.279986) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301158 (2.301158) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312144 (2.302986) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271381 (2.300786) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278560 (2.306186) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274630 (2.304499) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284576 (2.301791) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300683 (2.302777) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 62/100, Train Loss: 2.2800, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.344440 (2.344440) Accuracy: 0.031250 (0.031250)\n",
      "[10/251] Loss: 2.263422 (2.283061) Accuracy: 0.218750 (0.181818)\n",
      "[20/251] Loss: 2.222983 (2.275907) Accuracy: 0.187500 (0.187500)\n",
      "[30/251] Loss: 2.231735 (2.271082) Accuracy: 0.281250 (0.189516)\n",
      "[40/251] Loss: 2.304470 (2.270978) Accuracy: 0.062500 (0.184451)\n",
      "[50/251] Loss: 2.298710 (2.270527) Accuracy: 0.156250 (0.188725)\n",
      "[60/251] Loss: 2.282356 (2.273435) Accuracy: 0.156250 (0.182889)\n",
      "[70/251] Loss: 2.277622 (2.274660) Accuracy: 0.218750 (0.179137)\n",
      "[80/251] Loss: 2.304577 (2.275936) Accuracy: 0.125000 (0.176312)\n",
      "[90/251] Loss: 2.268986 (2.275615) Accuracy: 0.218750 (0.177198)\n",
      "[100/251] Loss: 2.302080 (2.276168) Accuracy: 0.093750 (0.176052)\n",
      "[110/251] Loss: 2.311600 (2.276185) Accuracy: 0.125000 (0.175394)\n",
      "[120/251] Loss: 2.279581 (2.276530) Accuracy: 0.187500 (0.174070)\n",
      "[130/251] Loss: 2.286226 (2.276445) Accuracy: 0.156250 (0.174618)\n",
      "[140/251] Loss: 2.283948 (2.277695) Accuracy: 0.187500 (0.171099)\n",
      "[150/251] Loss: 2.297721 (2.278428) Accuracy: 0.093750 (0.168874)\n",
      "[160/251] Loss: 2.306738 (2.279302) Accuracy: 0.093750 (0.166537)\n",
      "[170/251] Loss: 2.265601 (2.279343) Accuracy: 0.125000 (0.165753)\n",
      "[180/251] Loss: 2.291508 (2.279036) Accuracy: 0.156250 (0.166782)\n",
      "[190/251] Loss: 2.291455 (2.279406) Accuracy: 0.187500 (0.166067)\n",
      "[200/251] Loss: 2.257667 (2.279977) Accuracy: 0.218750 (0.164956)\n",
      "[210/251] Loss: 2.281981 (2.280057) Accuracy: 0.218750 (0.165136)\n",
      "[220/251] Loss: 2.262403 (2.280584) Accuracy: 0.250000 (0.164451)\n",
      "[230/251] Loss: 2.296974 (2.281193) Accuracy: 0.125000 (0.163690)\n",
      "[240/251] Loss: 2.293609 (2.281375) Accuracy: 0.125000 (0.162215)\n",
      "[250/251] Loss: 2.212667 (2.280360) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301168 (2.301168) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312137 (2.302988) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271370 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278573 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274604 (2.304504) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284558 (2.301795) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300676 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 63/100, Train Loss: 2.2804, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.295093 (2.295093) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.329489 (2.285396) Accuracy: 0.031250 (0.156250)\n",
      "[20/251] Loss: 2.276642 (2.286870) Accuracy: 0.218750 (0.148810)\n",
      "[30/251] Loss: 2.342629 (2.284853) Accuracy: 0.031250 (0.155242)\n",
      "[40/251] Loss: 2.294024 (2.284604) Accuracy: 0.187500 (0.154726)\n",
      "[50/251] Loss: 2.261452 (2.280652) Accuracy: 0.250000 (0.164828)\n",
      "[60/251] Loss: 2.324745 (2.279446) Accuracy: 0.093750 (0.167008)\n",
      "[70/251] Loss: 2.266721 (2.277429) Accuracy: 0.218750 (0.172975)\n",
      "[80/251] Loss: 2.299408 (2.276648) Accuracy: 0.093750 (0.174383)\n",
      "[90/251] Loss: 2.256581 (2.275988) Accuracy: 0.250000 (0.174451)\n",
      "[100/251] Loss: 2.263624 (2.277933) Accuracy: 0.125000 (0.169554)\n",
      "[110/251] Loss: 2.246600 (2.278262) Accuracy: 0.187500 (0.168637)\n",
      "[120/251] Loss: 2.294140 (2.277588) Accuracy: 0.187500 (0.170713)\n",
      "[130/251] Loss: 2.239402 (2.277716) Accuracy: 0.312500 (0.170563)\n",
      "[140/251] Loss: 2.302208 (2.278602) Accuracy: 0.156250 (0.169326)\n",
      "[150/251] Loss: 2.273687 (2.278853) Accuracy: 0.156250 (0.168874)\n",
      "[160/251] Loss: 2.214123 (2.278765) Accuracy: 0.312500 (0.169255)\n",
      "[170/251] Loss: 2.288392 (2.279007) Accuracy: 0.156250 (0.168860)\n",
      "[180/251] Loss: 2.255427 (2.279438) Accuracy: 0.218750 (0.167818)\n",
      "[190/251] Loss: 2.296692 (2.279502) Accuracy: 0.156250 (0.166721)\n",
      "[200/251] Loss: 2.278903 (2.279489) Accuracy: 0.156250 (0.166200)\n",
      "[210/251] Loss: 2.285998 (2.279573) Accuracy: 0.218750 (0.166765)\n",
      "[220/251] Loss: 2.324414 (2.280043) Accuracy: 0.062500 (0.165724)\n",
      "[230/251] Loss: 2.283957 (2.280593) Accuracy: 0.218750 (0.164908)\n",
      "[240/251] Loss: 2.270438 (2.280939) Accuracy: 0.187500 (0.163252)\n",
      "[250/251] Loss: 2.353225 (2.280899) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301168 (2.301168) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312136 (2.302987) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271365 (2.300788) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278569 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274612 (2.304503) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284561 (2.301794) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300688 (2.302781) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 64/100, Train Loss: 2.2809, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.253697 (2.253697) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.310764 (2.284323) Accuracy: 0.156250 (0.159091)\n",
      "[20/251] Loss: 2.272694 (2.277117) Accuracy: 0.156250 (0.168155)\n",
      "[30/251] Loss: 2.299503 (2.280138) Accuracy: 0.093750 (0.159274)\n",
      "[40/251] Loss: 2.271707 (2.280996) Accuracy: 0.156250 (0.160061)\n",
      "[50/251] Loss: 2.251345 (2.279281) Accuracy: 0.250000 (0.167279)\n",
      "[60/251] Loss: 2.261022 (2.279721) Accuracy: 0.187500 (0.163422)\n",
      "[70/251] Loss: 2.283389 (2.279672) Accuracy: 0.125000 (0.164613)\n",
      "[80/251] Loss: 2.283551 (2.279281) Accuracy: 0.156250 (0.165123)\n",
      "[90/251] Loss: 2.263631 (2.278844) Accuracy: 0.156250 (0.165865)\n",
      "[100/251] Loss: 2.262176 (2.279834) Accuracy: 0.125000 (0.163057)\n",
      "[110/251] Loss: 2.259813 (2.280099) Accuracy: 0.281250 (0.162725)\n",
      "[120/251] Loss: 2.256478 (2.280987) Accuracy: 0.187500 (0.162190)\n",
      "[130/251] Loss: 2.298709 (2.282136) Accuracy: 0.093750 (0.160305)\n",
      "[140/251] Loss: 2.244804 (2.281004) Accuracy: 0.281250 (0.163121)\n",
      "[150/251] Loss: 2.247268 (2.280989) Accuracy: 0.156250 (0.162459)\n",
      "[160/251] Loss: 2.335433 (2.281448) Accuracy: 0.031250 (0.159938)\n",
      "[170/251] Loss: 2.266023 (2.281450) Accuracy: 0.281250 (0.159905)\n",
      "[180/251] Loss: 2.279078 (2.281855) Accuracy: 0.218750 (0.158322)\n",
      "[190/251] Loss: 2.270231 (2.281459) Accuracy: 0.218750 (0.160504)\n",
      "[200/251] Loss: 2.248753 (2.281257) Accuracy: 0.218750 (0.161381)\n",
      "[210/251] Loss: 2.249053 (2.280379) Accuracy: 0.156250 (0.162470)\n",
      "[220/251] Loss: 2.253819 (2.280520) Accuracy: 0.187500 (0.162613)\n",
      "[230/251] Loss: 2.325491 (2.281316) Accuracy: 0.062500 (0.160308)\n",
      "[240/251] Loss: 2.291550 (2.281143) Accuracy: 0.125000 (0.161307)\n",
      "[250/251] Loss: 2.311478 (2.280734) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301152 (2.301152) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312139 (2.302987) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271358 (2.300787) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278567 (2.306189) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274616 (2.304502) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284570 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300696 (2.302779) Accuracy: 0.156250 (0.116803)\n",
      "Epoch: 65/100, Train Loss: 2.2807, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1177\n",
      "[0/251] Loss: 2.239012 (2.239012) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.249090 (2.272893) Accuracy: 0.218750 (0.178977)\n",
      "[20/251] Loss: 2.343586 (2.280555) Accuracy: 0.062500 (0.172619)\n",
      "[30/251] Loss: 2.215794 (2.278319) Accuracy: 0.281250 (0.163306)\n",
      "[40/251] Loss: 2.300819 (2.279992) Accuracy: 0.093750 (0.161585)\n",
      "[50/251] Loss: 2.258456 (2.275590) Accuracy: 0.187500 (0.168505)\n",
      "[60/251] Loss: 2.245167 (2.276218) Accuracy: 0.218750 (0.167008)\n",
      "[70/251] Loss: 2.287746 (2.276908) Accuracy: 0.125000 (0.166373)\n",
      "[80/251] Loss: 2.294719 (2.275834) Accuracy: 0.187500 (0.170910)\n",
      "[90/251] Loss: 2.293243 (2.276596) Accuracy: 0.093750 (0.169299)\n",
      "[100/251] Loss: 2.327342 (2.278951) Accuracy: 0.031250 (0.164604)\n",
      "[110/251] Loss: 2.270241 (2.278569) Accuracy: 0.187500 (0.166667)\n",
      "[120/251] Loss: 2.230045 (2.279356) Accuracy: 0.250000 (0.165031)\n",
      "[130/251] Loss: 2.322522 (2.280152) Accuracy: 0.093750 (0.163406)\n",
      "[140/251] Loss: 2.272279 (2.280960) Accuracy: 0.093750 (0.161126)\n",
      "[150/251] Loss: 2.229200 (2.281706) Accuracy: 0.218750 (0.161010)\n",
      "[160/251] Loss: 2.290594 (2.281726) Accuracy: 0.187500 (0.160520)\n",
      "[170/251] Loss: 2.292767 (2.281885) Accuracy: 0.187500 (0.160819)\n",
      "[180/251] Loss: 2.275502 (2.281998) Accuracy: 0.250000 (0.161257)\n",
      "[190/251] Loss: 2.259392 (2.281814) Accuracy: 0.218750 (0.160668)\n",
      "[200/251] Loss: 2.281631 (2.281393) Accuracy: 0.218750 (0.162935)\n",
      "[210/251] Loss: 2.267380 (2.280752) Accuracy: 0.218750 (0.163951)\n",
      "[220/251] Loss: 2.263850 (2.280457) Accuracy: 0.187500 (0.164593)\n",
      "[230/251] Loss: 2.278755 (2.280190) Accuracy: 0.187500 (0.165179)\n",
      "[240/251] Loss: 2.302601 (2.280518) Accuracy: 0.093750 (0.164160)\n",
      "[250/251] Loss: 2.399370 (2.281068) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301150 (2.301150) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312142 (2.302987) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271354 (2.300787) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278564 (2.306189) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274614 (2.304502) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284571 (2.301792) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300699 (2.302779) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 66/100, Train Loss: 2.2811, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.274243 (2.274243) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.264423 (2.274801) Accuracy: 0.218750 (0.184659)\n",
      "[20/251] Loss: 2.274495 (2.279092) Accuracy: 0.156250 (0.171131)\n",
      "[30/251] Loss: 2.303017 (2.276142) Accuracy: 0.156250 (0.177419)\n",
      "[40/251] Loss: 2.256571 (2.276123) Accuracy: 0.187500 (0.173780)\n",
      "[50/251] Loss: 2.293140 (2.278861) Accuracy: 0.156250 (0.169730)\n",
      "[60/251] Loss: 2.253668 (2.279947) Accuracy: 0.156250 (0.166496)\n",
      "[70/251] Loss: 2.230599 (2.279809) Accuracy: 0.250000 (0.166373)\n",
      "[80/251] Loss: 2.269752 (2.279798) Accuracy: 0.187500 (0.168596)\n",
      "[90/251] Loss: 2.253659 (2.278368) Accuracy: 0.250000 (0.172734)\n",
      "[100/251] Loss: 2.222757 (2.279955) Accuracy: 0.375000 (0.169554)\n",
      "[110/251] Loss: 2.285634 (2.279982) Accuracy: 0.093750 (0.168074)\n",
      "[120/251] Loss: 2.309992 (2.280706) Accuracy: 0.093750 (0.164773)\n",
      "[130/251] Loss: 2.290696 (2.280923) Accuracy: 0.187500 (0.164361)\n",
      "[140/251] Loss: 2.244372 (2.280034) Accuracy: 0.218750 (0.166002)\n",
      "[150/251] Loss: 2.269067 (2.279408) Accuracy: 0.218750 (0.167839)\n",
      "[160/251] Loss: 2.297717 (2.280027) Accuracy: 0.187500 (0.167314)\n",
      "[170/251] Loss: 2.322894 (2.280201) Accuracy: 0.031250 (0.166484)\n",
      "[180/251] Loss: 2.233551 (2.279489) Accuracy: 0.312500 (0.168163)\n",
      "[190/251] Loss: 2.267564 (2.279891) Accuracy: 0.156250 (0.166885)\n",
      "[200/251] Loss: 2.306378 (2.280334) Accuracy: 0.156250 (0.166667)\n",
      "[210/251] Loss: 2.274836 (2.280193) Accuracy: 0.125000 (0.165877)\n",
      "[220/251] Loss: 2.267833 (2.279780) Accuracy: 0.125000 (0.165865)\n",
      "[230/251] Loss: 2.329772 (2.280267) Accuracy: 0.062500 (0.164367)\n",
      "[240/251] Loss: 2.289010 (2.280987) Accuracy: 0.093750 (0.162993)\n",
      "[250/251] Loss: 2.397182 (2.281056) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301150 (2.301150) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312146 (2.302989) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271353 (2.300787) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278563 (2.306189) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274610 (2.304502) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284578 (2.301792) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300711 (2.302779) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 67/100, Train Loss: 2.2811, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.313690 (2.313690) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.307458 (2.299533) Accuracy: 0.125000 (0.119318)\n",
      "[20/251] Loss: 2.273765 (2.284520) Accuracy: 0.187500 (0.153274)\n",
      "[30/251] Loss: 2.299425 (2.285165) Accuracy: 0.125000 (0.157258)\n",
      "[40/251] Loss: 2.289717 (2.286040) Accuracy: 0.187500 (0.151677)\n",
      "[50/251] Loss: 2.290861 (2.285876) Accuracy: 0.156250 (0.153186)\n",
      "[60/251] Loss: 2.228950 (2.283264) Accuracy: 0.218750 (0.158811)\n",
      "[70/251] Loss: 2.328556 (2.282501) Accuracy: 0.031250 (0.160211)\n",
      "[80/251] Loss: 2.301641 (2.282119) Accuracy: 0.125000 (0.160494)\n",
      "[90/251] Loss: 2.292400 (2.282464) Accuracy: 0.156250 (0.158997)\n",
      "[100/251] Loss: 2.300822 (2.281373) Accuracy: 0.093750 (0.160272)\n",
      "[110/251] Loss: 2.320505 (2.281495) Accuracy: 0.093750 (0.161036)\n",
      "[120/251] Loss: 2.263821 (2.282258) Accuracy: 0.250000 (0.159091)\n",
      "[130/251] Loss: 2.290320 (2.282500) Accuracy: 0.093750 (0.159590)\n",
      "[140/251] Loss: 2.224964 (2.282436) Accuracy: 0.250000 (0.158910)\n",
      "[150/251] Loss: 2.302210 (2.282273) Accuracy: 0.125000 (0.158526)\n",
      "[160/251] Loss: 2.293953 (2.281795) Accuracy: 0.125000 (0.159550)\n",
      "[170/251] Loss: 2.330859 (2.281473) Accuracy: 0.062500 (0.160636)\n",
      "[180/251] Loss: 2.293003 (2.281339) Accuracy: 0.093750 (0.160566)\n",
      "[190/251] Loss: 2.299960 (2.281281) Accuracy: 0.156250 (0.161813)\n",
      "[200/251] Loss: 2.298638 (2.280911) Accuracy: 0.125000 (0.163091)\n",
      "[210/251] Loss: 2.269851 (2.280854) Accuracy: 0.281250 (0.163507)\n",
      "[220/251] Loss: 2.299211 (2.281151) Accuracy: 0.125000 (0.162472)\n",
      "[230/251] Loss: 2.313893 (2.281049) Accuracy: 0.062500 (0.163285)\n",
      "[240/251] Loss: 2.304327 (2.280855) Accuracy: 0.156250 (0.163252)\n",
      "[250/251] Loss: 2.383064 (2.280998) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301147 (2.301147) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312146 (2.302989) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271366 (2.300788) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278565 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274609 (2.304503) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284576 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300710 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 68/100, Train Loss: 2.2810, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.317380 (2.317380) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.208747 (2.270917) Accuracy: 0.343750 (0.187500)\n",
      "[20/251] Loss: 2.266882 (2.280927) Accuracy: 0.187500 (0.162202)\n",
      "[30/251] Loss: 2.296222 (2.282209) Accuracy: 0.093750 (0.158266)\n",
      "[40/251] Loss: 2.281492 (2.285889) Accuracy: 0.187500 (0.147866)\n",
      "[50/251] Loss: 2.247881 (2.284921) Accuracy: 0.250000 (0.152574)\n",
      "[60/251] Loss: 2.248743 (2.281742) Accuracy: 0.218750 (0.156762)\n",
      "[70/251] Loss: 2.259070 (2.279965) Accuracy: 0.218750 (0.160651)\n",
      "[80/251] Loss: 2.309216 (2.279924) Accuracy: 0.093750 (0.162037)\n",
      "[90/251] Loss: 2.252045 (2.279599) Accuracy: 0.281250 (0.161401)\n",
      "[100/251] Loss: 2.305558 (2.279681) Accuracy: 0.156250 (0.160891)\n",
      "[110/251] Loss: 2.251855 (2.279820) Accuracy: 0.187500 (0.160755)\n",
      "[120/251] Loss: 2.289410 (2.280346) Accuracy: 0.125000 (0.160124)\n",
      "[130/251] Loss: 2.291642 (2.280893) Accuracy: 0.156250 (0.159113)\n",
      "[140/251] Loss: 2.264788 (2.280365) Accuracy: 0.250000 (0.159353)\n",
      "[150/251] Loss: 2.318635 (2.280703) Accuracy: 0.093750 (0.160182)\n",
      "[160/251] Loss: 2.259149 (2.280369) Accuracy: 0.218750 (0.161297)\n",
      "[170/251] Loss: 2.305630 (2.281124) Accuracy: 0.093750 (0.160453)\n",
      "[180/251] Loss: 2.233057 (2.281089) Accuracy: 0.281250 (0.159876)\n",
      "[190/251] Loss: 2.263966 (2.280692) Accuracy: 0.250000 (0.161486)\n",
      "[200/251] Loss: 2.296229 (2.281470) Accuracy: 0.156250 (0.159826)\n",
      "[210/251] Loss: 2.301686 (2.281591) Accuracy: 0.125000 (0.159805)\n",
      "[220/251] Loss: 2.294883 (2.281170) Accuracy: 0.156250 (0.161623)\n",
      "[230/251] Loss: 2.278638 (2.281042) Accuracy: 0.156250 (0.162067)\n",
      "[240/251] Loss: 2.277126 (2.280802) Accuracy: 0.218750 (0.162474)\n",
      "[250/251] Loss: 2.345467 (2.280850) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301149 (2.301149) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312148 (2.302990) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271363 (2.300788) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278561 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274607 (2.304503) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284576 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300719 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 69/100, Train Loss: 2.2808, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.308723 (2.308723) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.306801 (2.279053) Accuracy: 0.093750 (0.147727)\n",
      "[20/251] Loss: 2.260305 (2.279629) Accuracy: 0.218750 (0.159226)\n",
      "[30/251] Loss: 2.285465 (2.279508) Accuracy: 0.218750 (0.169355)\n",
      "[40/251] Loss: 2.268496 (2.279534) Accuracy: 0.187500 (0.165396)\n",
      "[50/251] Loss: 2.302963 (2.281118) Accuracy: 0.093750 (0.162377)\n",
      "[60/251] Loss: 2.329330 (2.282633) Accuracy: 0.062500 (0.159324)\n",
      "[70/251] Loss: 2.294721 (2.280245) Accuracy: 0.062500 (0.165493)\n",
      "[80/251] Loss: 2.271822 (2.279533) Accuracy: 0.250000 (0.168981)\n",
      "[90/251] Loss: 2.304261 (2.279143) Accuracy: 0.125000 (0.168269)\n",
      "[100/251] Loss: 2.266940 (2.277493) Accuracy: 0.250000 (0.170792)\n",
      "[110/251] Loss: 2.325614 (2.277989) Accuracy: 0.031250 (0.170327)\n",
      "[120/251] Loss: 2.238456 (2.278414) Accuracy: 0.281250 (0.169938)\n",
      "[130/251] Loss: 2.313838 (2.277909) Accuracy: 0.093750 (0.169132)\n",
      "[140/251] Loss: 2.303834 (2.277705) Accuracy: 0.156250 (0.169105)\n",
      "[150/251] Loss: 2.274769 (2.278163) Accuracy: 0.156250 (0.167632)\n",
      "[160/251] Loss: 2.322712 (2.278370) Accuracy: 0.031250 (0.167314)\n",
      "[170/251] Loss: 2.305931 (2.278498) Accuracy: 0.093750 (0.168677)\n",
      "[180/251] Loss: 2.306790 (2.279183) Accuracy: 0.093750 (0.166436)\n",
      "[190/251] Loss: 2.280289 (2.279340) Accuracy: 0.125000 (0.165576)\n",
      "[200/251] Loss: 2.277973 (2.279861) Accuracy: 0.187500 (0.164335)\n",
      "[210/251] Loss: 2.208641 (2.279745) Accuracy: 0.343750 (0.164988)\n",
      "[220/251] Loss: 2.288995 (2.279684) Accuracy: 0.187500 (0.165300)\n",
      "[230/251] Loss: 2.296078 (2.279838) Accuracy: 0.125000 (0.165043)\n",
      "[240/251] Loss: 2.317535 (2.280602) Accuracy: 0.031250 (0.163382)\n",
      "[250/251] Loss: 2.345363 (2.280846) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301149 (2.301149) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312146 (2.302990) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271357 (2.300787) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278559 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274606 (2.304502) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284580 (2.301792) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300721 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 70/100, Train Loss: 2.2808, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.270972 (2.270972) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.254086 (2.277815) Accuracy: 0.250000 (0.173295)\n",
      "[20/251] Loss: 2.300794 (2.285813) Accuracy: 0.093750 (0.150298)\n",
      "[30/251] Loss: 2.296846 (2.284275) Accuracy: 0.187500 (0.158266)\n",
      "[40/251] Loss: 2.292625 (2.283217) Accuracy: 0.125000 (0.151677)\n",
      "[50/251] Loss: 2.289312 (2.282304) Accuracy: 0.125000 (0.149510)\n",
      "[60/251] Loss: 2.286491 (2.282917) Accuracy: 0.156250 (0.148566)\n",
      "[70/251] Loss: 2.266442 (2.282765) Accuracy: 0.125000 (0.151408)\n",
      "[80/251] Loss: 2.245468 (2.283250) Accuracy: 0.250000 (0.152006)\n",
      "[90/251] Loss: 2.301172 (2.282872) Accuracy: 0.156250 (0.154190)\n",
      "[100/251] Loss: 2.290815 (2.281864) Accuracy: 0.125000 (0.155012)\n",
      "[110/251] Loss: 2.248766 (2.282443) Accuracy: 0.250000 (0.155968)\n",
      "[120/251] Loss: 2.310359 (2.282982) Accuracy: 0.093750 (0.155475)\n",
      "[130/251] Loss: 2.265737 (2.282156) Accuracy: 0.093750 (0.157204)\n",
      "[140/251] Loss: 2.285096 (2.283003) Accuracy: 0.062500 (0.154699)\n",
      "[150/251] Loss: 2.302733 (2.282689) Accuracy: 0.125000 (0.155215)\n",
      "[160/251] Loss: 2.278486 (2.282129) Accuracy: 0.156250 (0.157415)\n",
      "[170/251] Loss: 2.290552 (2.281578) Accuracy: 0.187500 (0.159357)\n",
      "[180/251] Loss: 2.255443 (2.281598) Accuracy: 0.156250 (0.159703)\n",
      "[190/251] Loss: 2.251668 (2.281010) Accuracy: 0.281250 (0.161486)\n",
      "[200/251] Loss: 2.258168 (2.280983) Accuracy: 0.156250 (0.161381)\n",
      "[210/251] Loss: 2.271325 (2.280641) Accuracy: 0.156250 (0.162767)\n",
      "[220/251] Loss: 2.300037 (2.280845) Accuracy: 0.156250 (0.162472)\n",
      "[230/251] Loss: 2.294947 (2.281119) Accuracy: 0.156250 (0.162338)\n",
      "[240/251] Loss: 2.279789 (2.280898) Accuracy: 0.187500 (0.162604)\n",
      "[250/251] Loss: 2.374059 (2.280955) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301152 (2.301152) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312147 (2.302991) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271354 (2.300787) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278558 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274605 (2.304502) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284580 (2.301792) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300726 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 71/100, Train Loss: 2.2810, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.263192 (2.263192) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.275305 (2.281819) Accuracy: 0.187500 (0.156250)\n",
      "[20/251] Loss: 2.267700 (2.275110) Accuracy: 0.187500 (0.187500)\n",
      "[30/251] Loss: 2.274732 (2.276227) Accuracy: 0.250000 (0.185484)\n",
      "[40/251] Loss: 2.302349 (2.278120) Accuracy: 0.093750 (0.176067)\n",
      "[50/251] Loss: 2.254070 (2.275763) Accuracy: 0.218750 (0.177696)\n",
      "[60/251] Loss: 2.311634 (2.278561) Accuracy: 0.156250 (0.174693)\n",
      "[70/251] Loss: 2.268125 (2.280372) Accuracy: 0.125000 (0.168134)\n",
      "[80/251] Loss: 2.288550 (2.280955) Accuracy: 0.156250 (0.167824)\n",
      "[90/251] Loss: 2.298281 (2.280653) Accuracy: 0.062500 (0.168613)\n",
      "[100/251] Loss: 2.247677 (2.281023) Accuracy: 0.187500 (0.166151)\n",
      "[110/251] Loss: 2.312568 (2.280548) Accuracy: 0.093750 (0.166104)\n",
      "[120/251] Loss: 2.297809 (2.280929) Accuracy: 0.156250 (0.164256)\n",
      "[130/251] Loss: 2.289458 (2.280314) Accuracy: 0.187500 (0.165315)\n",
      "[140/251] Loss: 2.264780 (2.280780) Accuracy: 0.187500 (0.163121)\n",
      "[150/251] Loss: 2.289678 (2.280928) Accuracy: 0.218750 (0.163493)\n",
      "[160/251] Loss: 2.266241 (2.280649) Accuracy: 0.156250 (0.163626)\n",
      "[170/251] Loss: 2.283565 (2.280775) Accuracy: 0.125000 (0.162829)\n",
      "[180/251] Loss: 2.289735 (2.281105) Accuracy: 0.156250 (0.162638)\n",
      "[190/251] Loss: 2.273279 (2.280585) Accuracy: 0.218750 (0.163449)\n",
      "[200/251] Loss: 2.284928 (2.280777) Accuracy: 0.187500 (0.163402)\n",
      "[210/251] Loss: 2.269647 (2.280721) Accuracy: 0.156250 (0.164100)\n",
      "[220/251] Loss: 2.244030 (2.279740) Accuracy: 0.312500 (0.165865)\n",
      "[230/251] Loss: 2.248093 (2.279711) Accuracy: 0.218750 (0.165720)\n",
      "[240/251] Loss: 2.281972 (2.279976) Accuracy: 0.156250 (0.165197)\n",
      "[250/251] Loss: 2.290945 (2.280632) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301155 (2.301155) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312143 (2.302991) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271349 (2.300787) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278557 (2.306188) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274609 (2.304502) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284581 (2.301791) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300726 (2.302780) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 72/100, Train Loss: 2.2806, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.269437 (2.269437) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.282720 (2.282741) Accuracy: 0.156250 (0.170455)\n",
      "[20/251] Loss: 2.312866 (2.287044) Accuracy: 0.000000 (0.160714)\n",
      "[30/251] Loss: 2.233691 (2.283461) Accuracy: 0.187500 (0.161290)\n",
      "[40/251] Loss: 2.300128 (2.282800) Accuracy: 0.125000 (0.159299)\n",
      "[50/251] Loss: 2.273829 (2.282347) Accuracy: 0.187500 (0.162377)\n",
      "[60/251] Loss: 2.254299 (2.282982) Accuracy: 0.156250 (0.162398)\n",
      "[70/251] Loss: 2.273642 (2.281756) Accuracy: 0.187500 (0.164173)\n",
      "[80/251] Loss: 2.294226 (2.282446) Accuracy: 0.093750 (0.160880)\n",
      "[90/251] Loss: 2.293414 (2.281843) Accuracy: 0.156250 (0.161058)\n",
      "[100/251] Loss: 2.276548 (2.282517) Accuracy: 0.218750 (0.159035)\n",
      "[110/251] Loss: 2.265902 (2.280192) Accuracy: 0.218750 (0.162162)\n",
      "[120/251] Loss: 2.269413 (2.279562) Accuracy: 0.187500 (0.163481)\n",
      "[130/251] Loss: 2.312481 (2.279954) Accuracy: 0.093750 (0.161498)\n",
      "[140/251] Loss: 2.283217 (2.279944) Accuracy: 0.187500 (0.161348)\n",
      "[150/251] Loss: 2.239057 (2.279477) Accuracy: 0.281250 (0.164321)\n",
      "[160/251] Loss: 2.294075 (2.280037) Accuracy: 0.187500 (0.163432)\n",
      "[170/251] Loss: 2.285113 (2.278855) Accuracy: 0.187500 (0.166849)\n",
      "[180/251] Loss: 2.338438 (2.278751) Accuracy: 0.031250 (0.166609)\n",
      "[190/251] Loss: 2.208977 (2.278161) Accuracy: 0.312500 (0.168521)\n",
      "[200/251] Loss: 2.283889 (2.279338) Accuracy: 0.187500 (0.166045)\n",
      "[210/251] Loss: 2.256101 (2.279684) Accuracy: 0.281250 (0.166025)\n",
      "[220/251] Loss: 2.296299 (2.280150) Accuracy: 0.093750 (0.165158)\n",
      "[230/251] Loss: 2.295788 (2.279987) Accuracy: 0.156250 (0.165179)\n",
      "[240/251] Loss: 2.290941 (2.280491) Accuracy: 0.125000 (0.163382)\n",
      "[250/251] Loss: 2.384004 (2.280988) Accuracy: 0.000000 (0.163098)\n",
      "[0/63] Loss: 2.301156 (2.301156) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312145 (2.302991) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271351 (2.300787) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278558 (2.306189) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274606 (2.304502) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284580 (2.301791) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300729 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 73/100, Train Loss: 2.2810, Train Acc: 0.1631, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.312897 (2.312897) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.287491 (2.278622) Accuracy: 0.187500 (0.164773)\n",
      "[20/251] Loss: 2.312946 (2.278116) Accuracy: 0.125000 (0.169643)\n",
      "[30/251] Loss: 2.307153 (2.278745) Accuracy: 0.156250 (0.167339)\n",
      "[40/251] Loss: 2.263922 (2.279659) Accuracy: 0.187500 (0.161585)\n",
      "[50/251] Loss: 2.313260 (2.281154) Accuracy: 0.093750 (0.158701)\n",
      "[60/251] Loss: 2.255820 (2.280701) Accuracy: 0.250000 (0.158811)\n",
      "[70/251] Loss: 2.228535 (2.279074) Accuracy: 0.281250 (0.162412)\n",
      "[80/251] Loss: 2.286330 (2.277627) Accuracy: 0.156250 (0.167438)\n",
      "[90/251] Loss: 2.282285 (2.277671) Accuracy: 0.218750 (0.168269)\n",
      "[100/251] Loss: 2.294626 (2.277147) Accuracy: 0.125000 (0.170792)\n",
      "[110/251] Loss: 2.321642 (2.277468) Accuracy: 0.093750 (0.170890)\n",
      "[120/251] Loss: 2.306668 (2.277592) Accuracy: 0.156250 (0.170455)\n",
      "[130/251] Loss: 2.297070 (2.277786) Accuracy: 0.093750 (0.169847)\n",
      "[140/251] Loss: 2.279217 (2.278344) Accuracy: 0.218750 (0.168883)\n",
      "[150/251] Loss: 2.255237 (2.279387) Accuracy: 0.250000 (0.167632)\n",
      "[160/251] Loss: 2.247138 (2.279890) Accuracy: 0.250000 (0.167508)\n",
      "[170/251] Loss: 2.275877 (2.279753) Accuracy: 0.156250 (0.167398)\n",
      "[180/251] Loss: 2.253660 (2.279904) Accuracy: 0.218750 (0.167645)\n",
      "[190/251] Loss: 2.283158 (2.279870) Accuracy: 0.125000 (0.167212)\n",
      "[200/251] Loss: 2.257629 (2.280308) Accuracy: 0.218750 (0.166667)\n",
      "[210/251] Loss: 2.267072 (2.280499) Accuracy: 0.218750 (0.166025)\n",
      "[220/251] Loss: 2.244174 (2.280118) Accuracy: 0.218750 (0.166431)\n",
      "[230/251] Loss: 2.245461 (2.280020) Accuracy: 0.187500 (0.165990)\n",
      "[240/251] Loss: 2.278608 (2.280174) Accuracy: 0.125000 (0.164808)\n",
      "[250/251] Loss: 2.248461 (2.280464) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301160 (2.301160) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312147 (2.302994) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271338 (2.300787) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278554 (2.306189) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274606 (2.304503) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284589 (2.301791) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300732 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 74/100, Train Loss: 2.2805, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.281748 (2.281748) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.345819 (2.288387) Accuracy: 0.062500 (0.153409)\n",
      "[20/251] Loss: 2.255726 (2.283003) Accuracy: 0.187500 (0.156250)\n",
      "[30/251] Loss: 2.264199 (2.280882) Accuracy: 0.281250 (0.162298)\n",
      "[40/251] Loss: 2.297539 (2.280729) Accuracy: 0.125000 (0.163872)\n",
      "[50/251] Loss: 2.319029 (2.282215) Accuracy: 0.093750 (0.159314)\n",
      "[60/251] Loss: 2.258731 (2.282551) Accuracy: 0.281250 (0.161373)\n",
      "[70/251] Loss: 2.272827 (2.282187) Accuracy: 0.218750 (0.164173)\n",
      "[80/251] Loss: 2.273157 (2.282237) Accuracy: 0.187500 (0.163966)\n",
      "[90/251] Loss: 2.240859 (2.281238) Accuracy: 0.156250 (0.165522)\n",
      "[100/251] Loss: 2.253052 (2.280478) Accuracy: 0.187500 (0.165842)\n",
      "[110/251] Loss: 2.266122 (2.280584) Accuracy: 0.156250 (0.163570)\n",
      "[120/251] Loss: 2.267835 (2.280023) Accuracy: 0.125000 (0.164773)\n",
      "[130/251] Loss: 2.293773 (2.280685) Accuracy: 0.093750 (0.163168)\n",
      "[140/251] Loss: 2.294984 (2.280681) Accuracy: 0.156250 (0.162012)\n",
      "[150/251] Loss: 2.268590 (2.280253) Accuracy: 0.250000 (0.162873)\n",
      "[160/251] Loss: 2.263841 (2.279983) Accuracy: 0.187500 (0.165179)\n",
      "[170/251] Loss: 2.279414 (2.280646) Accuracy: 0.187500 (0.164839)\n",
      "[180/251] Loss: 2.295810 (2.281016) Accuracy: 0.125000 (0.164019)\n",
      "[190/251] Loss: 2.305566 (2.280933) Accuracy: 0.093750 (0.164267)\n",
      "[200/251] Loss: 2.271533 (2.280795) Accuracy: 0.218750 (0.164956)\n",
      "[210/251] Loss: 2.320323 (2.281114) Accuracy: 0.062500 (0.163803)\n",
      "[220/251] Loss: 2.272817 (2.281221) Accuracy: 0.187500 (0.163179)\n",
      "[230/251] Loss: 2.293301 (2.281131) Accuracy: 0.156250 (0.163420)\n",
      "[240/251] Loss: 2.271460 (2.280433) Accuracy: 0.187500 (0.164678)\n",
      "[250/251] Loss: 2.363189 (2.280904) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301161 (2.301161) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312150 (2.302996) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271339 (2.300788) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278555 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274608 (2.304504) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284588 (2.301792) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300735 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 75/100, Train Loss: 2.2809, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.243506 (2.243506) Accuracy: 0.281250 (0.281250)\n",
      "[10/251] Loss: 2.270144 (2.277298) Accuracy: 0.187500 (0.167614)\n",
      "[20/251] Loss: 2.331421 (2.285536) Accuracy: 0.062500 (0.142857)\n",
      "[30/251] Loss: 2.286869 (2.287961) Accuracy: 0.156250 (0.149194)\n",
      "[40/251] Loss: 2.306401 (2.287287) Accuracy: 0.187500 (0.153201)\n",
      "[50/251] Loss: 2.267055 (2.286624) Accuracy: 0.218750 (0.155637)\n",
      "[60/251] Loss: 2.248741 (2.286085) Accuracy: 0.218750 (0.151639)\n",
      "[70/251] Loss: 2.254560 (2.285469) Accuracy: 0.187500 (0.153609)\n",
      "[80/251] Loss: 2.252087 (2.283811) Accuracy: 0.218750 (0.158565)\n",
      "[90/251] Loss: 2.254154 (2.282288) Accuracy: 0.156250 (0.161745)\n",
      "[100/251] Loss: 2.281776 (2.280875) Accuracy: 0.125000 (0.163676)\n",
      "[110/251] Loss: 2.249560 (2.281133) Accuracy: 0.218750 (0.164696)\n",
      "[120/251] Loss: 2.336490 (2.280908) Accuracy: 0.031250 (0.163998)\n",
      "[130/251] Loss: 2.293627 (2.281248) Accuracy: 0.125000 (0.162929)\n",
      "[140/251] Loss: 2.261096 (2.281601) Accuracy: 0.125000 (0.162012)\n",
      "[150/251] Loss: 2.327688 (2.282155) Accuracy: 0.031250 (0.161217)\n",
      "[160/251] Loss: 2.271161 (2.281795) Accuracy: 0.187500 (0.162267)\n",
      "[170/251] Loss: 2.256444 (2.281751) Accuracy: 0.187500 (0.162098)\n",
      "[180/251] Loss: 2.328267 (2.280971) Accuracy: 0.093750 (0.163674)\n",
      "[190/251] Loss: 2.270622 (2.280393) Accuracy: 0.156250 (0.163776)\n",
      "[200/251] Loss: 2.294836 (2.280609) Accuracy: 0.093750 (0.163868)\n",
      "[210/251] Loss: 2.277690 (2.280762) Accuracy: 0.093750 (0.163507)\n",
      "[220/251] Loss: 2.272214 (2.280459) Accuracy: 0.125000 (0.163603)\n",
      "[230/251] Loss: 2.263174 (2.280602) Accuracy: 0.281250 (0.163961)\n",
      "[240/251] Loss: 2.224680 (2.280241) Accuracy: 0.250000 (0.163900)\n",
      "[250/251] Loss: 2.369200 (2.280926) Accuracy: 0.000000 (0.162849)\n",
      "[0/63] Loss: 2.301161 (2.301161) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312152 (2.302997) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271339 (2.300788) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278555 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274605 (2.304504) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284588 (2.301792) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300738 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 76/100, Train Loss: 2.2809, Train Acc: 0.1628, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.291610 (2.291610) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.313291 (2.272044) Accuracy: 0.125000 (0.173295)\n",
      "[20/251] Loss: 2.286871 (2.278082) Accuracy: 0.187500 (0.166667)\n",
      "[30/251] Loss: 2.250328 (2.279795) Accuracy: 0.187500 (0.169355)\n",
      "[40/251] Loss: 2.256949 (2.278400) Accuracy: 0.250000 (0.171494)\n",
      "[50/251] Loss: 2.285012 (2.280063) Accuracy: 0.125000 (0.166667)\n",
      "[60/251] Loss: 2.263365 (2.279088) Accuracy: 0.250000 (0.168545)\n",
      "[70/251] Loss: 2.236866 (2.278784) Accuracy: 0.218750 (0.167254)\n",
      "[80/251] Loss: 2.277603 (2.278500) Accuracy: 0.187500 (0.167824)\n",
      "[90/251] Loss: 2.307086 (2.279684) Accuracy: 0.218750 (0.167582)\n",
      "[100/251] Loss: 2.307014 (2.280474) Accuracy: 0.062500 (0.163985)\n",
      "[110/251] Loss: 2.281978 (2.281219) Accuracy: 0.187500 (0.163851)\n",
      "[120/251] Loss: 2.248968 (2.280161) Accuracy: 0.187500 (0.165289)\n",
      "[130/251] Loss: 2.263674 (2.281048) Accuracy: 0.218750 (0.162691)\n",
      "[140/251] Loss: 2.327099 (2.281419) Accuracy: 0.093750 (0.161791)\n",
      "[150/251] Loss: 2.283996 (2.281423) Accuracy: 0.218750 (0.161838)\n",
      "[160/251] Loss: 2.264561 (2.281806) Accuracy: 0.218750 (0.162267)\n",
      "[170/251] Loss: 2.254714 (2.281742) Accuracy: 0.187500 (0.161732)\n",
      "[180/251] Loss: 2.290439 (2.281630) Accuracy: 0.218750 (0.162293)\n",
      "[190/251] Loss: 2.283261 (2.281268) Accuracy: 0.187500 (0.163449)\n",
      "[200/251] Loss: 2.298083 (2.281518) Accuracy: 0.187500 (0.162780)\n",
      "[210/251] Loss: 2.300140 (2.281713) Accuracy: 0.125000 (0.162026)\n",
      "[220/251] Loss: 2.224728 (2.281072) Accuracy: 0.218750 (0.162755)\n",
      "[230/251] Loss: 2.281477 (2.281279) Accuracy: 0.187500 (0.162338)\n",
      "[240/251] Loss: 2.260686 (2.280898) Accuracy: 0.218750 (0.162733)\n",
      "[250/251] Loss: 2.321553 (2.280741) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301161 (2.301161) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312155 (2.302999) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271333 (2.300788) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278552 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274608 (2.304504) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284595 (2.301792) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300743 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 77/100, Train Loss: 2.2807, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.264752 (2.264752) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.228296 (2.278237) Accuracy: 0.250000 (0.170455)\n",
      "[20/251] Loss: 2.198168 (2.275101) Accuracy: 0.312500 (0.174107)\n",
      "[30/251] Loss: 2.291059 (2.279240) Accuracy: 0.093750 (0.162298)\n",
      "[40/251] Loss: 2.288473 (2.279452) Accuracy: 0.156250 (0.159299)\n",
      "[50/251] Loss: 2.319620 (2.279625) Accuracy: 0.062500 (0.162377)\n",
      "[60/251] Loss: 2.296446 (2.279340) Accuracy: 0.281250 (0.165984)\n",
      "[70/251] Loss: 2.297962 (2.280378) Accuracy: 0.125000 (0.164173)\n",
      "[80/251] Loss: 2.290994 (2.280497) Accuracy: 0.125000 (0.163194)\n",
      "[90/251] Loss: 2.264228 (2.280385) Accuracy: 0.187500 (0.163805)\n",
      "[100/251] Loss: 2.282704 (2.279917) Accuracy: 0.250000 (0.163985)\n",
      "[110/251] Loss: 2.281384 (2.279727) Accuracy: 0.187500 (0.164414)\n",
      "[120/251] Loss: 2.282345 (2.280625) Accuracy: 0.218750 (0.162965)\n",
      "[130/251] Loss: 2.315961 (2.281142) Accuracy: 0.062500 (0.161260)\n",
      "[140/251] Loss: 2.270229 (2.280481) Accuracy: 0.156250 (0.163342)\n",
      "[150/251] Loss: 2.311386 (2.281236) Accuracy: 0.156250 (0.163079)\n",
      "[160/251] Loss: 2.255493 (2.281603) Accuracy: 0.125000 (0.162461)\n",
      "[170/251] Loss: 2.303917 (2.282387) Accuracy: 0.125000 (0.161367)\n",
      "[180/251] Loss: 2.276777 (2.281889) Accuracy: 0.125000 (0.163329)\n",
      "[190/251] Loss: 2.283485 (2.282095) Accuracy: 0.125000 (0.162304)\n",
      "[200/251] Loss: 2.230003 (2.281423) Accuracy: 0.187500 (0.162780)\n",
      "[210/251] Loss: 2.227781 (2.281305) Accuracy: 0.281250 (0.162767)\n",
      "[220/251] Loss: 2.249208 (2.281066) Accuracy: 0.187500 (0.163320)\n",
      "[230/251] Loss: 2.281536 (2.280481) Accuracy: 0.125000 (0.163420)\n",
      "[240/251] Loss: 2.294127 (2.280267) Accuracy: 0.156250 (0.164549)\n",
      "[250/251] Loss: 2.261526 (2.280508) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301167 (2.301167) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312154 (2.302999) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271323 (2.300787) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306188) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274604 (2.304504) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284597 (2.301792) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300743 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 78/100, Train Loss: 2.2805, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.297656 (2.297656) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.271886 (2.282697) Accuracy: 0.250000 (0.159091)\n",
      "[20/251] Loss: 2.297835 (2.280744) Accuracy: 0.125000 (0.165179)\n",
      "[30/251] Loss: 2.279086 (2.279508) Accuracy: 0.187500 (0.165323)\n",
      "[40/251] Loss: 2.275013 (2.278597) Accuracy: 0.156250 (0.166159)\n",
      "[50/251] Loss: 2.292943 (2.280640) Accuracy: 0.125000 (0.163603)\n",
      "[60/251] Loss: 2.257721 (2.280498) Accuracy: 0.312500 (0.167520)\n",
      "[70/251] Loss: 2.275606 (2.280732) Accuracy: 0.218750 (0.163292)\n",
      "[80/251] Loss: 2.291057 (2.279938) Accuracy: 0.125000 (0.163194)\n",
      "[90/251] Loss: 2.303368 (2.280830) Accuracy: 0.093750 (0.164148)\n",
      "[100/251] Loss: 2.281263 (2.279410) Accuracy: 0.156250 (0.167079)\n",
      "[110/251] Loss: 2.218998 (2.278867) Accuracy: 0.312500 (0.168356)\n",
      "[120/251] Loss: 2.265105 (2.278725) Accuracy: 0.093750 (0.167872)\n",
      "[130/251] Loss: 2.262596 (2.279285) Accuracy: 0.125000 (0.165792)\n",
      "[140/251] Loss: 2.292483 (2.278997) Accuracy: 0.125000 (0.167332)\n",
      "[150/251] Loss: 2.298033 (2.278903) Accuracy: 0.125000 (0.167632)\n",
      "[160/251] Loss: 2.290802 (2.279095) Accuracy: 0.125000 (0.167896)\n",
      "[170/251] Loss: 2.324927 (2.279670) Accuracy: 0.125000 (0.167763)\n",
      "[180/251] Loss: 2.284268 (2.280010) Accuracy: 0.156250 (0.167645)\n",
      "[190/251] Loss: 2.291246 (2.280231) Accuracy: 0.093750 (0.166885)\n",
      "[200/251] Loss: 2.303095 (2.280398) Accuracy: 0.093750 (0.165734)\n",
      "[210/251] Loss: 2.272386 (2.280118) Accuracy: 0.187500 (0.165432)\n",
      "[220/251] Loss: 2.247520 (2.280067) Accuracy: 0.281250 (0.165583)\n",
      "[230/251] Loss: 2.257194 (2.280530) Accuracy: 0.187500 (0.164232)\n",
      "[240/251] Loss: 2.290423 (2.280968) Accuracy: 0.156250 (0.162733)\n",
      "[250/251] Loss: 2.226428 (2.280372) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312152 (2.303000) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300788) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278548 (2.306189) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274597 (2.304505) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284592 (2.301792) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300740 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 79/100, Train Loss: 2.2804, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.278821 (2.278821) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.283585 (2.280721) Accuracy: 0.156250 (0.173295)\n",
      "[20/251] Loss: 2.300949 (2.285568) Accuracy: 0.125000 (0.156250)\n",
      "[30/251] Loss: 2.280921 (2.277048) Accuracy: 0.125000 (0.173387)\n",
      "[40/251] Loss: 2.313050 (2.276225) Accuracy: 0.156250 (0.174543)\n",
      "[50/251] Loss: 2.305788 (2.276271) Accuracy: 0.093750 (0.172794)\n",
      "[60/251] Loss: 2.281413 (2.278020) Accuracy: 0.125000 (0.168545)\n",
      "[70/251] Loss: 2.292921 (2.279138) Accuracy: 0.187500 (0.167694)\n",
      "[80/251] Loss: 2.300922 (2.278832) Accuracy: 0.125000 (0.168210)\n",
      "[90/251] Loss: 2.293296 (2.280582) Accuracy: 0.125000 (0.163462)\n",
      "[100/251] Loss: 2.297587 (2.280614) Accuracy: 0.156250 (0.163676)\n",
      "[110/251] Loss: 2.276195 (2.280513) Accuracy: 0.156250 (0.164977)\n",
      "[120/251] Loss: 2.312092 (2.280014) Accuracy: 0.125000 (0.164514)\n",
      "[130/251] Loss: 2.237502 (2.279614) Accuracy: 0.312500 (0.165076)\n",
      "[140/251] Loss: 2.231101 (2.279735) Accuracy: 0.250000 (0.165337)\n",
      "[150/251] Loss: 2.222724 (2.280492) Accuracy: 0.250000 (0.162666)\n",
      "[160/251] Loss: 2.287230 (2.280617) Accuracy: 0.156250 (0.161685)\n",
      "[170/251] Loss: 2.216888 (2.281462) Accuracy: 0.218750 (0.158443)\n",
      "[180/251] Loss: 2.199752 (2.281663) Accuracy: 0.312500 (0.159358)\n",
      "[190/251] Loss: 2.295915 (2.280931) Accuracy: 0.187500 (0.160995)\n",
      "[200/251] Loss: 2.263417 (2.280916) Accuracy: 0.156250 (0.161070)\n",
      "[210/251] Loss: 2.322214 (2.281052) Accuracy: 0.156250 (0.161582)\n",
      "[220/251] Loss: 2.183626 (2.280603) Accuracy: 0.375000 (0.163179)\n",
      "[230/251] Loss: 2.260275 (2.281186) Accuracy: 0.187500 (0.162067)\n",
      "[240/251] Loss: 2.253722 (2.280298) Accuracy: 0.281250 (0.163900)\n",
      "[250/251] Loss: 1.819960 (2.278802) Accuracy: 1.000000 (0.166833)\n",
      "[0/63] Loss: 2.301171 (2.301171) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312154 (2.303001) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271324 (2.300788) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278548 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274600 (2.304505) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284594 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300745 (2.302781) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 80/100, Train Loss: 2.2788, Train Acc: 0.1668, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.265908 (2.265908) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.303355 (2.280658) Accuracy: 0.093750 (0.159091)\n",
      "[20/251] Loss: 2.312509 (2.292222) Accuracy: 0.093750 (0.142857)\n",
      "[30/251] Loss: 2.295991 (2.290202) Accuracy: 0.187500 (0.156250)\n",
      "[40/251] Loss: 2.269673 (2.284048) Accuracy: 0.218750 (0.161585)\n",
      "[50/251] Loss: 2.297579 (2.282876) Accuracy: 0.125000 (0.156863)\n",
      "[60/251] Loss: 2.302177 (2.281763) Accuracy: 0.093750 (0.162910)\n",
      "[70/251] Loss: 2.283123 (2.284296) Accuracy: 0.093750 (0.158451)\n",
      "[80/251] Loss: 2.298527 (2.283907) Accuracy: 0.093750 (0.158565)\n",
      "[90/251] Loss: 2.310591 (2.284319) Accuracy: 0.093750 (0.158654)\n",
      "[100/251] Loss: 2.277758 (2.281581) Accuracy: 0.156250 (0.165223)\n",
      "[110/251] Loss: 2.254846 (2.280764) Accuracy: 0.125000 (0.164696)\n",
      "[120/251] Loss: 2.285261 (2.280702) Accuracy: 0.187500 (0.163998)\n",
      "[130/251] Loss: 2.273370 (2.280216) Accuracy: 0.187500 (0.165792)\n",
      "[140/251] Loss: 2.217708 (2.279911) Accuracy: 0.218750 (0.167553)\n",
      "[150/251] Loss: 2.279670 (2.279867) Accuracy: 0.187500 (0.168046)\n",
      "[160/251] Loss: 2.299593 (2.279900) Accuracy: 0.093750 (0.167120)\n",
      "[170/251] Loss: 2.280413 (2.279481) Accuracy: 0.156250 (0.167032)\n",
      "[180/251] Loss: 2.298893 (2.278648) Accuracy: 0.125000 (0.168163)\n",
      "[190/251] Loss: 2.278754 (2.279106) Accuracy: 0.187500 (0.167212)\n",
      "[200/251] Loss: 2.282334 (2.279536) Accuracy: 0.156250 (0.165423)\n",
      "[210/251] Loss: 2.278051 (2.279096) Accuracy: 0.125000 (0.165432)\n",
      "[220/251] Loss: 2.299792 (2.279291) Accuracy: 0.187500 (0.166007)\n",
      "[230/251] Loss: 2.303390 (2.279545) Accuracy: 0.093750 (0.165720)\n",
      "[240/251] Loss: 2.287554 (2.280095) Accuracy: 0.125000 (0.164808)\n",
      "[250/251] Loss: 2.345005 (2.280827) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312153 (2.303002) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271323 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278550 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274599 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284592 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300743 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 81/100, Train Loss: 2.2808, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.277982 (2.277982) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.282349 (2.293878) Accuracy: 0.156250 (0.130682)\n",
      "[20/251] Loss: 2.202062 (2.284216) Accuracy: 0.343750 (0.153274)\n",
      "[30/251] Loss: 2.319720 (2.285985) Accuracy: 0.093750 (0.150202)\n",
      "[40/251] Loss: 2.289538 (2.284979) Accuracy: 0.156250 (0.153201)\n",
      "[50/251] Loss: 2.240413 (2.283186) Accuracy: 0.312500 (0.157475)\n",
      "[60/251] Loss: 2.310301 (2.285810) Accuracy: 0.125000 (0.151639)\n",
      "[70/251] Loss: 2.290807 (2.286115) Accuracy: 0.125000 (0.153609)\n",
      "[80/251] Loss: 2.288146 (2.284376) Accuracy: 0.187500 (0.154707)\n",
      "[90/251] Loss: 2.303222 (2.284586) Accuracy: 0.062500 (0.154876)\n",
      "[100/251] Loss: 2.298167 (2.284308) Accuracy: 0.093750 (0.156250)\n",
      "[110/251] Loss: 2.291923 (2.282620) Accuracy: 0.125000 (0.159065)\n",
      "[120/251] Loss: 2.315108 (2.281946) Accuracy: 0.062500 (0.161157)\n",
      "[130/251] Loss: 2.289436 (2.281893) Accuracy: 0.156250 (0.161498)\n",
      "[140/251] Loss: 2.256664 (2.281673) Accuracy: 0.218750 (0.161126)\n",
      "[150/251] Loss: 2.286953 (2.281396) Accuracy: 0.125000 (0.161217)\n",
      "[160/251] Loss: 2.320107 (2.281558) Accuracy: 0.031250 (0.160908)\n",
      "[170/251] Loss: 2.294447 (2.281174) Accuracy: 0.125000 (0.161184)\n",
      "[180/251] Loss: 2.261557 (2.280783) Accuracy: 0.125000 (0.161775)\n",
      "[190/251] Loss: 2.259213 (2.280527) Accuracy: 0.250000 (0.162958)\n",
      "[200/251] Loss: 2.290726 (2.280219) Accuracy: 0.156250 (0.164024)\n",
      "[210/251] Loss: 2.271416 (2.280024) Accuracy: 0.218750 (0.164248)\n",
      "[220/251] Loss: 2.305176 (2.279892) Accuracy: 0.093750 (0.165441)\n",
      "[230/251] Loss: 2.320587 (2.280355) Accuracy: 0.125000 (0.164637)\n",
      "[240/251] Loss: 2.311780 (2.280982) Accuracy: 0.062500 (0.163511)\n",
      "[250/251] Loss: 2.402727 (2.281049) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301171 (2.301171) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312154 (2.303002) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278550 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274598 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284593 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300745 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 82/100, Train Loss: 2.2810, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.287929 (2.287929) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.236488 (2.275658) Accuracy: 0.218750 (0.161932)\n",
      "[20/251] Loss: 2.261314 (2.279445) Accuracy: 0.187500 (0.162202)\n",
      "[30/251] Loss: 2.257329 (2.279986) Accuracy: 0.218750 (0.161290)\n",
      "[40/251] Loss: 2.246321 (2.277549) Accuracy: 0.281250 (0.166921)\n",
      "[50/251] Loss: 2.257948 (2.277878) Accuracy: 0.250000 (0.166054)\n",
      "[60/251] Loss: 2.290025 (2.276524) Accuracy: 0.125000 (0.171619)\n",
      "[70/251] Loss: 2.241825 (2.273228) Accuracy: 0.281250 (0.181778)\n",
      "[80/251] Loss: 2.258721 (2.273235) Accuracy: 0.343750 (0.180941)\n",
      "[90/251] Loss: 2.279439 (2.274526) Accuracy: 0.187500 (0.176854)\n",
      "[100/251] Loss: 2.234637 (2.274779) Accuracy: 0.250000 (0.176671)\n",
      "[110/251] Loss: 2.299125 (2.275453) Accuracy: 0.187500 (0.174831)\n",
      "[120/251] Loss: 2.296427 (2.277062) Accuracy: 0.156250 (0.171229)\n",
      "[130/251] Loss: 2.304440 (2.278036) Accuracy: 0.187500 (0.169370)\n",
      "[140/251] Loss: 2.299472 (2.278693) Accuracy: 0.093750 (0.168883)\n",
      "[150/251] Loss: 2.268171 (2.278882) Accuracy: 0.187500 (0.167425)\n",
      "[160/251] Loss: 2.254968 (2.278804) Accuracy: 0.125000 (0.166731)\n",
      "[170/251] Loss: 2.265868 (2.278714) Accuracy: 0.250000 (0.167398)\n",
      "[180/251] Loss: 2.280133 (2.278286) Accuracy: 0.187500 (0.169026)\n",
      "[190/251] Loss: 2.257488 (2.279045) Accuracy: 0.187500 (0.167539)\n",
      "[200/251] Loss: 2.321479 (2.279358) Accuracy: 0.093750 (0.166200)\n",
      "[210/251] Loss: 2.266410 (2.279135) Accuracy: 0.187500 (0.166765)\n",
      "[220/251] Loss: 2.265170 (2.279145) Accuracy: 0.156250 (0.166714)\n",
      "[230/251] Loss: 2.299156 (2.279402) Accuracy: 0.125000 (0.166126)\n",
      "[240/251] Loss: 2.272257 (2.279935) Accuracy: 0.125000 (0.164938)\n",
      "[250/251] Loss: 2.407117 (2.281066) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312154 (2.303002) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278549 (2.306190) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274599 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284593 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300746 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 83/100, Train Loss: 2.2811, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.226089 (2.226089) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.302762 (2.278556) Accuracy: 0.125000 (0.153409)\n",
      "[20/251] Loss: 2.280785 (2.277942) Accuracy: 0.218750 (0.178571)\n",
      "[30/251] Loss: 2.268435 (2.276030) Accuracy: 0.156250 (0.180444)\n",
      "[40/251] Loss: 2.261579 (2.275103) Accuracy: 0.250000 (0.181402)\n",
      "[50/251] Loss: 2.311100 (2.275667) Accuracy: 0.125000 (0.180147)\n",
      "[60/251] Loss: 2.285908 (2.274276) Accuracy: 0.218750 (0.184426)\n",
      "[70/251] Loss: 2.297233 (2.276347) Accuracy: 0.093750 (0.175176)\n",
      "[80/251] Loss: 2.280196 (2.277467) Accuracy: 0.187500 (0.172068)\n",
      "[90/251] Loss: 2.252234 (2.277837) Accuracy: 0.218750 (0.171703)\n",
      "[100/251] Loss: 2.248316 (2.278174) Accuracy: 0.250000 (0.171101)\n",
      "[110/251] Loss: 2.279123 (2.278245) Accuracy: 0.156250 (0.171171)\n",
      "[120/251] Loss: 2.257782 (2.278408) Accuracy: 0.187500 (0.171229)\n",
      "[130/251] Loss: 2.236384 (2.279247) Accuracy: 0.218750 (0.167462)\n",
      "[140/251] Loss: 2.275956 (2.278696) Accuracy: 0.125000 (0.167553)\n",
      "[150/251] Loss: 2.310164 (2.279352) Accuracy: 0.093750 (0.165770)\n",
      "[160/251] Loss: 2.349912 (2.279853) Accuracy: 0.031250 (0.164596)\n",
      "[170/251] Loss: 2.304913 (2.279802) Accuracy: 0.125000 (0.166484)\n",
      "[180/251] Loss: 2.301781 (2.280088) Accuracy: 0.125000 (0.165919)\n",
      "[190/251] Loss: 2.272713 (2.280152) Accuracy: 0.125000 (0.166230)\n",
      "[200/251] Loss: 2.330050 (2.281200) Accuracy: 0.031250 (0.163557)\n",
      "[210/251] Loss: 2.299708 (2.280757) Accuracy: 0.156250 (0.164248)\n",
      "[220/251] Loss: 2.315954 (2.280776) Accuracy: 0.125000 (0.164451)\n",
      "[230/251] Loss: 2.266242 (2.280944) Accuracy: 0.156250 (0.162879)\n",
      "[240/251] Loss: 2.262765 (2.281346) Accuracy: 0.125000 (0.161696)\n",
      "[250/251] Loss: 2.363395 (2.280896) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312154 (2.303002) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278549 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274598 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284593 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300746 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 84/100, Train Loss: 2.2809, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.264920 (2.264920) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.316737 (2.287591) Accuracy: 0.093750 (0.173295)\n",
      "[20/251] Loss: 2.241635 (2.278668) Accuracy: 0.156250 (0.174107)\n",
      "[30/251] Loss: 2.267270 (2.279381) Accuracy: 0.187500 (0.168347)\n",
      "[40/251] Loss: 2.312350 (2.279773) Accuracy: 0.093750 (0.169207)\n",
      "[50/251] Loss: 2.291698 (2.279567) Accuracy: 0.156250 (0.166054)\n",
      "[60/251] Loss: 2.242844 (2.277668) Accuracy: 0.218750 (0.171619)\n",
      "[70/251] Loss: 2.248609 (2.278331) Accuracy: 0.125000 (0.169014)\n",
      "[80/251] Loss: 2.324835 (2.279075) Accuracy: 0.093750 (0.166667)\n",
      "[90/251] Loss: 2.270546 (2.279211) Accuracy: 0.093750 (0.165865)\n",
      "[100/251] Loss: 2.275452 (2.279187) Accuracy: 0.187500 (0.166460)\n",
      "[110/251] Loss: 2.276835 (2.280153) Accuracy: 0.125000 (0.164133)\n",
      "[120/251] Loss: 2.298615 (2.281222) Accuracy: 0.093750 (0.161415)\n",
      "[130/251] Loss: 2.319081 (2.281124) Accuracy: 0.062500 (0.160544)\n",
      "[140/251] Loss: 2.273997 (2.280792) Accuracy: 0.125000 (0.160904)\n",
      "[150/251] Loss: 2.285077 (2.280551) Accuracy: 0.187500 (0.163286)\n",
      "[160/251] Loss: 2.234740 (2.280208) Accuracy: 0.312500 (0.164402)\n",
      "[170/251] Loss: 2.323423 (2.280599) Accuracy: 0.062500 (0.163194)\n",
      "[180/251] Loss: 2.261987 (2.280837) Accuracy: 0.218750 (0.162983)\n",
      "[190/251] Loss: 2.292575 (2.280804) Accuracy: 0.156250 (0.162467)\n",
      "[200/251] Loss: 2.301944 (2.280797) Accuracy: 0.187500 (0.162624)\n",
      "[210/251] Loss: 2.274301 (2.280307) Accuracy: 0.187500 (0.164248)\n",
      "[220/251] Loss: 2.229186 (2.280709) Accuracy: 0.312500 (0.163603)\n",
      "[230/251] Loss: 2.290167 (2.281048) Accuracy: 0.125000 (0.162473)\n",
      "[240/251] Loss: 2.281102 (2.280534) Accuracy: 0.125000 (0.162863)\n",
      "[250/251] Loss: 2.295453 (2.280633) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312155 (2.303002) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278548 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274597 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284594 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300745 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 85/100, Train Loss: 2.2806, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.269210 (2.269210) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.280231 (2.279686) Accuracy: 0.218750 (0.170455)\n",
      "[20/251] Loss: 2.309355 (2.284796) Accuracy: 0.062500 (0.159226)\n",
      "[30/251] Loss: 2.271467 (2.279555) Accuracy: 0.187500 (0.171371)\n",
      "[40/251] Loss: 2.312134 (2.284070) Accuracy: 0.093750 (0.160823)\n",
      "[50/251] Loss: 2.290686 (2.281692) Accuracy: 0.156250 (0.162990)\n",
      "[60/251] Loss: 2.234081 (2.279980) Accuracy: 0.375000 (0.168545)\n",
      "[70/251] Loss: 2.263543 (2.279920) Accuracy: 0.187500 (0.169014)\n",
      "[80/251] Loss: 2.208536 (2.277057) Accuracy: 0.312500 (0.173611)\n",
      "[90/251] Loss: 2.280515 (2.277114) Accuracy: 0.218750 (0.173764)\n",
      "[100/251] Loss: 2.319230 (2.277594) Accuracy: 0.125000 (0.171101)\n",
      "[110/251] Loss: 2.287244 (2.276845) Accuracy: 0.187500 (0.173705)\n",
      "[120/251] Loss: 2.288853 (2.278442) Accuracy: 0.156250 (0.171746)\n",
      "[130/251] Loss: 2.274097 (2.278866) Accuracy: 0.156250 (0.169847)\n",
      "[140/251] Loss: 2.274210 (2.279920) Accuracy: 0.156250 (0.168218)\n",
      "[150/251] Loss: 2.301367 (2.279587) Accuracy: 0.125000 (0.169495)\n",
      "[160/251] Loss: 2.283058 (2.279593) Accuracy: 0.125000 (0.168090)\n",
      "[170/251] Loss: 2.274405 (2.280020) Accuracy: 0.093750 (0.166849)\n",
      "[180/251] Loss: 2.307498 (2.280417) Accuracy: 0.125000 (0.166609)\n",
      "[190/251] Loss: 2.292159 (2.279622) Accuracy: 0.187500 (0.167703)\n",
      "[200/251] Loss: 2.279035 (2.280183) Accuracy: 0.187500 (0.166511)\n",
      "[210/251] Loss: 2.284868 (2.280262) Accuracy: 0.156250 (0.165581)\n",
      "[220/251] Loss: 2.231559 (2.280691) Accuracy: 0.281250 (0.164310)\n",
      "[230/251] Loss: 2.296026 (2.280843) Accuracy: 0.093750 (0.164096)\n",
      "[240/251] Loss: 2.256507 (2.280300) Accuracy: 0.187500 (0.164030)\n",
      "[250/251] Loss: 1.565776 (2.277817) Accuracy: 1.000000 (0.166833)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312156 (2.303002) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274598 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284596 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300746 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 86/100, Train Loss: 2.2778, Train Acc: 0.1668, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.299553 (2.299553) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.316840 (2.298644) Accuracy: 0.093750 (0.125000)\n",
      "[20/251] Loss: 2.277165 (2.283163) Accuracy: 0.187500 (0.159226)\n",
      "[30/251] Loss: 2.315968 (2.284685) Accuracy: 0.062500 (0.154234)\n",
      "[40/251] Loss: 2.294897 (2.283801) Accuracy: 0.187500 (0.159299)\n",
      "[50/251] Loss: 2.287182 (2.281990) Accuracy: 0.187500 (0.162990)\n",
      "[60/251] Loss: 2.294510 (2.280758) Accuracy: 0.187500 (0.165471)\n",
      "[70/251] Loss: 2.216201 (2.280210) Accuracy: 0.312500 (0.164613)\n",
      "[80/251] Loss: 2.278530 (2.279793) Accuracy: 0.187500 (0.167052)\n",
      "[90/251] Loss: 2.288172 (2.280429) Accuracy: 0.156250 (0.166209)\n",
      "[100/251] Loss: 2.296028 (2.280772) Accuracy: 0.125000 (0.165532)\n",
      "[110/251] Loss: 2.254433 (2.280052) Accuracy: 0.250000 (0.167230)\n",
      "[120/251] Loss: 2.325748 (2.280388) Accuracy: 0.093750 (0.165806)\n",
      "[130/251] Loss: 2.297953 (2.280461) Accuracy: 0.125000 (0.165792)\n",
      "[140/251] Loss: 2.240987 (2.279254) Accuracy: 0.343750 (0.168440)\n",
      "[150/251] Loss: 2.302606 (2.279974) Accuracy: 0.062500 (0.165977)\n",
      "[160/251] Loss: 2.306220 (2.278981) Accuracy: 0.031250 (0.168090)\n",
      "[170/251] Loss: 2.292248 (2.278880) Accuracy: 0.125000 (0.168677)\n",
      "[180/251] Loss: 2.323902 (2.279185) Accuracy: 0.062500 (0.167818)\n",
      "[190/251] Loss: 2.331565 (2.279927) Accuracy: 0.093750 (0.166067)\n",
      "[200/251] Loss: 2.292725 (2.280120) Accuracy: 0.125000 (0.165423)\n",
      "[210/251] Loss: 2.286084 (2.280049) Accuracy: 0.218750 (0.166025)\n",
      "[220/251] Loss: 2.211669 (2.279933) Accuracy: 0.312500 (0.166148)\n",
      "[230/251] Loss: 2.287847 (2.280351) Accuracy: 0.156250 (0.164773)\n",
      "[240/251] Loss: 2.298071 (2.280600) Accuracy: 0.062500 (0.163382)\n",
      "[250/251] Loss: 2.394105 (2.281013) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312157 (2.303002) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274598 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284595 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300746 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 87/100, Train Loss: 2.2810, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.290785 (2.290785) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.278051 (2.273471) Accuracy: 0.156250 (0.159091)\n",
      "[20/251] Loss: 2.288882 (2.279836) Accuracy: 0.218750 (0.163690)\n",
      "[30/251] Loss: 2.324068 (2.281307) Accuracy: 0.062500 (0.157258)\n",
      "[40/251] Loss: 2.353749 (2.286091) Accuracy: 0.062500 (0.148628)\n",
      "[50/251] Loss: 2.260089 (2.284189) Accuracy: 0.187500 (0.155025)\n",
      "[60/251] Loss: 2.298401 (2.283550) Accuracy: 0.093750 (0.158299)\n",
      "[70/251] Loss: 2.319024 (2.283739) Accuracy: 0.062500 (0.156250)\n",
      "[80/251] Loss: 2.277078 (2.284259) Accuracy: 0.218750 (0.156636)\n",
      "[90/251] Loss: 2.291906 (2.283614) Accuracy: 0.218750 (0.160714)\n",
      "[100/251] Loss: 2.278396 (2.283012) Accuracy: 0.156250 (0.161200)\n",
      "[110/251] Loss: 2.295552 (2.282442) Accuracy: 0.125000 (0.161036)\n",
      "[120/251] Loss: 2.297462 (2.282886) Accuracy: 0.062500 (0.159349)\n",
      "[130/251] Loss: 2.318002 (2.283199) Accuracy: 0.093750 (0.158158)\n",
      "[140/251] Loss: 2.252330 (2.282139) Accuracy: 0.218750 (0.160018)\n",
      "[150/251] Loss: 2.303541 (2.282523) Accuracy: 0.125000 (0.159561)\n",
      "[160/251] Loss: 2.241532 (2.281724) Accuracy: 0.250000 (0.161879)\n",
      "[170/251] Loss: 2.235974 (2.280946) Accuracy: 0.250000 (0.164108)\n",
      "[180/251] Loss: 2.235269 (2.280986) Accuracy: 0.281250 (0.163674)\n",
      "[190/251] Loss: 2.274648 (2.280817) Accuracy: 0.125000 (0.163449)\n",
      "[200/251] Loss: 2.284099 (2.280854) Accuracy: 0.218750 (0.163402)\n",
      "[210/251] Loss: 2.262344 (2.280703) Accuracy: 0.187500 (0.163951)\n",
      "[220/251] Loss: 2.257336 (2.280766) Accuracy: 0.156250 (0.163037)\n",
      "[230/251] Loss: 2.310684 (2.280775) Accuracy: 0.062500 (0.162473)\n",
      "[240/251] Loss: 2.241405 (2.280249) Accuracy: 0.312500 (0.163641)\n",
      "[250/251] Loss: 2.318476 (2.280721) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312156 (2.303002) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271321 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274597 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284596 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300746 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 88/100, Train Loss: 2.2807, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.265345 (2.265345) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.305368 (2.281396) Accuracy: 0.093750 (0.156250)\n",
      "[20/251] Loss: 2.321097 (2.283738) Accuracy: 0.062500 (0.156250)\n",
      "[30/251] Loss: 2.267619 (2.282773) Accuracy: 0.125000 (0.156250)\n",
      "[40/251] Loss: 2.257585 (2.281385) Accuracy: 0.250000 (0.158537)\n",
      "[50/251] Loss: 2.283054 (2.279137) Accuracy: 0.125000 (0.166667)\n",
      "[60/251] Loss: 2.256962 (2.278783) Accuracy: 0.218750 (0.167520)\n",
      "[70/251] Loss: 2.292508 (2.279740) Accuracy: 0.093750 (0.164613)\n",
      "[80/251] Loss: 2.196664 (2.278649) Accuracy: 0.343750 (0.168596)\n",
      "[90/251] Loss: 2.271604 (2.280569) Accuracy: 0.250000 (0.166209)\n",
      "[100/251] Loss: 2.223846 (2.279606) Accuracy: 0.250000 (0.166770)\n",
      "[110/251] Loss: 2.307913 (2.280349) Accuracy: 0.062500 (0.164696)\n",
      "[120/251] Loss: 2.273576 (2.281078) Accuracy: 0.156250 (0.162190)\n",
      "[130/251] Loss: 2.350251 (2.281683) Accuracy: 0.000000 (0.160544)\n",
      "[140/251] Loss: 2.269255 (2.281090) Accuracy: 0.187500 (0.161791)\n",
      "[150/251] Loss: 2.288083 (2.281221) Accuracy: 0.156250 (0.161631)\n",
      "[160/251] Loss: 2.296924 (2.281095) Accuracy: 0.062500 (0.161685)\n",
      "[170/251] Loss: 2.312247 (2.280969) Accuracy: 0.093750 (0.162281)\n",
      "[180/251] Loss: 2.297719 (2.281282) Accuracy: 0.093750 (0.161948)\n",
      "[190/251] Loss: 2.308035 (2.281391) Accuracy: 0.125000 (0.162631)\n",
      "[200/251] Loss: 2.276506 (2.280240) Accuracy: 0.125000 (0.164646)\n",
      "[210/251] Loss: 2.316668 (2.280576) Accuracy: 0.093750 (0.163951)\n",
      "[220/251] Loss: 2.255315 (2.280177) Accuracy: 0.250000 (0.165300)\n",
      "[230/251] Loss: 2.277128 (2.280232) Accuracy: 0.187500 (0.165043)\n",
      "[240/251] Loss: 2.267996 (2.279973) Accuracy: 0.156250 (0.165067)\n",
      "[250/251] Loss: 2.383329 (2.280971) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312157 (2.303002) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274597 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284596 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300746 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 89/100, Train Loss: 2.2810, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.265381 (2.265381) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.281492 (2.272750) Accuracy: 0.125000 (0.181818)\n",
      "[20/251] Loss: 2.324390 (2.276464) Accuracy: 0.156250 (0.188988)\n",
      "[30/251] Loss: 2.304685 (2.283112) Accuracy: 0.062500 (0.171371)\n",
      "[40/251] Loss: 2.318640 (2.281030) Accuracy: 0.125000 (0.168445)\n",
      "[50/251] Loss: 2.236879 (2.279196) Accuracy: 0.250000 (0.170956)\n",
      "[60/251] Loss: 2.249230 (2.277797) Accuracy: 0.218750 (0.172131)\n",
      "[70/251] Loss: 2.302682 (2.277614) Accuracy: 0.156250 (0.172095)\n",
      "[80/251] Loss: 2.275160 (2.277735) Accuracy: 0.156250 (0.172068)\n",
      "[90/251] Loss: 2.279352 (2.277663) Accuracy: 0.062500 (0.172047)\n",
      "[100/251] Loss: 2.318374 (2.277750) Accuracy: 0.093750 (0.170792)\n",
      "[110/251] Loss: 2.277927 (2.278445) Accuracy: 0.156250 (0.169482)\n",
      "[120/251] Loss: 2.328011 (2.278894) Accuracy: 0.093750 (0.168388)\n",
      "[130/251] Loss: 2.294603 (2.278643) Accuracy: 0.156250 (0.168893)\n",
      "[140/251] Loss: 2.306711 (2.279179) Accuracy: 0.125000 (0.167775)\n",
      "[150/251] Loss: 2.269513 (2.278793) Accuracy: 0.187500 (0.168874)\n",
      "[160/251] Loss: 2.294942 (2.279173) Accuracy: 0.156250 (0.167120)\n",
      "[170/251] Loss: 2.268210 (2.280062) Accuracy: 0.218750 (0.166118)\n",
      "[180/251] Loss: 2.314363 (2.280823) Accuracy: 0.093750 (0.162811)\n",
      "[190/251] Loss: 2.278748 (2.280356) Accuracy: 0.218750 (0.164431)\n",
      "[200/251] Loss: 2.291263 (2.280648) Accuracy: 0.093750 (0.163402)\n",
      "[210/251] Loss: 2.290284 (2.280386) Accuracy: 0.125000 (0.164692)\n",
      "[220/251] Loss: 2.290920 (2.280062) Accuracy: 0.187500 (0.165724)\n",
      "[230/251] Loss: 2.280969 (2.280155) Accuracy: 0.156250 (0.165314)\n",
      "[240/251] Loss: 2.280380 (2.280265) Accuracy: 0.156250 (0.164160)\n",
      "[250/251] Loss: 2.330018 (2.280765) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312157 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274597 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284595 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300748 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 90/100, Train Loss: 2.2808, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.316584 (2.316584) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.287329 (2.278296) Accuracy: 0.156250 (0.176136)\n",
      "[20/251] Loss: 2.318214 (2.277379) Accuracy: 0.093750 (0.171131)\n",
      "[30/251] Loss: 2.267626 (2.276655) Accuracy: 0.156250 (0.174395)\n",
      "[40/251] Loss: 2.261072 (2.277670) Accuracy: 0.250000 (0.176067)\n",
      "[50/251] Loss: 2.295700 (2.279738) Accuracy: 0.218750 (0.174632)\n",
      "[60/251] Loss: 2.245166 (2.278981) Accuracy: 0.218750 (0.180328)\n",
      "[70/251] Loss: 2.311164 (2.280017) Accuracy: 0.125000 (0.176056)\n",
      "[80/251] Loss: 2.308692 (2.281139) Accuracy: 0.125000 (0.171296)\n",
      "[90/251] Loss: 2.289245 (2.281455) Accuracy: 0.156250 (0.168269)\n",
      "[100/251] Loss: 2.326277 (2.283282) Accuracy: 0.062500 (0.162748)\n",
      "[110/251] Loss: 2.340851 (2.283358) Accuracy: 0.031250 (0.163851)\n",
      "[120/251] Loss: 2.315304 (2.283412) Accuracy: 0.062500 (0.162448)\n",
      "[130/251] Loss: 2.280848 (2.284172) Accuracy: 0.187500 (0.159828)\n",
      "[140/251] Loss: 2.228793 (2.283970) Accuracy: 0.250000 (0.159353)\n",
      "[150/251] Loss: 2.252755 (2.283466) Accuracy: 0.218750 (0.160182)\n",
      "[160/251] Loss: 2.232266 (2.282598) Accuracy: 0.281250 (0.162073)\n",
      "[170/251] Loss: 2.318027 (2.282453) Accuracy: 0.093750 (0.160453)\n",
      "[180/251] Loss: 2.308525 (2.281813) Accuracy: 0.093750 (0.161084)\n",
      "[190/251] Loss: 2.289702 (2.281753) Accuracy: 0.187500 (0.160504)\n",
      "[200/251] Loss: 2.270214 (2.281889) Accuracy: 0.125000 (0.160759)\n",
      "[210/251] Loss: 2.289524 (2.281831) Accuracy: 0.093750 (0.161137)\n",
      "[220/251] Loss: 2.303768 (2.281986) Accuracy: 0.187500 (0.160916)\n",
      "[230/251] Loss: 2.246437 (2.281384) Accuracy: 0.250000 (0.161661)\n",
      "[240/251] Loss: 2.224630 (2.280806) Accuracy: 0.281250 (0.162863)\n",
      "[250/251] Loss: 2.395012 (2.281015) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301171 (2.301171) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312158 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274597 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284595 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300748 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 91/100, Train Loss: 2.2810, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.316770 (2.316770) Accuracy: 0.031250 (0.031250)\n",
      "[10/251] Loss: 2.288281 (2.291563) Accuracy: 0.093750 (0.110795)\n",
      "[20/251] Loss: 2.284481 (2.282592) Accuracy: 0.250000 (0.153274)\n",
      "[30/251] Loss: 2.277941 (2.285494) Accuracy: 0.125000 (0.146169)\n",
      "[40/251] Loss: 2.295415 (2.281278) Accuracy: 0.187500 (0.159299)\n",
      "[50/251] Loss: 2.283991 (2.280554) Accuracy: 0.125000 (0.158088)\n",
      "[60/251] Loss: 2.268826 (2.280178) Accuracy: 0.156250 (0.159836)\n",
      "[70/251] Loss: 2.280428 (2.281073) Accuracy: 0.156250 (0.161532)\n",
      "[80/251] Loss: 2.257572 (2.281074) Accuracy: 0.218750 (0.162037)\n",
      "[90/251] Loss: 2.253633 (2.279360) Accuracy: 0.187500 (0.165179)\n",
      "[100/251] Loss: 2.277916 (2.279387) Accuracy: 0.250000 (0.163057)\n",
      "[110/251] Loss: 2.294196 (2.279574) Accuracy: 0.093750 (0.162725)\n",
      "[120/251] Loss: 2.229281 (2.278618) Accuracy: 0.250000 (0.164514)\n",
      "[130/251] Loss: 2.314132 (2.278413) Accuracy: 0.093750 (0.162214)\n",
      "[140/251] Loss: 2.249172 (2.279169) Accuracy: 0.218750 (0.161569)\n",
      "[150/251] Loss: 2.282331 (2.279084) Accuracy: 0.187500 (0.162873)\n",
      "[160/251] Loss: 2.237480 (2.279512) Accuracy: 0.281250 (0.162073)\n",
      "[170/251] Loss: 2.282912 (2.279009) Accuracy: 0.156250 (0.165022)\n",
      "[180/251] Loss: 2.320053 (2.279649) Accuracy: 0.093750 (0.163329)\n",
      "[190/251] Loss: 2.316360 (2.280318) Accuracy: 0.125000 (0.163449)\n",
      "[200/251] Loss: 2.293787 (2.280581) Accuracy: 0.093750 (0.162624)\n",
      "[210/251] Loss: 2.271038 (2.280734) Accuracy: 0.187500 (0.163359)\n",
      "[220/251] Loss: 2.256450 (2.280148) Accuracy: 0.156250 (0.164027)\n",
      "[230/251] Loss: 2.247405 (2.280392) Accuracy: 0.218750 (0.163420)\n",
      "[240/251] Loss: 2.291640 (2.280338) Accuracy: 0.187500 (0.163641)\n",
      "[250/251] Loss: 2.382610 (2.280967) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301171 (2.301171) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312158 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278546 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274597 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284595 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300749 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 92/100, Train Loss: 2.2810, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.278954 (2.278954) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.333352 (2.279549) Accuracy: 0.062500 (0.161932)\n",
      "[20/251] Loss: 2.251375 (2.279263) Accuracy: 0.250000 (0.172619)\n",
      "[30/251] Loss: 2.282634 (2.279852) Accuracy: 0.125000 (0.163306)\n",
      "[40/251] Loss: 2.273970 (2.279323) Accuracy: 0.187500 (0.159299)\n",
      "[50/251] Loss: 2.304095 (2.281643) Accuracy: 0.156250 (0.156863)\n",
      "[60/251] Loss: 2.309396 (2.284241) Accuracy: 0.125000 (0.151639)\n",
      "[70/251] Loss: 2.254436 (2.281740) Accuracy: 0.187500 (0.158891)\n",
      "[80/251] Loss: 2.285227 (2.281079) Accuracy: 0.156250 (0.162809)\n",
      "[90/251] Loss: 2.295561 (2.280298) Accuracy: 0.062500 (0.163805)\n",
      "[100/251] Loss: 2.298022 (2.279788) Accuracy: 0.125000 (0.164913)\n",
      "[110/251] Loss: 2.286567 (2.278284) Accuracy: 0.093750 (0.167793)\n",
      "[120/251] Loss: 2.282067 (2.278268) Accuracy: 0.187500 (0.167872)\n",
      "[130/251] Loss: 2.275766 (2.278966) Accuracy: 0.187500 (0.166031)\n",
      "[140/251] Loss: 2.289432 (2.278390) Accuracy: 0.156250 (0.167553)\n",
      "[150/251] Loss: 2.317862 (2.278555) Accuracy: 0.093750 (0.168253)\n",
      "[160/251] Loss: 2.301486 (2.278584) Accuracy: 0.093750 (0.168672)\n",
      "[170/251] Loss: 2.229516 (2.278927) Accuracy: 0.281250 (0.167398)\n",
      "[180/251] Loss: 2.292300 (2.278525) Accuracy: 0.156250 (0.168508)\n",
      "[190/251] Loss: 2.321599 (2.279025) Accuracy: 0.062500 (0.168194)\n",
      "[200/251] Loss: 2.267751 (2.278460) Accuracy: 0.125000 (0.167910)\n",
      "[210/251] Loss: 2.318776 (2.279377) Accuracy: 0.062500 (0.165877)\n",
      "[220/251] Loss: 2.327553 (2.279845) Accuracy: 0.093750 (0.165441)\n",
      "[230/251] Loss: 2.314056 (2.280547) Accuracy: 0.031250 (0.163690)\n",
      "[240/251] Loss: 2.265534 (2.280508) Accuracy: 0.250000 (0.163771)\n",
      "[250/251] Loss: 2.409759 (2.281072) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312157 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271323 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274596 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284595 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300748 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 93/100, Train Loss: 2.2811, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.275592 (2.275592) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.319057 (2.287110) Accuracy: 0.062500 (0.142045)\n",
      "[20/251] Loss: 2.309665 (2.281166) Accuracy: 0.125000 (0.160714)\n",
      "[30/251] Loss: 2.267140 (2.282481) Accuracy: 0.187500 (0.156250)\n",
      "[40/251] Loss: 2.287122 (2.282560) Accuracy: 0.125000 (0.157012)\n",
      "[50/251] Loss: 2.300152 (2.284386) Accuracy: 0.125000 (0.151961)\n",
      "[60/251] Loss: 2.280639 (2.281640) Accuracy: 0.187500 (0.158811)\n",
      "[70/251] Loss: 2.275450 (2.283104) Accuracy: 0.062500 (0.154049)\n",
      "[80/251] Loss: 2.255343 (2.283810) Accuracy: 0.187500 (0.151235)\n",
      "[90/251] Loss: 2.272897 (2.281531) Accuracy: 0.156250 (0.157280)\n",
      "[100/251] Loss: 2.269794 (2.280006) Accuracy: 0.156250 (0.159653)\n",
      "[110/251] Loss: 2.242069 (2.280131) Accuracy: 0.250000 (0.161036)\n",
      "[120/251] Loss: 2.258631 (2.280456) Accuracy: 0.218750 (0.160640)\n",
      "[130/251] Loss: 2.274505 (2.279711) Accuracy: 0.156250 (0.163645)\n",
      "[140/251] Loss: 2.267549 (2.278854) Accuracy: 0.187500 (0.165115)\n",
      "[150/251] Loss: 2.266129 (2.278330) Accuracy: 0.218750 (0.167012)\n",
      "[160/251] Loss: 2.261437 (2.278624) Accuracy: 0.218750 (0.167508)\n",
      "[170/251] Loss: 2.283355 (2.279440) Accuracy: 0.156250 (0.166484)\n",
      "[180/251] Loss: 2.306416 (2.279317) Accuracy: 0.156250 (0.166264)\n",
      "[190/251] Loss: 2.247355 (2.279983) Accuracy: 0.250000 (0.164431)\n",
      "[200/251] Loss: 2.267643 (2.279964) Accuracy: 0.218750 (0.165112)\n",
      "[210/251] Loss: 2.268484 (2.280143) Accuracy: 0.218750 (0.165136)\n",
      "[220/251] Loss: 2.275255 (2.280260) Accuracy: 0.218750 (0.164169)\n",
      "[230/251] Loss: 2.211688 (2.280140) Accuracy: 0.281250 (0.164773)\n",
      "[240/251] Loss: 2.314978 (2.280524) Accuracy: 0.093750 (0.163641)\n",
      "[250/251] Loss: 2.352767 (2.280851) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312158 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278546 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274596 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284595 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300749 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 94/100, Train Loss: 2.2809, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.239470 (2.239470) Accuracy: 0.281250 (0.281250)\n",
      "[10/251] Loss: 2.293442 (2.267560) Accuracy: 0.125000 (0.207386)\n",
      "[20/251] Loss: 2.280519 (2.272096) Accuracy: 0.218750 (0.188988)\n",
      "[30/251] Loss: 2.276820 (2.277367) Accuracy: 0.125000 (0.173387)\n",
      "[40/251] Loss: 2.233613 (2.275687) Accuracy: 0.250000 (0.179116)\n",
      "[50/251] Loss: 2.312983 (2.279704) Accuracy: 0.062500 (0.169118)\n",
      "[60/251] Loss: 2.287449 (2.280620) Accuracy: 0.125000 (0.166496)\n",
      "[70/251] Loss: 2.268033 (2.280696) Accuracy: 0.156250 (0.166373)\n",
      "[80/251] Loss: 2.285075 (2.280653) Accuracy: 0.187500 (0.166667)\n",
      "[90/251] Loss: 2.274019 (2.280384) Accuracy: 0.156250 (0.167582)\n",
      "[100/251] Loss: 2.277834 (2.280803) Accuracy: 0.093750 (0.166151)\n",
      "[110/251] Loss: 2.317984 (2.280715) Accuracy: 0.093750 (0.167511)\n",
      "[120/251] Loss: 2.272471 (2.280578) Accuracy: 0.156250 (0.167355)\n",
      "[130/251] Loss: 2.268421 (2.281759) Accuracy: 0.187500 (0.164361)\n",
      "[140/251] Loss: 2.274812 (2.280303) Accuracy: 0.125000 (0.166667)\n",
      "[150/251] Loss: 2.304801 (2.280373) Accuracy: 0.125000 (0.165977)\n",
      "[160/251] Loss: 2.292090 (2.280700) Accuracy: 0.125000 (0.165179)\n",
      "[170/251] Loss: 2.254942 (2.281236) Accuracy: 0.250000 (0.163925)\n",
      "[180/251] Loss: 2.308542 (2.280780) Accuracy: 0.125000 (0.164537)\n",
      "[190/251] Loss: 2.285245 (2.281363) Accuracy: 0.156250 (0.162631)\n",
      "[200/251] Loss: 2.311398 (2.281093) Accuracy: 0.125000 (0.163713)\n",
      "[210/251] Loss: 2.276079 (2.281147) Accuracy: 0.218750 (0.163655)\n",
      "[220/251] Loss: 2.293657 (2.280569) Accuracy: 0.093750 (0.164310)\n",
      "[230/251] Loss: 2.286308 (2.280549) Accuracy: 0.125000 (0.163420)\n",
      "[240/251] Loss: 2.297278 (2.280528) Accuracy: 0.062500 (0.163771)\n",
      "[250/251] Loss: 2.399452 (2.281031) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312158 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271322 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278546 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274596 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284595 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300749 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 95/100, Train Loss: 2.2810, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.281664 (2.281664) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.284764 (2.285620) Accuracy: 0.125000 (0.153409)\n",
      "[20/251] Loss: 2.232512 (2.280559) Accuracy: 0.281250 (0.165179)\n",
      "[30/251] Loss: 2.290564 (2.278188) Accuracy: 0.156250 (0.177419)\n",
      "[40/251] Loss: 2.256544 (2.274560) Accuracy: 0.218750 (0.191311)\n",
      "[50/251] Loss: 2.234152 (2.274061) Accuracy: 0.281250 (0.189338)\n",
      "[60/251] Loss: 2.237920 (2.272935) Accuracy: 0.218750 (0.189037)\n",
      "[70/251] Loss: 2.307899 (2.274824) Accuracy: 0.093750 (0.186180)\n",
      "[80/251] Loss: 2.245760 (2.273721) Accuracy: 0.156250 (0.183256)\n",
      "[90/251] Loss: 2.231855 (2.273073) Accuracy: 0.343750 (0.184409)\n",
      "[100/251] Loss: 2.270444 (2.274193) Accuracy: 0.218750 (0.182550)\n",
      "[110/251] Loss: 2.285422 (2.275940) Accuracy: 0.156250 (0.179336)\n",
      "[120/251] Loss: 2.332748 (2.276549) Accuracy: 0.031250 (0.175620)\n",
      "[130/251] Loss: 2.270363 (2.276291) Accuracy: 0.187500 (0.175573)\n",
      "[140/251] Loss: 2.278389 (2.278198) Accuracy: 0.187500 (0.172207)\n",
      "[150/251] Loss: 2.246872 (2.278538) Accuracy: 0.312500 (0.171358)\n",
      "[160/251] Loss: 2.291091 (2.278982) Accuracy: 0.156250 (0.170419)\n",
      "[170/251] Loss: 2.271772 (2.278749) Accuracy: 0.156250 (0.170504)\n",
      "[180/251] Loss: 2.290845 (2.277993) Accuracy: 0.062500 (0.170925)\n",
      "[190/251] Loss: 2.290755 (2.278535) Accuracy: 0.125000 (0.169339)\n",
      "[200/251] Loss: 2.252065 (2.278034) Accuracy: 0.250000 (0.170398)\n",
      "[210/251] Loss: 2.281676 (2.278503) Accuracy: 0.093750 (0.169283)\n",
      "[220/251] Loss: 2.291708 (2.279231) Accuracy: 0.156250 (0.167421)\n",
      "[230/251] Loss: 2.306314 (2.280258) Accuracy: 0.125000 (0.164367)\n",
      "[240/251] Loss: 2.347877 (2.280073) Accuracy: 0.000000 (0.164938)\n",
      "[250/251] Loss: 2.388371 (2.280988) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312157 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271323 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278546 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274596 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284595 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300749 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 96/100, Train Loss: 2.2810, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.295407 (2.295407) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.249387 (2.274876) Accuracy: 0.250000 (0.178977)\n",
      "[20/251] Loss: 2.228936 (2.275431) Accuracy: 0.250000 (0.174107)\n",
      "[30/251] Loss: 2.249341 (2.279390) Accuracy: 0.250000 (0.167339)\n",
      "[40/251] Loss: 2.290204 (2.277010) Accuracy: 0.218750 (0.174543)\n",
      "[50/251] Loss: 2.249305 (2.275748) Accuracy: 0.218750 (0.178309)\n",
      "[60/251] Loss: 2.267368 (2.277623) Accuracy: 0.125000 (0.170594)\n",
      "[70/251] Loss: 2.304182 (2.278808) Accuracy: 0.093750 (0.168574)\n",
      "[80/251] Loss: 2.294802 (2.278052) Accuracy: 0.062500 (0.168981)\n",
      "[90/251] Loss: 2.289419 (2.277526) Accuracy: 0.187500 (0.170673)\n",
      "[100/251] Loss: 2.284338 (2.278529) Accuracy: 0.156250 (0.169245)\n",
      "[110/251] Loss: 2.278895 (2.280027) Accuracy: 0.093750 (0.164977)\n",
      "[120/251] Loss: 2.294223 (2.279774) Accuracy: 0.125000 (0.166064)\n",
      "[130/251] Loss: 2.317378 (2.279729) Accuracy: 0.093750 (0.167223)\n",
      "[140/251] Loss: 2.325190 (2.279485) Accuracy: 0.031250 (0.166445)\n",
      "[150/251] Loss: 2.269804 (2.279874) Accuracy: 0.218750 (0.165563)\n",
      "[160/251] Loss: 2.314951 (2.280142) Accuracy: 0.093750 (0.165179)\n",
      "[170/251] Loss: 2.219796 (2.279512) Accuracy: 0.343750 (0.166484)\n",
      "[180/251] Loss: 2.270115 (2.278895) Accuracy: 0.156250 (0.167472)\n",
      "[190/251] Loss: 2.255836 (2.279131) Accuracy: 0.156250 (0.166721)\n",
      "[200/251] Loss: 2.285214 (2.279496) Accuracy: 0.125000 (0.165889)\n",
      "[210/251] Loss: 2.300971 (2.280992) Accuracy: 0.156250 (0.163655)\n",
      "[220/251] Loss: 2.282287 (2.280788) Accuracy: 0.093750 (0.164027)\n",
      "[230/251] Loss: 2.262627 (2.280794) Accuracy: 0.218750 (0.163826)\n",
      "[240/251] Loss: 2.252487 (2.280813) Accuracy: 0.156250 (0.162993)\n",
      "[250/251] Loss: 2.406428 (2.281058) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312158 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271323 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278546 (2.306191) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274596 (2.304506) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284595 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300749 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 97/100, Train Loss: 2.2811, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.270192 (2.270192) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.274124 (2.279995) Accuracy: 0.187500 (0.164773)\n",
      "[20/251] Loss: 2.309012 (2.281728) Accuracy: 0.062500 (0.156250)\n",
      "[30/251] Loss: 2.304372 (2.284158) Accuracy: 0.125000 (0.154234)\n",
      "[40/251] Loss: 2.288400 (2.285384) Accuracy: 0.187500 (0.152439)\n",
      "[50/251] Loss: 2.309486 (2.286622) Accuracy: 0.093750 (0.149510)\n",
      "[60/251] Loss: 2.302576 (2.286921) Accuracy: 0.093750 (0.149590)\n",
      "[70/251] Loss: 2.266905 (2.284671) Accuracy: 0.187500 (0.150968)\n",
      "[80/251] Loss: 2.242924 (2.283704) Accuracy: 0.218750 (0.151235)\n",
      "[90/251] Loss: 2.247170 (2.283958) Accuracy: 0.250000 (0.151442)\n",
      "[100/251] Loss: 2.257930 (2.282190) Accuracy: 0.281250 (0.155941)\n",
      "[110/251] Loss: 2.285590 (2.283357) Accuracy: 0.250000 (0.154279)\n",
      "[120/251] Loss: 2.268755 (2.283050) Accuracy: 0.218750 (0.156250)\n",
      "[130/251] Loss: 2.274522 (2.282586) Accuracy: 0.156250 (0.157204)\n",
      "[140/251] Loss: 2.289772 (2.282109) Accuracy: 0.093750 (0.158245)\n",
      "[150/251] Loss: 2.340353 (2.282560) Accuracy: 0.031250 (0.159147)\n",
      "[160/251] Loss: 2.292955 (2.282767) Accuracy: 0.187500 (0.159938)\n",
      "[170/251] Loss: 2.289765 (2.282480) Accuracy: 0.125000 (0.160088)\n",
      "[180/251] Loss: 2.287871 (2.282186) Accuracy: 0.156250 (0.160912)\n",
      "[190/251] Loss: 2.277359 (2.281693) Accuracy: 0.218750 (0.161813)\n",
      "[200/251] Loss: 2.276862 (2.280858) Accuracy: 0.125000 (0.163091)\n",
      "[210/251] Loss: 2.288886 (2.281215) Accuracy: 0.156250 (0.162618)\n",
      "[220/251] Loss: 2.246344 (2.280845) Accuracy: 0.250000 (0.163744)\n",
      "[230/251] Loss: 2.314093 (2.281082) Accuracy: 0.062500 (0.163420)\n",
      "[240/251] Loss: 2.271801 (2.280673) Accuracy: 0.187500 (0.163382)\n",
      "[250/251] Loss: 2.213937 (2.280315) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312158 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271323 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306192) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274596 (2.304507) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284596 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300750 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 98/100, Train Loss: 2.2803, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.275651 (2.275651) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.267803 (2.274684) Accuracy: 0.281250 (0.178977)\n",
      "[20/251] Loss: 2.249551 (2.279111) Accuracy: 0.187500 (0.160714)\n",
      "[30/251] Loss: 2.311868 (2.280051) Accuracy: 0.093750 (0.161290)\n",
      "[40/251] Loss: 2.291980 (2.274917) Accuracy: 0.093750 (0.173780)\n",
      "[50/251] Loss: 2.230281 (2.276615) Accuracy: 0.218750 (0.166054)\n",
      "[60/251] Loss: 2.287552 (2.276903) Accuracy: 0.125000 (0.167520)\n",
      "[70/251] Loss: 2.283478 (2.277564) Accuracy: 0.218750 (0.167694)\n",
      "[80/251] Loss: 2.274290 (2.277978) Accuracy: 0.125000 (0.168210)\n",
      "[90/251] Loss: 2.232001 (2.276786) Accuracy: 0.250000 (0.169986)\n",
      "[100/251] Loss: 2.282190 (2.277692) Accuracy: 0.093750 (0.168007)\n",
      "[110/251] Loss: 2.325077 (2.278651) Accuracy: 0.062500 (0.167511)\n",
      "[120/251] Loss: 2.339121 (2.279242) Accuracy: 0.062500 (0.166581)\n",
      "[130/251] Loss: 2.311397 (2.279298) Accuracy: 0.062500 (0.166269)\n",
      "[140/251] Loss: 2.267778 (2.279314) Accuracy: 0.218750 (0.166888)\n",
      "[150/251] Loss: 2.260283 (2.279600) Accuracy: 0.250000 (0.167219)\n",
      "[160/251] Loss: 2.325586 (2.279974) Accuracy: 0.031250 (0.166731)\n",
      "[170/251] Loss: 2.326286 (2.280415) Accuracy: 0.125000 (0.165936)\n",
      "[180/251] Loss: 2.286117 (2.280595) Accuracy: 0.187500 (0.166954)\n",
      "[190/251] Loss: 2.313735 (2.279726) Accuracy: 0.093750 (0.168357)\n",
      "[200/251] Loss: 2.309169 (2.280269) Accuracy: 0.062500 (0.166978)\n",
      "[210/251] Loss: 2.268970 (2.280464) Accuracy: 0.187500 (0.165432)\n",
      "[220/251] Loss: 2.296922 (2.281020) Accuracy: 0.062500 (0.163886)\n",
      "[230/251] Loss: 2.283497 (2.281062) Accuracy: 0.062500 (0.162608)\n",
      "[240/251] Loss: 2.354435 (2.281053) Accuracy: 0.000000 (0.162733)\n",
      "[250/251] Loss: 2.419584 (2.281108) Accuracy: 0.000000 (0.162973)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312158 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271323 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306192) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274596 (2.304507) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284596 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300750 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 99/100, Train Loss: 2.2811, Train Acc: 0.1630, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "[0/251] Loss: 2.295884 (2.295884) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.260840 (2.282132) Accuracy: 0.187500 (0.150568)\n",
      "[20/251] Loss: 2.310464 (2.277901) Accuracy: 0.031250 (0.162202)\n",
      "[30/251] Loss: 2.238451 (2.278651) Accuracy: 0.218750 (0.158266)\n",
      "[40/251] Loss: 2.227301 (2.274092) Accuracy: 0.218750 (0.167683)\n",
      "[50/251] Loss: 2.298703 (2.276491) Accuracy: 0.093750 (0.166054)\n",
      "[60/251] Loss: 2.244254 (2.276843) Accuracy: 0.218750 (0.164447)\n",
      "[70/251] Loss: 2.295972 (2.276350) Accuracy: 0.093750 (0.169014)\n",
      "[80/251] Loss: 2.275299 (2.275529) Accuracy: 0.218750 (0.173611)\n",
      "[90/251] Loss: 2.273770 (2.276503) Accuracy: 0.156250 (0.171360)\n",
      "[100/251] Loss: 2.200115 (2.275792) Accuracy: 0.250000 (0.173577)\n",
      "[110/251] Loss: 2.320669 (2.275757) Accuracy: 0.031250 (0.172860)\n",
      "[120/251] Loss: 2.300505 (2.276891) Accuracy: 0.187500 (0.171229)\n",
      "[130/251] Loss: 2.271798 (2.277928) Accuracy: 0.125000 (0.168416)\n",
      "[140/251] Loss: 2.278610 (2.278338) Accuracy: 0.156250 (0.167996)\n",
      "[150/251] Loss: 2.319239 (2.279075) Accuracy: 0.093750 (0.166598)\n",
      "[160/251] Loss: 2.265428 (2.279470) Accuracy: 0.156250 (0.164790)\n",
      "[170/251] Loss: 2.292811 (2.280037) Accuracy: 0.156250 (0.163012)\n",
      "[180/251] Loss: 2.223897 (2.279675) Accuracy: 0.312500 (0.164883)\n",
      "[190/251] Loss: 2.236998 (2.279455) Accuracy: 0.187500 (0.164758)\n",
      "[200/251] Loss: 2.250620 (2.279983) Accuracy: 0.218750 (0.163713)\n",
      "[210/251] Loss: 2.279849 (2.280262) Accuracy: 0.187500 (0.163655)\n",
      "[220/251] Loss: 2.269392 (2.280697) Accuracy: 0.218750 (0.162896)\n",
      "[230/251] Loss: 2.297569 (2.280428) Accuracy: 0.093750 (0.163961)\n",
      "[240/251] Loss: 2.271902 (2.280057) Accuracy: 0.125000 (0.164808)\n",
      "[250/251] Loss: 2.200330 (2.280262) Accuracy: 1.000000 (0.166833)\n",
      "[0/63] Loss: 2.301170 (2.301170) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.312159 (2.303003) Accuracy: 0.125000 (0.127841)\n",
      "[20/63] Loss: 2.271323 (2.300789) Accuracy: 0.125000 (0.119048)\n",
      "[30/63] Loss: 2.278547 (2.306192) Accuracy: 0.125000 (0.104839)\n",
      "[40/63] Loss: 2.274596 (2.304507) Accuracy: 0.125000 (0.109756)\n",
      "[50/63] Loss: 2.284596 (2.301793) Accuracy: 0.187500 (0.117647)\n",
      "[60/63] Loss: 2.300750 (2.302782) Accuracy: 0.156250 (0.117316)\n",
      "Epoch: 100/100, Train Loss: 2.2803, Train Acc: 0.1668, Val. Loss: 2.3027, Val. Acc: 0.1182\n",
      "Done training.\n",
      "Total length of dataset:  10000\n",
      "[0/251] Loss: 2.293165 (2.293165) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.307986 (2.312859) Accuracy: 0.062500 (0.099432)\n",
      "[20/251] Loss: 2.284779 (2.311146) Accuracy: 0.093750 (0.096726)\n",
      "[30/251] Loss: 2.319010 (2.309268) Accuracy: 0.093750 (0.104839)\n",
      "[40/251] Loss: 2.328101 (2.308510) Accuracy: 0.031250 (0.105945)\n",
      "[50/251] Loss: 2.329851 (2.307238) Accuracy: 0.031250 (0.106618)\n",
      "[60/251] Loss: 2.316176 (2.307177) Accuracy: 0.062500 (0.108094)\n",
      "[70/251] Loss: 2.312846 (2.306700) Accuracy: 0.093750 (0.112236)\n",
      "[80/251] Loss: 2.300476 (2.307969) Accuracy: 0.125000 (0.108410)\n",
      "[90/251] Loss: 2.314042 (2.307691) Accuracy: 0.062500 (0.107486)\n",
      "[100/251] Loss: 2.325285 (2.308105) Accuracy: 0.062500 (0.104889)\n",
      "[110/251] Loss: 2.308241 (2.308151) Accuracy: 0.062500 (0.101351)\n",
      "[120/251] Loss: 2.294054 (2.307537) Accuracy: 0.156250 (0.102273)\n",
      "[130/251] Loss: 2.288291 (2.307534) Accuracy: 0.125000 (0.101145)\n",
      "[140/251] Loss: 2.311441 (2.307363) Accuracy: 0.062500 (0.101064)\n",
      "[150/251] Loss: 2.283164 (2.307096) Accuracy: 0.187500 (0.100166)\n",
      "[160/251] Loss: 2.295499 (2.307076) Accuracy: 0.125000 (0.099961)\n",
      "[170/251] Loss: 2.312947 (2.307398) Accuracy: 0.062500 (0.097953)\n",
      "[180/251] Loss: 2.310570 (2.307356) Accuracy: 0.062500 (0.097894)\n",
      "[190/251] Loss: 2.291910 (2.307484) Accuracy: 0.187500 (0.097186)\n",
      "[200/251] Loss: 2.294950 (2.307160) Accuracy: 0.156250 (0.097170)\n",
      "[210/251] Loss: 2.307975 (2.307060) Accuracy: 0.093750 (0.097453)\n",
      "[220/251] Loss: 2.309526 (2.307085) Accuracy: 0.093750 (0.096861)\n",
      "[230/251] Loss: 2.322107 (2.307121) Accuracy: 0.031250 (0.096456)\n",
      "[240/251] Loss: 2.290911 (2.306800) Accuracy: 0.156250 (0.096603)\n",
      "[250/251] Loss: 2.286449 (2.306793) Accuracy: 0.000000 (0.095618)\n",
      "[0/63] Loss: 2.300833 (2.300833) Accuracy: 0.093750 (0.093750)\n",
      "[10/63] Loss: 2.319010 (2.309176) Accuracy: 0.062500 (0.088068)\n",
      "[20/63] Loss: 2.295055 (2.306027) Accuracy: 0.125000 (0.090774)\n",
      "[30/63] Loss: 2.315523 (2.306022) Accuracy: 0.031250 (0.091734)\n",
      "[40/63] Loss: 2.276036 (2.306338) Accuracy: 0.187500 (0.092226)\n",
      "[50/63] Loss: 2.283342 (2.304765) Accuracy: 0.187500 (0.101103)\n",
      "[60/63] Loss: 2.288646 (2.304460) Accuracy: 0.125000 (0.102971)\n",
      "Epoch: 1/100, Train Loss: 2.3068, Train Acc: 0.0956, Val. Loss: 2.3046, Val. Acc: 0.1018\n",
      "[0/251] Loss: 2.320676 (2.320676) Accuracy: 0.031250 (0.031250)\n",
      "[10/251] Loss: 2.298197 (2.303697) Accuracy: 0.093750 (0.096591)\n",
      "[20/251] Loss: 2.308263 (2.304522) Accuracy: 0.093750 (0.099702)\n",
      "[30/251] Loss: 2.292462 (2.304760) Accuracy: 0.218750 (0.094758)\n",
      "[40/251] Loss: 2.283292 (2.303114) Accuracy: 0.093750 (0.096799)\n",
      "[50/251] Loss: 2.318465 (2.302793) Accuracy: 0.000000 (0.098039)\n",
      "[60/251] Loss: 2.290242 (2.302733) Accuracy: 0.156250 (0.100922)\n",
      "[70/251] Loss: 2.306830 (2.303234) Accuracy: 0.031250 (0.099032)\n",
      "[80/251] Loss: 2.306144 (2.303179) Accuracy: 0.093750 (0.101852)\n",
      "[90/251] Loss: 2.301636 (2.303488) Accuracy: 0.062500 (0.103709)\n",
      "[100/251] Loss: 2.314668 (2.303751) Accuracy: 0.093750 (0.102723)\n",
      "[110/251] Loss: 2.301948 (2.303548) Accuracy: 0.125000 (0.102759)\n",
      "[120/251] Loss: 2.288021 (2.303141) Accuracy: 0.125000 (0.103564)\n",
      "[130/251] Loss: 2.313893 (2.303265) Accuracy: 0.062500 (0.103292)\n",
      "[140/251] Loss: 2.304252 (2.303587) Accuracy: 0.125000 (0.102394)\n",
      "[150/251] Loss: 2.305672 (2.303824) Accuracy: 0.093750 (0.101407)\n",
      "[160/251] Loss: 2.310271 (2.303652) Accuracy: 0.031250 (0.100738)\n",
      "[170/251] Loss: 2.314359 (2.303267) Accuracy: 0.093750 (0.101608)\n",
      "[180/251] Loss: 2.306973 (2.303379) Accuracy: 0.093750 (0.101174)\n",
      "[190/251] Loss: 2.307297 (2.303412) Accuracy: 0.093750 (0.102258)\n",
      "[200/251] Loss: 2.325459 (2.303557) Accuracy: 0.000000 (0.101835)\n",
      "[210/251] Loss: 2.313426 (2.303489) Accuracy: 0.031250 (0.101748)\n",
      "[220/251] Loss: 2.315793 (2.303389) Accuracy: 0.031250 (0.101386)\n",
      "[230/251] Loss: 2.305467 (2.303568) Accuracy: 0.156250 (0.100649)\n",
      "[240/251] Loss: 2.299916 (2.303442) Accuracy: 0.156250 (0.101011)\n",
      "[250/251] Loss: 2.312090 (2.303525) Accuracy: 0.000000 (0.100100)\n",
      "[0/63] Loss: 2.305045 (2.305045) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.296801 (2.306826) Accuracy: 0.093750 (0.090909)\n",
      "[20/63] Loss: 2.318923 (2.305816) Accuracy: 0.062500 (0.093750)\n",
      "[30/63] Loss: 2.321744 (2.306068) Accuracy: 0.031250 (0.098790)\n",
      "[40/63] Loss: 2.305253 (2.305255) Accuracy: 0.031250 (0.100610)\n",
      "[50/63] Loss: 2.312212 (2.305030) Accuracy: 0.093750 (0.101716)\n",
      "[60/63] Loss: 2.301192 (2.304839) Accuracy: 0.093750 (0.101434)\n",
      "Epoch: 2/100, Train Loss: 2.3035, Train Acc: 0.1001, Val. Loss: 2.3053, Val. Acc: 0.1008\n",
      "[0/251] Loss: 2.285325 (2.285325) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.305551 (2.304934) Accuracy: 0.156250 (0.096591)\n",
      "[20/251] Loss: 2.303866 (2.302263) Accuracy: 0.062500 (0.108631)\n",
      "[30/251] Loss: 2.322748 (2.301334) Accuracy: 0.031250 (0.108871)\n",
      "[40/251] Loss: 2.293847 (2.301410) Accuracy: 0.218750 (0.118902)\n",
      "[50/251] Loss: 2.298173 (2.300498) Accuracy: 0.062500 (0.117647)\n",
      "[60/251] Loss: 2.319355 (2.300669) Accuracy: 0.062500 (0.115779)\n",
      "[70/251] Loss: 2.260222 (2.300427) Accuracy: 0.218750 (0.117958)\n",
      "[80/251] Loss: 2.286654 (2.300936) Accuracy: 0.156250 (0.116127)\n",
      "[90/251] Loss: 2.289360 (2.300628) Accuracy: 0.125000 (0.116071)\n",
      "[100/251] Loss: 2.303927 (2.301462) Accuracy: 0.062500 (0.113552)\n",
      "[110/251] Loss: 2.286711 (2.301883) Accuracy: 0.187500 (0.111205)\n",
      "[120/251] Loss: 2.291735 (2.301933) Accuracy: 0.156250 (0.111054)\n",
      "[130/251] Loss: 2.298545 (2.301934) Accuracy: 0.000000 (0.110687)\n",
      "[140/251] Loss: 2.292765 (2.301733) Accuracy: 0.156250 (0.110594)\n",
      "[150/251] Loss: 2.298855 (2.301802) Accuracy: 0.125000 (0.111134)\n",
      "[160/251] Loss: 2.289058 (2.301791) Accuracy: 0.187500 (0.110637)\n",
      "[170/251] Loss: 2.301993 (2.301363) Accuracy: 0.093750 (0.111477)\n",
      "[180/251] Loss: 2.282535 (2.300998) Accuracy: 0.187500 (0.112742)\n",
      "[190/251] Loss: 2.323338 (2.301127) Accuracy: 0.062500 (0.112565)\n",
      "[200/251] Loss: 2.285109 (2.301379) Accuracy: 0.156250 (0.112718)\n",
      "[210/251] Loss: 2.315144 (2.301412) Accuracy: 0.031250 (0.112707)\n",
      "[220/251] Loss: 2.318886 (2.301436) Accuracy: 0.062500 (0.112698)\n",
      "[230/251] Loss: 2.289944 (2.301569) Accuracy: 0.187500 (0.112284)\n",
      "[240/251] Loss: 2.299797 (2.301674) Accuracy: 0.093750 (0.111255)\n",
      "[250/251] Loss: 2.364165 (2.301926) Accuracy: 0.000000 (0.110060)\n",
      "[0/63] Loss: 2.299007 (2.299007) Accuracy: 0.187500 (0.187500)\n",
      "[10/63] Loss: 2.311131 (2.307143) Accuracy: 0.031250 (0.079545)\n",
      "[20/63] Loss: 2.302610 (2.305062) Accuracy: 0.093750 (0.087798)\n",
      "[30/63] Loss: 2.294615 (2.304794) Accuracy: 0.125000 (0.091734)\n",
      "[40/63] Loss: 2.282837 (2.303859) Accuracy: 0.156250 (0.096799)\n",
      "[50/63] Loss: 2.300609 (2.303457) Accuracy: 0.093750 (0.100490)\n",
      "[60/63] Loss: 2.300267 (2.303713) Accuracy: 0.093750 (0.098873)\n",
      "Epoch: 3/100, Train Loss: 2.3019, Train Acc: 0.1101, Val. Loss: 2.3040, Val. Acc: 0.0978\n",
      "[0/251] Loss: 2.299420 (2.299420) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.272400 (2.289070) Accuracy: 0.187500 (0.136364)\n",
      "[20/251] Loss: 2.293286 (2.295206) Accuracy: 0.125000 (0.126488)\n",
      "[30/251] Loss: 2.315701 (2.298107) Accuracy: 0.125000 (0.126008)\n",
      "[40/251] Loss: 2.309206 (2.297973) Accuracy: 0.093750 (0.124238)\n",
      "[50/251] Loss: 2.312164 (2.298781) Accuracy: 0.031250 (0.115809)\n",
      "[60/251] Loss: 2.282192 (2.298275) Accuracy: 0.156250 (0.115266)\n",
      "[70/251] Loss: 2.309788 (2.298823) Accuracy: 0.093750 (0.115317)\n",
      "[80/251] Loss: 2.300756 (2.299491) Accuracy: 0.187500 (0.116127)\n",
      "[90/251] Loss: 2.313314 (2.299968) Accuracy: 0.093750 (0.114698)\n",
      "[100/251] Loss: 2.292260 (2.300246) Accuracy: 0.156250 (0.114171)\n",
      "[110/251] Loss: 2.277105 (2.299744) Accuracy: 0.156250 (0.115991)\n",
      "[120/251] Loss: 2.314471 (2.300056) Accuracy: 0.062500 (0.116477)\n",
      "[130/251] Loss: 2.311122 (2.300185) Accuracy: 0.031250 (0.114265)\n",
      "[140/251] Loss: 2.320533 (2.300063) Accuracy: 0.031250 (0.113475)\n",
      "[150/251] Loss: 2.298785 (2.300001) Accuracy: 0.156250 (0.114238)\n",
      "[160/251] Loss: 2.289582 (2.300652) Accuracy: 0.250000 (0.114325)\n",
      "[170/251] Loss: 2.301172 (2.300710) Accuracy: 0.093750 (0.113487)\n",
      "[180/251] Loss: 2.288667 (2.300670) Accuracy: 0.093750 (0.113605)\n",
      "[190/251] Loss: 2.280149 (2.300680) Accuracy: 0.281250 (0.113711)\n",
      "[200/251] Loss: 2.299684 (2.300750) Accuracy: 0.125000 (0.114117)\n",
      "[210/251] Loss: 2.273474 (2.300326) Accuracy: 0.218750 (0.116558)\n",
      "[220/251] Loss: 2.304534 (2.300463) Accuracy: 0.031250 (0.115950)\n",
      "[230/251] Loss: 2.293134 (2.300483) Accuracy: 0.156250 (0.115801)\n",
      "[240/251] Loss: 2.309439 (2.300512) Accuracy: 0.031250 (0.114886)\n",
      "[250/251] Loss: 2.259505 (2.300308) Accuracy: 0.000000 (0.114417)\n",
      "[0/63] Loss: 2.297414 (2.297414) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.301394 (2.310039) Accuracy: 0.062500 (0.071023)\n",
      "[20/63] Loss: 2.321431 (2.309524) Accuracy: 0.093750 (0.078869)\n",
      "[30/63] Loss: 2.294457 (2.309988) Accuracy: 0.125000 (0.072581)\n",
      "[40/63] Loss: 2.295053 (2.307697) Accuracy: 0.093750 (0.081555)\n",
      "[50/63] Loss: 2.303893 (2.307116) Accuracy: 0.156250 (0.088848)\n",
      "[60/63] Loss: 2.325099 (2.307458) Accuracy: 0.031250 (0.091189)\n",
      "Epoch: 4/100, Train Loss: 2.3003, Train Acc: 0.1144, Val. Loss: 2.3078, Val. Acc: 0.0903\n",
      "[0/251] Loss: 2.281217 (2.281217) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.275564 (2.291248) Accuracy: 0.187500 (0.150568)\n",
      "[20/251] Loss: 2.306326 (2.297469) Accuracy: 0.062500 (0.117560)\n",
      "[30/251] Loss: 2.304951 (2.297636) Accuracy: 0.125000 (0.128024)\n",
      "[40/251] Loss: 2.303596 (2.297463) Accuracy: 0.062500 (0.132622)\n",
      "[50/251] Loss: 2.302266 (2.297733) Accuracy: 0.125000 (0.129902)\n",
      "[60/251] Loss: 2.303479 (2.299732) Accuracy: 0.062500 (0.122439)\n",
      "[70/251] Loss: 2.296418 (2.298973) Accuracy: 0.093750 (0.124120)\n",
      "[80/251] Loss: 2.301874 (2.299128) Accuracy: 0.093750 (0.125000)\n",
      "[90/251] Loss: 2.311201 (2.299048) Accuracy: 0.031250 (0.127060)\n",
      "[100/251] Loss: 2.307843 (2.299474) Accuracy: 0.062500 (0.125619)\n",
      "[110/251] Loss: 2.296543 (2.299023) Accuracy: 0.093750 (0.127252)\n",
      "[120/251] Loss: 2.307208 (2.299086) Accuracy: 0.062500 (0.127066)\n",
      "[130/251] Loss: 2.291778 (2.299012) Accuracy: 0.250000 (0.125000)\n",
      "[140/251] Loss: 2.307206 (2.298273) Accuracy: 0.031250 (0.126995)\n",
      "[150/251] Loss: 2.331836 (2.298435) Accuracy: 0.062500 (0.125414)\n",
      "[160/251] Loss: 2.307550 (2.298095) Accuracy: 0.156250 (0.125582)\n",
      "[170/251] Loss: 2.325405 (2.298504) Accuracy: 0.031250 (0.124086)\n",
      "[180/251] Loss: 2.288694 (2.298722) Accuracy: 0.125000 (0.123791)\n",
      "[190/251] Loss: 2.278822 (2.298610) Accuracy: 0.125000 (0.123364)\n",
      "[200/251] Loss: 2.307841 (2.298778) Accuracy: 0.093750 (0.122357)\n",
      "[210/251] Loss: 2.302245 (2.298710) Accuracy: 0.093750 (0.122482)\n",
      "[220/251] Loss: 2.312283 (2.298856) Accuracy: 0.125000 (0.121041)\n",
      "[230/251] Loss: 2.306000 (2.299293) Accuracy: 0.125000 (0.119859)\n",
      "[240/251] Loss: 2.285145 (2.299187) Accuracy: 0.093750 (0.119684)\n",
      "[250/251] Loss: 2.386054 (2.299536) Accuracy: 0.000000 (0.118899)\n",
      "[0/63] Loss: 2.307877 (2.307877) Accuracy: 0.093750 (0.093750)\n",
      "[10/63] Loss: 2.305394 (2.307923) Accuracy: 0.062500 (0.102273)\n",
      "[20/63] Loss: 2.309144 (2.306365) Accuracy: 0.062500 (0.095238)\n",
      "[30/63] Loss: 2.304969 (2.306538) Accuracy: 0.062500 (0.096774)\n",
      "[40/63] Loss: 2.297418 (2.305744) Accuracy: 0.062500 (0.095274)\n",
      "[50/63] Loss: 2.302360 (2.305663) Accuracy: 0.156250 (0.098039)\n",
      "[60/63] Loss: 2.314177 (2.305998) Accuracy: 0.031250 (0.097848)\n",
      "Epoch: 5/100, Train Loss: 2.2995, Train Acc: 0.1189, Val. Loss: 2.3062, Val. Acc: 0.0962\n",
      "[0/251] Loss: 2.295455 (2.295455) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.318111 (2.303661) Accuracy: 0.031250 (0.125000)\n",
      "[20/251] Loss: 2.280181 (2.298138) Accuracy: 0.187500 (0.120536)\n",
      "[30/251] Loss: 2.275504 (2.297896) Accuracy: 0.250000 (0.121976)\n",
      "[40/251] Loss: 2.293757 (2.298889) Accuracy: 0.125000 (0.120427)\n",
      "[50/251] Loss: 2.328453 (2.298149) Accuracy: 0.031250 (0.116422)\n",
      "[60/251] Loss: 2.325785 (2.299125) Accuracy: 0.031250 (0.114242)\n",
      "[70/251] Loss: 2.314786 (2.300052) Accuracy: 0.093750 (0.116197)\n",
      "[80/251] Loss: 2.296122 (2.300062) Accuracy: 0.125000 (0.114969)\n",
      "[90/251] Loss: 2.292462 (2.300302) Accuracy: 0.156250 (0.114011)\n",
      "[100/251] Loss: 2.304476 (2.300142) Accuracy: 0.031250 (0.115408)\n",
      "[110/251] Loss: 2.301994 (2.299158) Accuracy: 0.156250 (0.118525)\n",
      "[120/251] Loss: 2.313514 (2.299598) Accuracy: 0.093750 (0.117252)\n",
      "[130/251] Loss: 2.323268 (2.298468) Accuracy: 0.062500 (0.119752)\n",
      "[140/251] Loss: 2.305968 (2.298580) Accuracy: 0.093750 (0.118351)\n",
      "[150/251] Loss: 2.303940 (2.298837) Accuracy: 0.093750 (0.116515)\n",
      "[160/251] Loss: 2.273514 (2.298411) Accuracy: 0.218750 (0.118789)\n",
      "[170/251] Loss: 2.296840 (2.298533) Accuracy: 0.218750 (0.120066)\n",
      "[180/251] Loss: 2.284007 (2.298434) Accuracy: 0.156250 (0.120166)\n",
      "[190/251] Loss: 2.272897 (2.298009) Accuracy: 0.125000 (0.120582)\n",
      "[200/251] Loss: 2.299254 (2.298148) Accuracy: 0.093750 (0.120802)\n",
      "[210/251] Loss: 2.322519 (2.298017) Accuracy: 0.062500 (0.122186)\n",
      "[220/251] Loss: 2.303864 (2.297746) Accuracy: 0.093750 (0.123020)\n",
      "[230/251] Loss: 2.284051 (2.297854) Accuracy: 0.187500 (0.122835)\n",
      "[240/251] Loss: 2.303973 (2.297879) Accuracy: 0.062500 (0.122407)\n",
      "[250/251] Loss: 2.314761 (2.297874) Accuracy: 0.000000 (0.122385)\n",
      "[0/63] Loss: 2.301019 (2.301019) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.309375 (2.310632) Accuracy: 0.062500 (0.082386)\n",
      "[20/63] Loss: 2.313333 (2.307057) Accuracy: 0.031250 (0.080357)\n",
      "[30/63] Loss: 2.301708 (2.306425) Accuracy: 0.093750 (0.083669)\n",
      "[40/63] Loss: 2.291622 (2.306305) Accuracy: 0.125000 (0.089177)\n",
      "[50/63] Loss: 2.301280 (2.305156) Accuracy: 0.156250 (0.096814)\n",
      "[60/63] Loss: 2.319109 (2.305858) Accuracy: 0.031250 (0.094262)\n",
      "Epoch: 6/100, Train Loss: 2.2979, Train Acc: 0.1224, Val. Loss: 2.3060, Val. Acc: 0.0944\n",
      "[0/251] Loss: 2.301849 (2.301849) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.312895 (2.300909) Accuracy: 0.062500 (0.116477)\n",
      "[20/251] Loss: 2.272825 (2.295589) Accuracy: 0.281250 (0.132440)\n",
      "[30/251] Loss: 2.303661 (2.296018) Accuracy: 0.125000 (0.128024)\n",
      "[40/251] Loss: 2.302695 (2.298027) Accuracy: 0.093750 (0.121189)\n",
      "[50/251] Loss: 2.303510 (2.296259) Accuracy: 0.093750 (0.125613)\n",
      "[60/251] Loss: 2.293851 (2.296974) Accuracy: 0.093750 (0.126025)\n",
      "[70/251] Loss: 2.287050 (2.297290) Accuracy: 0.156250 (0.127201)\n",
      "[80/251] Loss: 2.288378 (2.297903) Accuracy: 0.156250 (0.124614)\n",
      "[90/251] Loss: 2.315006 (2.297718) Accuracy: 0.062500 (0.125343)\n",
      "[100/251] Loss: 2.311260 (2.298250) Accuracy: 0.062500 (0.121597)\n",
      "[110/251] Loss: 2.309304 (2.297617) Accuracy: 0.093750 (0.125845)\n",
      "[120/251] Loss: 2.326975 (2.297774) Accuracy: 0.031250 (0.124483)\n",
      "[130/251] Loss: 2.301686 (2.297992) Accuracy: 0.093750 (0.124523)\n",
      "[140/251] Loss: 2.275333 (2.297770) Accuracy: 0.187500 (0.125443)\n",
      "[150/251] Loss: 2.264352 (2.297600) Accuracy: 0.281250 (0.125621)\n",
      "[160/251] Loss: 2.288657 (2.297319) Accuracy: 0.125000 (0.125776)\n",
      "[170/251] Loss: 2.293608 (2.296982) Accuracy: 0.187500 (0.125731)\n",
      "[180/251] Loss: 2.300911 (2.297206) Accuracy: 0.093750 (0.124655)\n",
      "[190/251] Loss: 2.301963 (2.297296) Accuracy: 0.093750 (0.123527)\n",
      "[200/251] Loss: 2.252847 (2.296948) Accuracy: 0.250000 (0.124378)\n",
      "[210/251] Loss: 2.282595 (2.297161) Accuracy: 0.156250 (0.123815)\n",
      "[220/251] Loss: 2.301845 (2.296828) Accuracy: 0.156250 (0.125000)\n",
      "[230/251] Loss: 2.270727 (2.296540) Accuracy: 0.156250 (0.124865)\n",
      "[240/251] Loss: 2.311633 (2.296534) Accuracy: 0.187500 (0.125259)\n",
      "[250/251] Loss: 2.310081 (2.296522) Accuracy: 0.000000 (0.124626)\n",
      "[0/63] Loss: 2.308073 (2.308073) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.306146 (2.310831) Accuracy: 0.062500 (0.088068)\n",
      "[20/63] Loss: 2.318614 (2.306763) Accuracy: 0.062500 (0.098214)\n",
      "[30/63] Loss: 2.304612 (2.307037) Accuracy: 0.062500 (0.096774)\n",
      "[40/63] Loss: 2.303214 (2.307356) Accuracy: 0.031250 (0.090701)\n",
      "[50/63] Loss: 2.300627 (2.306516) Accuracy: 0.125000 (0.096201)\n",
      "[60/63] Loss: 2.321539 (2.306995) Accuracy: 0.062500 (0.094775)\n",
      "Epoch: 7/100, Train Loss: 2.2965, Train Acc: 0.1246, Val. Loss: 2.3070, Val. Acc: 0.0928\n",
      "[0/251] Loss: 2.288095 (2.288095) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.311520 (2.295591) Accuracy: 0.125000 (0.142045)\n",
      "[20/251] Loss: 2.310931 (2.296568) Accuracy: 0.125000 (0.133929)\n",
      "[30/251] Loss: 2.273163 (2.293973) Accuracy: 0.156250 (0.134073)\n",
      "[40/251] Loss: 2.310988 (2.293478) Accuracy: 0.062500 (0.136433)\n",
      "[50/251] Loss: 2.298804 (2.292695) Accuracy: 0.125000 (0.137255)\n",
      "[60/251] Loss: 2.258960 (2.291689) Accuracy: 0.250000 (0.141906)\n",
      "[70/251] Loss: 2.287404 (2.290121) Accuracy: 0.156250 (0.147007)\n",
      "[80/251] Loss: 2.300776 (2.291085) Accuracy: 0.093750 (0.142747)\n",
      "[90/251] Loss: 2.309602 (2.291641) Accuracy: 0.093750 (0.140110)\n",
      "[100/251] Loss: 2.314373 (2.291437) Accuracy: 0.093750 (0.139851)\n",
      "[110/251] Loss: 2.310729 (2.292766) Accuracy: 0.156250 (0.137669)\n",
      "[120/251] Loss: 2.280800 (2.292998) Accuracy: 0.125000 (0.136105)\n",
      "[130/251] Loss: 2.266629 (2.292705) Accuracy: 0.156250 (0.136212)\n",
      "[140/251] Loss: 2.272247 (2.293393) Accuracy: 0.156250 (0.134530)\n",
      "[150/251] Loss: 2.304824 (2.293875) Accuracy: 0.062500 (0.132864)\n",
      "[160/251] Loss: 2.334348 (2.294253) Accuracy: 0.031250 (0.131988)\n",
      "[170/251] Loss: 2.284596 (2.294009) Accuracy: 0.125000 (0.132675)\n",
      "[180/251] Loss: 2.317797 (2.293941) Accuracy: 0.031250 (0.132424)\n",
      "[190/251] Loss: 2.269945 (2.293908) Accuracy: 0.156250 (0.132526)\n",
      "[200/251] Loss: 2.307752 (2.294033) Accuracy: 0.125000 (0.133240)\n",
      "[210/251] Loss: 2.295987 (2.294412) Accuracy: 0.156250 (0.132109)\n",
      "[220/251] Loss: 2.319219 (2.294705) Accuracy: 0.062500 (0.130515)\n",
      "[230/251] Loss: 2.282977 (2.294752) Accuracy: 0.093750 (0.129870)\n",
      "[240/251] Loss: 2.309116 (2.294961) Accuracy: 0.062500 (0.129409)\n",
      "[250/251] Loss: 2.344411 (2.295095) Accuracy: 0.000000 (0.128860)\n",
      "[0/63] Loss: 2.298900 (2.298900) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.299752 (2.307231) Accuracy: 0.062500 (0.079545)\n",
      "[20/63] Loss: 2.311651 (2.305228) Accuracy: 0.062500 (0.096726)\n",
      "[30/63] Loss: 2.308182 (2.305881) Accuracy: 0.000000 (0.093750)\n",
      "[40/63] Loss: 2.302389 (2.306300) Accuracy: 0.062500 (0.096037)\n",
      "[50/63] Loss: 2.306144 (2.306607) Accuracy: 0.125000 (0.095588)\n",
      "[60/63] Loss: 2.312676 (2.306950) Accuracy: 0.062500 (0.093238)\n",
      "Epoch: 8/100, Train Loss: 2.2951, Train Acc: 0.1289, Val. Loss: 2.3070, Val. Acc: 0.0933\n",
      "[0/251] Loss: 2.298084 (2.298084) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.275043 (2.299110) Accuracy: 0.187500 (0.096591)\n",
      "[20/251] Loss: 2.298639 (2.299710) Accuracy: 0.125000 (0.098214)\n",
      "[30/251] Loss: 2.307133 (2.296995) Accuracy: 0.093750 (0.110887)\n",
      "[40/251] Loss: 2.302106 (2.296924) Accuracy: 0.187500 (0.123476)\n",
      "[50/251] Loss: 2.304511 (2.295705) Accuracy: 0.156250 (0.124387)\n",
      "[60/251] Loss: 2.307264 (2.294642) Accuracy: 0.125000 (0.127049)\n",
      "[70/251] Loss: 2.293010 (2.294316) Accuracy: 0.125000 (0.129842)\n",
      "[80/251] Loss: 2.296877 (2.293822) Accuracy: 0.093750 (0.131944)\n",
      "[90/251] Loss: 2.290054 (2.293075) Accuracy: 0.156250 (0.134272)\n",
      "[100/251] Loss: 2.282239 (2.293565) Accuracy: 0.125000 (0.131498)\n",
      "[110/251] Loss: 2.295973 (2.293346) Accuracy: 0.125000 (0.132601)\n",
      "[120/251] Loss: 2.275527 (2.293746) Accuracy: 0.218750 (0.132748)\n",
      "[130/251] Loss: 2.278628 (2.293258) Accuracy: 0.156250 (0.133588)\n",
      "[140/251] Loss: 2.253555 (2.293579) Accuracy: 0.218750 (0.133200)\n",
      "[150/251] Loss: 2.309242 (2.293781) Accuracy: 0.062500 (0.133071)\n",
      "[160/251] Loss: 2.306247 (2.294146) Accuracy: 0.093750 (0.132182)\n",
      "[170/251] Loss: 2.281426 (2.293649) Accuracy: 0.156250 (0.133224)\n",
      "[180/251] Loss: 2.309690 (2.293524) Accuracy: 0.125000 (0.133460)\n",
      "[190/251] Loss: 2.320465 (2.293662) Accuracy: 0.156250 (0.132526)\n",
      "[200/251] Loss: 2.316055 (2.293607) Accuracy: 0.000000 (0.131841)\n",
      "[210/251] Loss: 2.295213 (2.293455) Accuracy: 0.156250 (0.132109)\n",
      "[220/251] Loss: 2.268691 (2.293622) Accuracy: 0.156250 (0.131222)\n",
      "[230/251] Loss: 2.314356 (2.293913) Accuracy: 0.093750 (0.130817)\n",
      "[240/251] Loss: 2.316634 (2.294153) Accuracy: 0.062500 (0.130576)\n",
      "[250/251] Loss: 2.360330 (2.294398) Accuracy: 0.000000 (0.129482)\n",
      "[0/63] Loss: 2.299679 (2.299679) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.302934 (2.309172) Accuracy: 0.093750 (0.085227)\n",
      "[20/63] Loss: 2.311385 (2.306739) Accuracy: 0.031250 (0.095238)\n",
      "[30/63] Loss: 2.305906 (2.306152) Accuracy: 0.093750 (0.095766)\n",
      "[40/63] Loss: 2.293834 (2.306288) Accuracy: 0.031250 (0.095274)\n",
      "[50/63] Loss: 2.303045 (2.306531) Accuracy: 0.125000 (0.094975)\n",
      "[60/63] Loss: 2.311208 (2.306882) Accuracy: 0.125000 (0.094262)\n",
      "Epoch: 9/100, Train Loss: 2.2944, Train Acc: 0.1295, Val. Loss: 2.3071, Val. Acc: 0.0938\n",
      "[0/251] Loss: 2.274205 (2.274205) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.299468 (2.292981) Accuracy: 0.156250 (0.127841)\n",
      "[20/251] Loss: 2.306063 (2.294907) Accuracy: 0.156250 (0.129464)\n",
      "[30/251] Loss: 2.294575 (2.292391) Accuracy: 0.156250 (0.135081)\n",
      "[40/251] Loss: 2.309444 (2.292569) Accuracy: 0.093750 (0.128811)\n",
      "[50/251] Loss: 2.249730 (2.292247) Accuracy: 0.250000 (0.129902)\n",
      "[60/251] Loss: 2.299965 (2.293289) Accuracy: 0.218750 (0.126537)\n",
      "[70/251] Loss: 2.309904 (2.291601) Accuracy: 0.062500 (0.130722)\n",
      "[80/251] Loss: 2.258843 (2.292489) Accuracy: 0.218750 (0.129630)\n",
      "[90/251] Loss: 2.256379 (2.291630) Accuracy: 0.187500 (0.134959)\n",
      "[100/251] Loss: 2.283270 (2.291722) Accuracy: 0.156250 (0.134592)\n",
      "[110/251] Loss: 2.287276 (2.291413) Accuracy: 0.000000 (0.132038)\n",
      "[120/251] Loss: 2.297355 (2.291576) Accuracy: 0.125000 (0.130940)\n",
      "[130/251] Loss: 2.271259 (2.291782) Accuracy: 0.187500 (0.131679)\n",
      "[140/251] Loss: 2.291253 (2.291489) Accuracy: 0.156250 (0.133200)\n",
      "[150/251] Loss: 2.282315 (2.291965) Accuracy: 0.093750 (0.132864)\n",
      "[160/251] Loss: 2.295664 (2.292450) Accuracy: 0.218750 (0.133152)\n",
      "[170/251] Loss: 2.315391 (2.292528) Accuracy: 0.062500 (0.132858)\n",
      "[180/251] Loss: 2.303244 (2.292956) Accuracy: 0.093750 (0.132424)\n",
      "[190/251] Loss: 2.286246 (2.292650) Accuracy: 0.093750 (0.132363)\n",
      "[200/251] Loss: 2.302023 (2.293021) Accuracy: 0.062500 (0.132774)\n",
      "[210/251] Loss: 2.272666 (2.293095) Accuracy: 0.187500 (0.133294)\n",
      "[220/251] Loss: 2.300613 (2.293007) Accuracy: 0.125000 (0.133767)\n",
      "[230/251] Loss: 2.282708 (2.292869) Accuracy: 0.218750 (0.134199)\n",
      "[240/251] Loss: 2.303117 (2.293307) Accuracy: 0.093750 (0.133169)\n",
      "[250/251] Loss: 2.033551 (2.291982) Accuracy: 1.000000 (0.137824)\n",
      "[0/63] Loss: 2.305564 (2.305564) Accuracy: 0.093750 (0.093750)\n",
      "[10/63] Loss: 2.287579 (2.308048) Accuracy: 0.093750 (0.085227)\n",
      "[20/63] Loss: 2.320969 (2.307255) Accuracy: 0.062500 (0.098214)\n",
      "[30/63] Loss: 2.307414 (2.307213) Accuracy: 0.093750 (0.098790)\n",
      "[40/63] Loss: 2.307639 (2.308674) Accuracy: 0.093750 (0.097561)\n",
      "[50/63] Loss: 2.310054 (2.308278) Accuracy: 0.125000 (0.101103)\n",
      "[60/63] Loss: 2.327841 (2.308531) Accuracy: 0.062500 (0.101947)\n",
      "Epoch: 10/100, Train Loss: 2.2920, Train Acc: 0.1378, Val. Loss: 2.3089, Val. Acc: 0.1008\n",
      "[0/251] Loss: 2.304514 (2.304514) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.292234 (2.291620) Accuracy: 0.093750 (0.150568)\n",
      "[20/251] Loss: 2.278831 (2.290076) Accuracy: 0.156250 (0.138393)\n",
      "[30/251] Loss: 2.337381 (2.293398) Accuracy: 0.031250 (0.133065)\n",
      "[40/251] Loss: 2.241709 (2.291367) Accuracy: 0.281250 (0.137195)\n",
      "[50/251] Loss: 2.261750 (2.292729) Accuracy: 0.187500 (0.136029)\n",
      "[60/251] Loss: 2.270193 (2.291411) Accuracy: 0.218750 (0.135758)\n",
      "[70/251] Loss: 2.304990 (2.291252) Accuracy: 0.062500 (0.135563)\n",
      "[80/251] Loss: 2.282712 (2.291416) Accuracy: 0.125000 (0.135417)\n",
      "[90/251] Loss: 2.279355 (2.291953) Accuracy: 0.187500 (0.135302)\n",
      "[100/251] Loss: 2.322299 (2.291994) Accuracy: 0.062500 (0.136139)\n",
      "[110/251] Loss: 2.307963 (2.291366) Accuracy: 0.093750 (0.139077)\n",
      "[120/251] Loss: 2.290425 (2.291394) Accuracy: 0.062500 (0.138171)\n",
      "[130/251] Loss: 2.291975 (2.291650) Accuracy: 0.156250 (0.138120)\n",
      "[140/251] Loss: 2.253016 (2.290814) Accuracy: 0.187500 (0.138520)\n",
      "[150/251] Loss: 2.277495 (2.291411) Accuracy: 0.218750 (0.136796)\n",
      "[160/251] Loss: 2.261425 (2.290604) Accuracy: 0.218750 (0.138587)\n",
      "[170/251] Loss: 2.260454 (2.290984) Accuracy: 0.218750 (0.138158)\n",
      "[180/251] Loss: 2.304329 (2.291310) Accuracy: 0.125000 (0.137258)\n",
      "[190/251] Loss: 2.300821 (2.291247) Accuracy: 0.187500 (0.137762)\n",
      "[200/251] Loss: 2.306592 (2.291305) Accuracy: 0.156250 (0.137749)\n",
      "[210/251] Loss: 2.274408 (2.291789) Accuracy: 0.156250 (0.136404)\n",
      "[220/251] Loss: 2.317014 (2.292271) Accuracy: 0.093750 (0.135888)\n",
      "[230/251] Loss: 2.290825 (2.291560) Accuracy: 0.062500 (0.137311)\n",
      "[240/251] Loss: 2.296329 (2.291867) Accuracy: 0.062500 (0.136281)\n",
      "[250/251] Loss: 2.106528 (2.291033) Accuracy: 1.000000 (0.139816)\n",
      "[0/63] Loss: 2.285450 (2.285450) Accuracy: 0.218750 (0.218750)\n",
      "[10/63] Loss: 2.301778 (2.310664) Accuracy: 0.093750 (0.090909)\n",
      "[20/63] Loss: 2.315325 (2.308193) Accuracy: 0.093750 (0.093750)\n",
      "[30/63] Loss: 2.304905 (2.307894) Accuracy: 0.093750 (0.091734)\n",
      "[40/63] Loss: 2.294887 (2.307836) Accuracy: 0.125000 (0.095274)\n",
      "[50/63] Loss: 2.305687 (2.307939) Accuracy: 0.156250 (0.096201)\n",
      "[60/63] Loss: 2.323527 (2.308377) Accuracy: 0.062500 (0.095287)\n",
      "Epoch: 11/100, Train Loss: 2.2910, Train Acc: 0.1398, Val. Loss: 2.3086, Val. Acc: 0.0948\n",
      "[0/251] Loss: 2.276511 (2.276511) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.273715 (2.286995) Accuracy: 0.187500 (0.142045)\n",
      "[20/251] Loss: 2.272309 (2.286038) Accuracy: 0.187500 (0.139881)\n",
      "[30/251] Loss: 2.225781 (2.288127) Accuracy: 0.343750 (0.128024)\n",
      "[40/251] Loss: 2.306661 (2.286981) Accuracy: 0.093750 (0.137195)\n",
      "[50/251] Loss: 2.316205 (2.288141) Accuracy: 0.093750 (0.136642)\n",
      "[60/251] Loss: 2.294249 (2.287640) Accuracy: 0.218750 (0.138832)\n",
      "[70/251] Loss: 2.275781 (2.288198) Accuracy: 0.187500 (0.139525)\n",
      "[80/251] Loss: 2.298886 (2.289079) Accuracy: 0.093750 (0.138117)\n",
      "[90/251] Loss: 2.315083 (2.290350) Accuracy: 0.031250 (0.135989)\n",
      "[100/251] Loss: 2.312612 (2.290119) Accuracy: 0.093750 (0.136757)\n",
      "[110/251] Loss: 2.320473 (2.290163) Accuracy: 0.062500 (0.137387)\n",
      "[120/251] Loss: 2.280711 (2.290228) Accuracy: 0.187500 (0.139205)\n",
      "[130/251] Loss: 2.304646 (2.289947) Accuracy: 0.062500 (0.140744)\n",
      "[140/251] Loss: 2.303282 (2.289873) Accuracy: 0.187500 (0.141622)\n",
      "[150/251] Loss: 2.282535 (2.289756) Accuracy: 0.156250 (0.141349)\n",
      "[160/251] Loss: 2.280419 (2.290372) Accuracy: 0.156250 (0.140334)\n",
      "[170/251] Loss: 2.283071 (2.290754) Accuracy: 0.187500 (0.139803)\n",
      "[180/251] Loss: 2.300862 (2.290832) Accuracy: 0.156250 (0.139503)\n",
      "[190/251] Loss: 2.303777 (2.290720) Accuracy: 0.156250 (0.140052)\n",
      "[200/251] Loss: 2.310546 (2.291009) Accuracy: 0.093750 (0.140081)\n",
      "[210/251] Loss: 2.294257 (2.290998) Accuracy: 0.125000 (0.140403)\n",
      "[220/251] Loss: 2.268033 (2.290999) Accuracy: 0.218750 (0.140696)\n",
      "[230/251] Loss: 2.260469 (2.290890) Accuracy: 0.156250 (0.140422)\n",
      "[240/251] Loss: 2.303659 (2.290701) Accuracy: 0.156250 (0.140301)\n",
      "[250/251] Loss: 2.310797 (2.290815) Accuracy: 0.000000 (0.139069)\n",
      "[0/63] Loss: 2.285662 (2.285662) Accuracy: 0.125000 (0.125000)\n",
      "[10/63] Loss: 2.300294 (2.309206) Accuracy: 0.125000 (0.090909)\n",
      "[20/63] Loss: 2.314690 (2.307812) Accuracy: 0.062500 (0.099702)\n",
      "[30/63] Loss: 2.308730 (2.306977) Accuracy: 0.093750 (0.102823)\n",
      "[40/63] Loss: 2.293235 (2.306852) Accuracy: 0.093750 (0.105183)\n",
      "[50/63] Loss: 2.306252 (2.307793) Accuracy: 0.125000 (0.101103)\n",
      "[60/63] Loss: 2.315159 (2.308041) Accuracy: 0.093750 (0.097336)\n",
      "Epoch: 12/100, Train Loss: 2.2908, Train Acc: 0.1391, Val. Loss: 2.3083, Val. Acc: 0.0968\n",
      "[0/251] Loss: 2.287333 (2.287333) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.301799 (2.286955) Accuracy: 0.062500 (0.144886)\n",
      "[20/251] Loss: 2.291165 (2.290498) Accuracy: 0.218750 (0.138393)\n",
      "[30/251] Loss: 2.254902 (2.288020) Accuracy: 0.281250 (0.150202)\n",
      "[40/251] Loss: 2.294731 (2.289074) Accuracy: 0.062500 (0.145579)\n",
      "[50/251] Loss: 2.267338 (2.289400) Accuracy: 0.187500 (0.142770)\n",
      "[60/251] Loss: 2.238174 (2.288864) Accuracy: 0.281250 (0.147029)\n",
      "[70/251] Loss: 2.307616 (2.289514) Accuracy: 0.031250 (0.141285)\n",
      "[80/251] Loss: 2.288914 (2.289113) Accuracy: 0.125000 (0.144290)\n",
      "[90/251] Loss: 2.300764 (2.289172) Accuracy: 0.125000 (0.143887)\n",
      "[100/251] Loss: 2.277340 (2.289212) Accuracy: 0.218750 (0.142946)\n",
      "[110/251] Loss: 2.259639 (2.288448) Accuracy: 0.281250 (0.145833)\n",
      "[120/251] Loss: 2.280872 (2.287645) Accuracy: 0.218750 (0.147986)\n",
      "[130/251] Loss: 2.257411 (2.287759) Accuracy: 0.156250 (0.145515)\n",
      "[140/251] Loss: 2.263656 (2.288525) Accuracy: 0.218750 (0.143839)\n",
      "[150/251] Loss: 2.307081 (2.288542) Accuracy: 0.156250 (0.143833)\n",
      "[160/251] Loss: 2.298125 (2.289329) Accuracy: 0.156250 (0.143051)\n",
      "[170/251] Loss: 2.304704 (2.289664) Accuracy: 0.093750 (0.142544)\n",
      "[180/251] Loss: 2.309967 (2.289476) Accuracy: 0.062500 (0.143474)\n",
      "[190/251] Loss: 2.259813 (2.289204) Accuracy: 0.187500 (0.143652)\n",
      "[200/251] Loss: 2.275544 (2.289472) Accuracy: 0.062500 (0.142568)\n",
      "[210/251] Loss: 2.317926 (2.289944) Accuracy: 0.093750 (0.141143)\n",
      "[220/251] Loss: 2.297193 (2.289945) Accuracy: 0.062500 (0.141261)\n",
      "[230/251] Loss: 2.321457 (2.289871) Accuracy: 0.031250 (0.140828)\n",
      "[240/251] Loss: 2.247944 (2.289857) Accuracy: 0.218750 (0.141209)\n",
      "[250/251] Loss: 2.401555 (2.290488) Accuracy: 0.000000 (0.140438)\n",
      "[0/63] Loss: 2.289447 (2.289447) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.301496 (2.310006) Accuracy: 0.062500 (0.088068)\n",
      "[20/63] Loss: 2.315493 (2.307465) Accuracy: 0.062500 (0.096726)\n",
      "[30/63] Loss: 2.306525 (2.307109) Accuracy: 0.031250 (0.094758)\n",
      "[40/63] Loss: 2.292942 (2.306816) Accuracy: 0.093750 (0.096037)\n",
      "[50/63] Loss: 2.303000 (2.307495) Accuracy: 0.218750 (0.096814)\n",
      "[60/63] Loss: 2.321834 (2.308028) Accuracy: 0.062500 (0.094262)\n",
      "Epoch: 13/100, Train Loss: 2.2905, Train Acc: 0.1404, Val. Loss: 2.3083, Val. Acc: 0.0938\n",
      "[0/251] Loss: 2.293545 (2.293545) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.231554 (2.278413) Accuracy: 0.187500 (0.159091)\n",
      "[20/251] Loss: 2.312124 (2.289883) Accuracy: 0.093750 (0.135417)\n",
      "[30/251] Loss: 2.296884 (2.290923) Accuracy: 0.125000 (0.133065)\n",
      "[40/251] Loss: 2.317853 (2.289292) Accuracy: 0.062500 (0.132622)\n",
      "[50/251] Loss: 2.278993 (2.287648) Accuracy: 0.125000 (0.140319)\n",
      "[60/251] Loss: 2.283210 (2.289361) Accuracy: 0.187500 (0.135758)\n",
      "[70/251] Loss: 2.300483 (2.289617) Accuracy: 0.125000 (0.137764)\n",
      "[80/251] Loss: 2.304577 (2.289272) Accuracy: 0.125000 (0.140046)\n",
      "[90/251] Loss: 2.291780 (2.289718) Accuracy: 0.093750 (0.139080)\n",
      "[100/251] Loss: 2.273465 (2.289074) Accuracy: 0.093750 (0.140780)\n",
      "[110/251] Loss: 2.302405 (2.289561) Accuracy: 0.093750 (0.139358)\n",
      "[120/251] Loss: 2.275454 (2.289363) Accuracy: 0.218750 (0.139205)\n",
      "[130/251] Loss: 2.275488 (2.288841) Accuracy: 0.156250 (0.140744)\n",
      "[140/251] Loss: 2.303533 (2.288615) Accuracy: 0.093750 (0.141401)\n",
      "[150/251] Loss: 2.306341 (2.288384) Accuracy: 0.062500 (0.141142)\n",
      "[160/251] Loss: 2.260866 (2.288853) Accuracy: 0.281250 (0.141110)\n",
      "[170/251] Loss: 2.304716 (2.289122) Accuracy: 0.125000 (0.141996)\n",
      "[180/251] Loss: 2.299140 (2.288810) Accuracy: 0.093750 (0.142956)\n",
      "[190/251] Loss: 2.321976 (2.288735) Accuracy: 0.062500 (0.142997)\n",
      "[200/251] Loss: 2.264273 (2.288657) Accuracy: 0.250000 (0.143501)\n",
      "[210/251] Loss: 2.266851 (2.288948) Accuracy: 0.187500 (0.143217)\n",
      "[220/251] Loss: 2.297387 (2.289714) Accuracy: 0.031250 (0.140837)\n",
      "[230/251] Loss: 2.293686 (2.289767) Accuracy: 0.187500 (0.141504)\n",
      "[240/251] Loss: 2.273930 (2.289131) Accuracy: 0.281250 (0.143932)\n",
      "[250/251] Loss: 2.329057 (2.289336) Accuracy: 0.000000 (0.144298)\n",
      "[0/63] Loss: 2.293736 (2.293736) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.297782 (2.310161) Accuracy: 0.062500 (0.079545)\n",
      "[20/63] Loss: 2.320110 (2.307949) Accuracy: 0.062500 (0.090774)\n",
      "[30/63] Loss: 2.305001 (2.307571) Accuracy: 0.093750 (0.093750)\n",
      "[40/63] Loss: 2.296872 (2.307385) Accuracy: 0.062500 (0.092226)\n",
      "[50/63] Loss: 2.304699 (2.308223) Accuracy: 0.125000 (0.091299)\n",
      "[60/63] Loss: 2.321519 (2.308803) Accuracy: 0.093750 (0.087602)\n",
      "Epoch: 14/100, Train Loss: 2.2893, Train Acc: 0.1443, Val. Loss: 2.3091, Val. Acc: 0.0884\n",
      "[0/251] Loss: 2.325024 (2.325024) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.250754 (2.296384) Accuracy: 0.250000 (0.130682)\n",
      "[20/251] Loss: 2.310234 (2.293864) Accuracy: 0.187500 (0.135417)\n",
      "[30/251] Loss: 2.276050 (2.286826) Accuracy: 0.250000 (0.152218)\n",
      "[40/251] Loss: 2.303735 (2.288539) Accuracy: 0.125000 (0.144055)\n",
      "[50/251] Loss: 2.274841 (2.287847) Accuracy: 0.187500 (0.149510)\n",
      "[60/251] Loss: 2.225852 (2.288383) Accuracy: 0.250000 (0.146516)\n",
      "[70/251] Loss: 2.283485 (2.287959) Accuracy: 0.156250 (0.147007)\n",
      "[80/251] Loss: 2.304818 (2.288461) Accuracy: 0.062500 (0.142747)\n",
      "[90/251] Loss: 2.303295 (2.289252) Accuracy: 0.125000 (0.140797)\n",
      "[100/251] Loss: 2.266680 (2.289040) Accuracy: 0.218750 (0.141399)\n",
      "[110/251] Loss: 2.284139 (2.289425) Accuracy: 0.156250 (0.141610)\n",
      "[120/251] Loss: 2.317242 (2.288938) Accuracy: 0.093750 (0.141529)\n",
      "[130/251] Loss: 2.297145 (2.288895) Accuracy: 0.125000 (0.141460)\n",
      "[140/251] Loss: 2.316854 (2.288322) Accuracy: 0.062500 (0.142287)\n",
      "[150/251] Loss: 2.268536 (2.288756) Accuracy: 0.156250 (0.140728)\n",
      "[160/251] Loss: 2.253907 (2.288371) Accuracy: 0.187500 (0.142857)\n",
      "[170/251] Loss: 2.284516 (2.287561) Accuracy: 0.125000 (0.144189)\n",
      "[180/251] Loss: 2.280013 (2.287369) Accuracy: 0.093750 (0.143992)\n",
      "[190/251] Loss: 2.313163 (2.287296) Accuracy: 0.000000 (0.143815)\n",
      "[200/251] Loss: 2.273659 (2.287546) Accuracy: 0.187500 (0.144123)\n",
      "[210/251] Loss: 2.279888 (2.287637) Accuracy: 0.156250 (0.144994)\n",
      "[220/251] Loss: 2.253663 (2.287409) Accuracy: 0.250000 (0.144938)\n",
      "[230/251] Loss: 2.300002 (2.287711) Accuracy: 0.062500 (0.144886)\n",
      "[240/251] Loss: 2.266998 (2.287721) Accuracy: 0.187500 (0.144191)\n",
      "[250/251] Loss: 2.320830 (2.288368) Accuracy: 0.000000 (0.142555)\n",
      "[0/63] Loss: 2.288719 (2.288719) Accuracy: 0.187500 (0.187500)\n",
      "[10/63] Loss: 2.300790 (2.310557) Accuracy: 0.062500 (0.090909)\n",
      "[20/63] Loss: 2.317692 (2.308274) Accuracy: 0.062500 (0.098214)\n",
      "[30/63] Loss: 2.309100 (2.307957) Accuracy: 0.062500 (0.094758)\n",
      "[40/63] Loss: 2.290384 (2.307621) Accuracy: 0.093750 (0.096037)\n",
      "[50/63] Loss: 2.303583 (2.308258) Accuracy: 0.187500 (0.094975)\n",
      "[60/63] Loss: 2.325424 (2.308696) Accuracy: 0.062500 (0.093238)\n",
      "Epoch: 15/100, Train Loss: 2.2884, Train Acc: 0.1426, Val. Loss: 2.3090, Val. Acc: 0.0928\n",
      "[0/251] Loss: 2.298688 (2.298688) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.292946 (2.293981) Accuracy: 0.125000 (0.144886)\n",
      "[20/251] Loss: 2.289398 (2.291556) Accuracy: 0.093750 (0.139881)\n",
      "[30/251] Loss: 2.287113 (2.287875) Accuracy: 0.187500 (0.151210)\n",
      "[40/251] Loss: 2.245982 (2.287894) Accuracy: 0.218750 (0.146341)\n",
      "[50/251] Loss: 2.318943 (2.289149) Accuracy: 0.062500 (0.141544)\n",
      "[60/251] Loss: 2.286116 (2.288048) Accuracy: 0.156250 (0.143955)\n",
      "[70/251] Loss: 2.273544 (2.289151) Accuracy: 0.187500 (0.141285)\n",
      "[80/251] Loss: 2.298162 (2.289566) Accuracy: 0.156250 (0.142361)\n",
      "[90/251] Loss: 2.294882 (2.289775) Accuracy: 0.125000 (0.139423)\n",
      "[100/251] Loss: 2.279436 (2.290758) Accuracy: 0.187500 (0.137376)\n",
      "[110/251] Loss: 2.295947 (2.290232) Accuracy: 0.250000 (0.138514)\n",
      "[120/251] Loss: 2.291518 (2.290839) Accuracy: 0.125000 (0.136880)\n",
      "[130/251] Loss: 2.274770 (2.290281) Accuracy: 0.187500 (0.138597)\n",
      "[140/251] Loss: 2.269682 (2.289948) Accuracy: 0.187500 (0.140293)\n",
      "[150/251] Loss: 2.283468 (2.289838) Accuracy: 0.125000 (0.139694)\n",
      "[160/251] Loss: 2.302994 (2.289419) Accuracy: 0.125000 (0.141110)\n",
      "[170/251] Loss: 2.270605 (2.289194) Accuracy: 0.218750 (0.140351)\n",
      "[180/251] Loss: 2.321244 (2.289133) Accuracy: 0.093750 (0.140884)\n",
      "[190/251] Loss: 2.293033 (2.288400) Accuracy: 0.156250 (0.143325)\n",
      "[200/251] Loss: 2.294292 (2.288524) Accuracy: 0.093750 (0.143035)\n",
      "[210/251] Loss: 2.296666 (2.288170) Accuracy: 0.125000 (0.143809)\n",
      "[220/251] Loss: 2.291087 (2.288029) Accuracy: 0.093750 (0.144231)\n",
      "[230/251] Loss: 2.316329 (2.287869) Accuracy: 0.093750 (0.144751)\n",
      "[240/251] Loss: 2.258975 (2.287765) Accuracy: 0.281250 (0.145488)\n",
      "[250/251] Loss: 2.311843 (2.287617) Accuracy: 0.000000 (0.146414)\n",
      "[0/63] Loss: 2.293713 (2.293713) Accuracy: 0.187500 (0.187500)\n",
      "[10/63] Loss: 2.297585 (2.310703) Accuracy: 0.093750 (0.096591)\n",
      "[20/63] Loss: 2.320262 (2.308277) Accuracy: 0.062500 (0.098214)\n",
      "[30/63] Loss: 2.310860 (2.308260) Accuracy: 0.062500 (0.099798)\n",
      "[40/63] Loss: 2.296336 (2.308245) Accuracy: 0.093750 (0.099848)\n",
      "[50/63] Loss: 2.304322 (2.308571) Accuracy: 0.218750 (0.099877)\n",
      "[60/63] Loss: 2.328768 (2.309155) Accuracy: 0.062500 (0.096311)\n",
      "Epoch: 16/100, Train Loss: 2.2876, Train Acc: 0.1464, Val. Loss: 2.3094, Val. Acc: 0.0958\n",
      "[0/251] Loss: 2.314871 (2.314871) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.289840 (2.297176) Accuracy: 0.125000 (0.122159)\n",
      "[20/251] Loss: 2.311856 (2.296654) Accuracy: 0.125000 (0.127976)\n",
      "[30/251] Loss: 2.296428 (2.289498) Accuracy: 0.156250 (0.144153)\n",
      "[40/251] Loss: 2.294534 (2.290617) Accuracy: 0.125000 (0.138720)\n",
      "[50/251] Loss: 2.289121 (2.288773) Accuracy: 0.156250 (0.143382)\n",
      "[60/251] Loss: 2.352241 (2.289186) Accuracy: 0.031250 (0.144980)\n",
      "[70/251] Loss: 2.230992 (2.288112) Accuracy: 0.281250 (0.148327)\n",
      "[80/251] Loss: 2.314954 (2.288400) Accuracy: 0.000000 (0.146991)\n",
      "[90/251] Loss: 2.331677 (2.288244) Accuracy: 0.062500 (0.149038)\n",
      "[100/251] Loss: 2.271844 (2.287838) Accuracy: 0.218750 (0.149443)\n",
      "[110/251] Loss: 2.287176 (2.288070) Accuracy: 0.156250 (0.147804)\n",
      "[120/251] Loss: 2.294926 (2.288153) Accuracy: 0.125000 (0.147469)\n",
      "[130/251] Loss: 2.294842 (2.287921) Accuracy: 0.156250 (0.149094)\n",
      "[140/251] Loss: 2.289978 (2.287909) Accuracy: 0.125000 (0.147606)\n",
      "[150/251] Loss: 2.286644 (2.288253) Accuracy: 0.125000 (0.147351)\n",
      "[160/251] Loss: 2.264148 (2.288017) Accuracy: 0.156250 (0.146933)\n",
      "[170/251] Loss: 2.225588 (2.287444) Accuracy: 0.343750 (0.147844)\n",
      "[180/251] Loss: 2.271290 (2.287784) Accuracy: 0.156250 (0.146236)\n",
      "[190/251] Loss: 2.254512 (2.286909) Accuracy: 0.187500 (0.147906)\n",
      "[200/251] Loss: 2.273772 (2.287311) Accuracy: 0.125000 (0.147233)\n",
      "[210/251] Loss: 2.253267 (2.286915) Accuracy: 0.312500 (0.147956)\n",
      "[220/251] Loss: 2.282314 (2.287040) Accuracy: 0.125000 (0.148049)\n",
      "[230/251] Loss: 2.287874 (2.286779) Accuracy: 0.156250 (0.148674)\n",
      "[240/251] Loss: 2.289878 (2.286560) Accuracy: 0.156250 (0.149378)\n",
      "[250/251] Loss: 2.400745 (2.287257) Accuracy: 0.000000 (0.148655)\n",
      "[0/63] Loss: 2.295021 (2.295021) Accuracy: 0.187500 (0.187500)\n",
      "[10/63] Loss: 2.296808 (2.310991) Accuracy: 0.093750 (0.099432)\n",
      "[20/63] Loss: 2.321716 (2.309140) Accuracy: 0.062500 (0.098214)\n",
      "[30/63] Loss: 2.307215 (2.308653) Accuracy: 0.062500 (0.098790)\n",
      "[40/63] Loss: 2.299988 (2.308383) Accuracy: 0.093750 (0.098323)\n",
      "[50/63] Loss: 2.304604 (2.308897) Accuracy: 0.218750 (0.098652)\n",
      "[60/63] Loss: 2.327382 (2.309425) Accuracy: 0.062500 (0.094262)\n",
      "Epoch: 17/100, Train Loss: 2.2873, Train Acc: 0.1487, Val. Loss: 2.3097, Val. Acc: 0.0949\n",
      "[0/251] Loss: 2.269000 (2.269000) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.289256 (2.282630) Accuracy: 0.125000 (0.164773)\n",
      "[20/251] Loss: 2.322824 (2.283669) Accuracy: 0.031250 (0.157738)\n",
      "[30/251] Loss: 2.305029 (2.283974) Accuracy: 0.093750 (0.157258)\n",
      "[40/251] Loss: 2.279287 (2.284619) Accuracy: 0.187500 (0.154726)\n",
      "[50/251] Loss: 2.228423 (2.282960) Accuracy: 0.218750 (0.154412)\n",
      "[60/251] Loss: 2.219518 (2.281763) Accuracy: 0.250000 (0.158299)\n",
      "[70/251] Loss: 2.312288 (2.281063) Accuracy: 0.125000 (0.159771)\n",
      "[80/251] Loss: 2.262973 (2.282364) Accuracy: 0.187500 (0.155478)\n",
      "[90/251] Loss: 2.284665 (2.283172) Accuracy: 0.187500 (0.153846)\n",
      "[100/251] Loss: 2.267138 (2.283681) Accuracy: 0.250000 (0.154703)\n",
      "[110/251] Loss: 2.243716 (2.283620) Accuracy: 0.218750 (0.153998)\n",
      "[120/251] Loss: 2.286593 (2.283504) Accuracy: 0.156250 (0.155217)\n",
      "[130/251] Loss: 2.311460 (2.284138) Accuracy: 0.125000 (0.153865)\n",
      "[140/251] Loss: 2.275120 (2.283973) Accuracy: 0.125000 (0.153590)\n",
      "[150/251] Loss: 2.303462 (2.284154) Accuracy: 0.156250 (0.152525)\n",
      "[160/251] Loss: 2.291558 (2.284432) Accuracy: 0.093750 (0.152174)\n",
      "[170/251] Loss: 2.317966 (2.285484) Accuracy: 0.062500 (0.149671)\n",
      "[180/251] Loss: 2.293246 (2.285149) Accuracy: 0.156250 (0.150035)\n",
      "[190/251] Loss: 2.291952 (2.285416) Accuracy: 0.156250 (0.149869)\n",
      "[200/251] Loss: 2.269314 (2.285221) Accuracy: 0.187500 (0.150187)\n",
      "[210/251] Loss: 2.308744 (2.285299) Accuracy: 0.093750 (0.149882)\n",
      "[220/251] Loss: 2.291606 (2.285797) Accuracy: 0.156250 (0.149604)\n",
      "[230/251] Loss: 2.306959 (2.285544) Accuracy: 0.125000 (0.150027)\n",
      "[240/251] Loss: 2.288905 (2.285767) Accuracy: 0.218750 (0.150285)\n",
      "[250/251] Loss: 2.304882 (2.286208) Accuracy: 0.000000 (0.149402)\n",
      "[0/63] Loss: 2.293113 (2.293113) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.299740 (2.311127) Accuracy: 0.062500 (0.090909)\n",
      "[20/63] Loss: 2.320815 (2.309471) Accuracy: 0.062500 (0.096726)\n",
      "[30/63] Loss: 2.306311 (2.308573) Accuracy: 0.093750 (0.098790)\n",
      "[40/63] Loss: 2.294757 (2.308130) Accuracy: 0.093750 (0.099085)\n",
      "[50/63] Loss: 2.305024 (2.308898) Accuracy: 0.218750 (0.101103)\n",
      "[60/63] Loss: 2.322712 (2.309318) Accuracy: 0.062500 (0.097336)\n",
      "Epoch: 18/100, Train Loss: 2.2862, Train Acc: 0.1494, Val. Loss: 2.3095, Val. Acc: 0.0968\n",
      "[0/251] Loss: 2.290946 (2.290946) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.298261 (2.280411) Accuracy: 0.125000 (0.159091)\n",
      "[20/251] Loss: 2.280621 (2.286782) Accuracy: 0.125000 (0.145833)\n",
      "[30/251] Loss: 2.304748 (2.286098) Accuracy: 0.125000 (0.147177)\n",
      "[40/251] Loss: 2.325727 (2.288194) Accuracy: 0.093750 (0.141768)\n",
      "[50/251] Loss: 2.241825 (2.285174) Accuracy: 0.187500 (0.142157)\n",
      "[60/251] Loss: 2.256915 (2.285590) Accuracy: 0.218750 (0.144980)\n",
      "[70/251] Loss: 2.248924 (2.284902) Accuracy: 0.281250 (0.148768)\n",
      "[80/251] Loss: 2.309973 (2.286308) Accuracy: 0.156250 (0.145062)\n",
      "[90/251] Loss: 2.258307 (2.286433) Accuracy: 0.250000 (0.145948)\n",
      "[100/251] Loss: 2.294655 (2.285739) Accuracy: 0.125000 (0.144802)\n",
      "[110/251] Loss: 2.301232 (2.284625) Accuracy: 0.062500 (0.146396)\n",
      "[120/251] Loss: 2.293880 (2.284777) Accuracy: 0.125000 (0.145145)\n",
      "[130/251] Loss: 2.309610 (2.285352) Accuracy: 0.156250 (0.145754)\n",
      "[140/251] Loss: 2.257835 (2.285548) Accuracy: 0.218750 (0.147606)\n",
      "[150/251] Loss: 2.281264 (2.284652) Accuracy: 0.250000 (0.149214)\n",
      "[160/251] Loss: 2.240033 (2.285682) Accuracy: 0.281250 (0.147127)\n",
      "[170/251] Loss: 2.305593 (2.285207) Accuracy: 0.125000 (0.149306)\n",
      "[180/251] Loss: 2.301124 (2.285520) Accuracy: 0.125000 (0.149517)\n",
      "[190/251] Loss: 2.297789 (2.285559) Accuracy: 0.156250 (0.150196)\n",
      "[200/251] Loss: 2.323678 (2.285303) Accuracy: 0.062500 (0.151119)\n",
      "[210/251] Loss: 2.314372 (2.285989) Accuracy: 0.125000 (0.149289)\n",
      "[220/251] Loss: 2.307408 (2.285572) Accuracy: 0.093750 (0.149887)\n",
      "[230/251] Loss: 2.277793 (2.285430) Accuracy: 0.125000 (0.150298)\n",
      "[240/251] Loss: 2.265008 (2.285473) Accuracy: 0.187500 (0.150934)\n",
      "[250/251] Loss: 1.761562 (2.283480) Accuracy: 1.000000 (0.153884)\n",
      "[0/63] Loss: 2.291245 (2.291245) Accuracy: 0.187500 (0.187500)\n",
      "[10/63] Loss: 2.300594 (2.311365) Accuracy: 0.093750 (0.093750)\n",
      "[20/63] Loss: 2.322167 (2.309297) Accuracy: 0.062500 (0.099702)\n",
      "[30/63] Loss: 2.308668 (2.308473) Accuracy: 0.062500 (0.098790)\n",
      "[40/63] Loss: 2.287894 (2.308012) Accuracy: 0.093750 (0.098323)\n",
      "[50/63] Loss: 2.303478 (2.308806) Accuracy: 0.187500 (0.096814)\n",
      "[60/63] Loss: 2.323347 (2.309372) Accuracy: 0.062500 (0.093238)\n",
      "Epoch: 19/100, Train Loss: 2.2835, Train Acc: 0.1539, Val. Loss: 2.3096, Val. Acc: 0.0928\n",
      "[0/251] Loss: 2.281765 (2.281765) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.302286 (2.276369) Accuracy: 0.125000 (0.176136)\n",
      "[20/251] Loss: 2.295455 (2.282412) Accuracy: 0.125000 (0.154762)\n",
      "[30/251] Loss: 2.269118 (2.286177) Accuracy: 0.187500 (0.147177)\n",
      "[40/251] Loss: 2.293674 (2.284720) Accuracy: 0.125000 (0.148628)\n",
      "[50/251] Loss: 2.308517 (2.285348) Accuracy: 0.062500 (0.143995)\n",
      "[60/251] Loss: 2.266742 (2.286002) Accuracy: 0.156250 (0.142930)\n",
      "[70/251] Loss: 2.265133 (2.288127) Accuracy: 0.281250 (0.140405)\n",
      "[80/251] Loss: 2.299203 (2.287906) Accuracy: 0.218750 (0.141975)\n",
      "[90/251] Loss: 2.303443 (2.288792) Accuracy: 0.156250 (0.140797)\n",
      "[100/251] Loss: 2.245371 (2.289053) Accuracy: 0.218750 (0.138614)\n",
      "[110/251] Loss: 2.297945 (2.289366) Accuracy: 0.156250 (0.140766)\n",
      "[120/251] Loss: 2.270050 (2.289647) Accuracy: 0.250000 (0.139205)\n",
      "[130/251] Loss: 2.257874 (2.289142) Accuracy: 0.187500 (0.141460)\n",
      "[140/251] Loss: 2.270588 (2.287213) Accuracy: 0.218750 (0.144504)\n",
      "[150/251] Loss: 2.310157 (2.287612) Accuracy: 0.062500 (0.143626)\n",
      "[160/251] Loss: 2.290190 (2.286912) Accuracy: 0.187500 (0.145769)\n",
      "[170/251] Loss: 2.296694 (2.286734) Accuracy: 0.156250 (0.146382)\n",
      "[180/251] Loss: 2.287753 (2.286061) Accuracy: 0.156250 (0.149344)\n",
      "[190/251] Loss: 2.271410 (2.285708) Accuracy: 0.187500 (0.150033)\n",
      "[200/251] Loss: 2.264860 (2.285952) Accuracy: 0.218750 (0.150498)\n",
      "[210/251] Loss: 2.257470 (2.286296) Accuracy: 0.156250 (0.150030)\n",
      "[220/251] Loss: 2.293962 (2.285892) Accuracy: 0.156250 (0.151867)\n",
      "[230/251] Loss: 2.307484 (2.285602) Accuracy: 0.062500 (0.152192)\n",
      "[240/251] Loss: 2.310818 (2.285435) Accuracy: 0.062500 (0.151582)\n",
      "[250/251] Loss: 2.351848 (2.285187) Accuracy: 0.000000 (0.152017)\n",
      "[0/63] Loss: 2.292989 (2.292989) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.297931 (2.311269) Accuracy: 0.093750 (0.093750)\n",
      "[20/63] Loss: 2.321567 (2.309256) Accuracy: 0.062500 (0.096726)\n",
      "[30/63] Loss: 2.307102 (2.308558) Accuracy: 0.093750 (0.098790)\n",
      "[40/63] Loss: 2.292516 (2.308228) Accuracy: 0.093750 (0.100610)\n",
      "[50/63] Loss: 2.305508 (2.309003) Accuracy: 0.218750 (0.100490)\n",
      "[60/63] Loss: 2.325775 (2.309525) Accuracy: 0.062500 (0.097336)\n",
      "Epoch: 20/100, Train Loss: 2.2852, Train Acc: 0.1520, Val. Loss: 2.3098, Val. Acc: 0.0979\n",
      "[0/251] Loss: 2.219144 (2.219144) Accuracy: 0.343750 (0.343750)\n",
      "[10/251] Loss: 2.300933 (2.277995) Accuracy: 0.125000 (0.161932)\n",
      "[20/251] Loss: 2.288689 (2.283153) Accuracy: 0.125000 (0.151786)\n",
      "[30/251] Loss: 2.346039 (2.284483) Accuracy: 0.062500 (0.153226)\n",
      "[40/251] Loss: 2.276505 (2.286488) Accuracy: 0.250000 (0.149390)\n",
      "[50/251] Loss: 2.296639 (2.285541) Accuracy: 0.062500 (0.143995)\n",
      "[60/251] Loss: 2.303320 (2.284815) Accuracy: 0.125000 (0.147029)\n",
      "[70/251] Loss: 2.298658 (2.283789) Accuracy: 0.125000 (0.149648)\n",
      "[80/251] Loss: 2.262251 (2.284511) Accuracy: 0.156250 (0.147377)\n",
      "[90/251] Loss: 2.307178 (2.283352) Accuracy: 0.093750 (0.151099)\n",
      "[100/251] Loss: 2.302361 (2.283792) Accuracy: 0.156250 (0.150681)\n",
      "[110/251] Loss: 2.280003 (2.283141) Accuracy: 0.156250 (0.153153)\n",
      "[120/251] Loss: 2.270300 (2.283113) Accuracy: 0.156250 (0.152893)\n",
      "[130/251] Loss: 2.284518 (2.283594) Accuracy: 0.125000 (0.151956)\n",
      "[140/251] Loss: 2.290997 (2.283684) Accuracy: 0.125000 (0.151817)\n",
      "[150/251] Loss: 2.295379 (2.284613) Accuracy: 0.156250 (0.150662)\n",
      "[160/251] Loss: 2.280663 (2.284148) Accuracy: 0.218750 (0.151980)\n",
      "[170/251] Loss: 2.326459 (2.284806) Accuracy: 0.093750 (0.151133)\n",
      "[180/251] Loss: 2.232460 (2.284440) Accuracy: 0.281250 (0.152452)\n",
      "[190/251] Loss: 2.266802 (2.284991) Accuracy: 0.281250 (0.151505)\n",
      "[200/251] Loss: 2.291645 (2.284167) Accuracy: 0.125000 (0.152985)\n",
      "[210/251] Loss: 2.233706 (2.284230) Accuracy: 0.281250 (0.152547)\n",
      "[220/251] Loss: 2.284202 (2.284509) Accuracy: 0.156250 (0.151584)\n",
      "[230/251] Loss: 2.271833 (2.284783) Accuracy: 0.218750 (0.150433)\n",
      "[240/251] Loss: 2.274519 (2.284911) Accuracy: 0.093750 (0.151452)\n",
      "[250/251] Loss: 2.344445 (2.284650) Accuracy: 0.000000 (0.152266)\n",
      "[0/63] Loss: 2.293915 (2.293915) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.296340 (2.311064) Accuracy: 0.093750 (0.096591)\n",
      "[20/63] Loss: 2.324852 (2.309236) Accuracy: 0.062500 (0.096726)\n",
      "[30/63] Loss: 2.308811 (2.308495) Accuracy: 0.093750 (0.097782)\n",
      "[40/63] Loss: 2.296462 (2.308154) Accuracy: 0.093750 (0.099085)\n",
      "[50/63] Loss: 2.305574 (2.308892) Accuracy: 0.187500 (0.097426)\n",
      "[60/63] Loss: 2.329712 (2.309481) Accuracy: 0.062500 (0.094262)\n",
      "Epoch: 21/100, Train Loss: 2.2846, Train Acc: 0.1523, Val. Loss: 2.3098, Val. Acc: 0.0949\n",
      "[0/251] Loss: 2.315117 (2.315117) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.297801 (2.300261) Accuracy: 0.187500 (0.116477)\n",
      "[20/251] Loss: 2.305774 (2.294036) Accuracy: 0.125000 (0.147321)\n",
      "[30/251] Loss: 2.281530 (2.289767) Accuracy: 0.156250 (0.147177)\n",
      "[40/251] Loss: 2.279542 (2.285555) Accuracy: 0.125000 (0.153963)\n",
      "[50/251] Loss: 2.297038 (2.285418) Accuracy: 0.156250 (0.155025)\n",
      "[60/251] Loss: 2.276881 (2.286629) Accuracy: 0.187500 (0.154201)\n",
      "[70/251] Loss: 2.307142 (2.285533) Accuracy: 0.093750 (0.154489)\n",
      "[80/251] Loss: 2.284582 (2.285597) Accuracy: 0.187500 (0.153164)\n",
      "[90/251] Loss: 2.307573 (2.285212) Accuracy: 0.093750 (0.154190)\n",
      "[100/251] Loss: 2.209222 (2.283915) Accuracy: 0.312500 (0.158416)\n",
      "[110/251] Loss: 2.254883 (2.283275) Accuracy: 0.250000 (0.158784)\n",
      "[120/251] Loss: 2.247032 (2.283898) Accuracy: 0.218750 (0.156250)\n",
      "[130/251] Loss: 2.277292 (2.283863) Accuracy: 0.156250 (0.156966)\n",
      "[140/251] Loss: 2.277295 (2.283077) Accuracy: 0.156250 (0.157137)\n",
      "[150/251] Loss: 2.234921 (2.283204) Accuracy: 0.187500 (0.155422)\n",
      "[160/251] Loss: 2.314735 (2.283090) Accuracy: 0.093750 (0.156250)\n",
      "[170/251] Loss: 2.294226 (2.282623) Accuracy: 0.187500 (0.156981)\n",
      "[180/251] Loss: 2.274086 (2.282857) Accuracy: 0.187500 (0.157113)\n",
      "[190/251] Loss: 2.300701 (2.283131) Accuracy: 0.093750 (0.156741)\n",
      "[200/251] Loss: 2.246439 (2.282717) Accuracy: 0.187500 (0.157338)\n",
      "[210/251] Loss: 2.297389 (2.282953) Accuracy: 0.125000 (0.157731)\n",
      "[220/251] Loss: 2.305558 (2.283342) Accuracy: 0.093750 (0.155967)\n",
      "[230/251] Loss: 2.318125 (2.283884) Accuracy: 0.062500 (0.155168)\n",
      "[240/251] Loss: 2.291583 (2.283674) Accuracy: 0.187500 (0.155991)\n",
      "[250/251] Loss: 2.337280 (2.284228) Accuracy: 0.000000 (0.154382)\n",
      "[0/63] Loss: 2.293845 (2.293845) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.296867 (2.311084) Accuracy: 0.125000 (0.102273)\n",
      "[20/63] Loss: 2.324872 (2.309449) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.308245 (2.308473) Accuracy: 0.093750 (0.102823)\n",
      "[40/63] Loss: 2.296755 (2.308154) Accuracy: 0.093750 (0.102896)\n",
      "[50/63] Loss: 2.306086 (2.308963) Accuracy: 0.187500 (0.101716)\n",
      "[60/63] Loss: 2.328129 (2.309488) Accuracy: 0.062500 (0.098873)\n",
      "Epoch: 22/100, Train Loss: 2.2842, Train Acc: 0.1544, Val. Loss: 2.3098, Val. Acc: 0.0978\n",
      "[0/251] Loss: 2.299435 (2.299435) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.288313 (2.286445) Accuracy: 0.156250 (0.156250)\n",
      "[20/251] Loss: 2.316209 (2.291563) Accuracy: 0.093750 (0.150298)\n",
      "[30/251] Loss: 2.251112 (2.291816) Accuracy: 0.250000 (0.146169)\n",
      "[40/251] Loss: 2.303975 (2.287983) Accuracy: 0.093750 (0.147104)\n",
      "[50/251] Loss: 2.223227 (2.285381) Accuracy: 0.250000 (0.153799)\n",
      "[60/251] Loss: 2.251532 (2.284197) Accuracy: 0.156250 (0.154201)\n",
      "[70/251] Loss: 2.246909 (2.285040) Accuracy: 0.218750 (0.153169)\n",
      "[80/251] Loss: 2.285179 (2.285959) Accuracy: 0.187500 (0.151620)\n",
      "[90/251] Loss: 2.292787 (2.286109) Accuracy: 0.125000 (0.150069)\n",
      "[100/251] Loss: 2.249972 (2.284523) Accuracy: 0.250000 (0.154394)\n",
      "[110/251] Loss: 2.261444 (2.284974) Accuracy: 0.250000 (0.153998)\n",
      "[120/251] Loss: 2.283761 (2.284591) Accuracy: 0.156250 (0.155217)\n",
      "[130/251] Loss: 2.289006 (2.284959) Accuracy: 0.187500 (0.154819)\n",
      "[140/251] Loss: 2.311625 (2.285392) Accuracy: 0.093750 (0.153812)\n",
      "[150/251] Loss: 2.243768 (2.284105) Accuracy: 0.250000 (0.156250)\n",
      "[160/251] Loss: 2.282605 (2.284400) Accuracy: 0.187500 (0.155862)\n",
      "[170/251] Loss: 2.247951 (2.283687) Accuracy: 0.187500 (0.157712)\n",
      "[180/251] Loss: 2.276523 (2.283856) Accuracy: 0.156250 (0.155905)\n",
      "[190/251] Loss: 2.263854 (2.283385) Accuracy: 0.218750 (0.157232)\n",
      "[200/251] Loss: 2.295595 (2.283494) Accuracy: 0.125000 (0.156250)\n",
      "[210/251] Loss: 2.257799 (2.283439) Accuracy: 0.156250 (0.156102)\n",
      "[220/251] Loss: 2.259645 (2.283720) Accuracy: 0.187500 (0.155684)\n",
      "[230/251] Loss: 2.315526 (2.283533) Accuracy: 0.125000 (0.156115)\n",
      "[240/251] Loss: 2.301337 (2.283767) Accuracy: 0.125000 (0.155602)\n",
      "[250/251] Loss: 2.353784 (2.283872) Accuracy: 0.000000 (0.155627)\n",
      "[0/63] Loss: 2.292785 (2.292785) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.295707 (2.310888) Accuracy: 0.125000 (0.096591)\n",
      "[20/63] Loss: 2.324344 (2.309293) Accuracy: 0.062500 (0.099702)\n",
      "[30/63] Loss: 2.309377 (2.308487) Accuracy: 0.062500 (0.100806)\n",
      "[40/63] Loss: 2.294533 (2.308111) Accuracy: 0.093750 (0.101372)\n",
      "[50/63] Loss: 2.306496 (2.309062) Accuracy: 0.187500 (0.099265)\n",
      "[60/63] Loss: 2.327424 (2.309553) Accuracy: 0.062500 (0.096824)\n",
      "Epoch: 23/100, Train Loss: 2.2839, Train Acc: 0.1556, Val. Loss: 2.3098, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.276255 (2.276255) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.310286 (2.286034) Accuracy: 0.062500 (0.142045)\n",
      "[20/251] Loss: 2.284467 (2.283080) Accuracy: 0.218750 (0.153274)\n",
      "[30/251] Loss: 2.217641 (2.277812) Accuracy: 0.312500 (0.165323)\n",
      "[40/251] Loss: 2.281480 (2.278579) Accuracy: 0.156250 (0.163110)\n",
      "[50/251] Loss: 2.255686 (2.280364) Accuracy: 0.187500 (0.156863)\n",
      "[60/251] Loss: 2.213394 (2.279379) Accuracy: 0.281250 (0.158299)\n",
      "[70/251] Loss: 2.304806 (2.279977) Accuracy: 0.093750 (0.158891)\n",
      "[80/251] Loss: 2.324838 (2.282470) Accuracy: 0.062500 (0.155864)\n",
      "[90/251] Loss: 2.307210 (2.280951) Accuracy: 0.125000 (0.158310)\n",
      "[100/251] Loss: 2.278332 (2.281920) Accuracy: 0.187500 (0.157797)\n",
      "[110/251] Loss: 2.298100 (2.281981) Accuracy: 0.156250 (0.156813)\n",
      "[120/251] Loss: 2.289330 (2.282465) Accuracy: 0.093750 (0.155217)\n",
      "[130/251] Loss: 2.271204 (2.281966) Accuracy: 0.187500 (0.155534)\n",
      "[140/251] Loss: 2.310810 (2.283155) Accuracy: 0.093750 (0.152261)\n",
      "[150/251] Loss: 2.261118 (2.283269) Accuracy: 0.250000 (0.153974)\n",
      "[160/251] Loss: 2.243391 (2.283104) Accuracy: 0.281250 (0.154891)\n",
      "[170/251] Loss: 2.250626 (2.283413) Accuracy: 0.218750 (0.154240)\n",
      "[180/251] Loss: 2.298717 (2.282317) Accuracy: 0.062500 (0.155732)\n",
      "[190/251] Loss: 2.311094 (2.282997) Accuracy: 0.093750 (0.154941)\n",
      "[200/251] Loss: 2.301663 (2.283288) Accuracy: 0.187500 (0.154695)\n",
      "[210/251] Loss: 2.251907 (2.283139) Accuracy: 0.250000 (0.155213)\n",
      "[220/251] Loss: 2.286808 (2.283326) Accuracy: 0.156250 (0.154553)\n",
      "[230/251] Loss: 2.222374 (2.282705) Accuracy: 0.343750 (0.156656)\n",
      "[240/251] Loss: 2.280601 (2.283059) Accuracy: 0.125000 (0.154953)\n",
      "[250/251] Loss: 2.393483 (2.283588) Accuracy: 0.000000 (0.153884)\n",
      "[0/63] Loss: 2.293220 (2.293220) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.296105 (2.311080) Accuracy: 0.125000 (0.096591)\n",
      "[20/63] Loss: 2.324951 (2.309693) Accuracy: 0.062500 (0.098214)\n",
      "[30/63] Loss: 2.306193 (2.308744) Accuracy: 0.093750 (0.099798)\n",
      "[40/63] Loss: 2.297116 (2.308300) Accuracy: 0.093750 (0.100610)\n",
      "[50/63] Loss: 2.306602 (2.309215) Accuracy: 0.156250 (0.098039)\n",
      "[60/63] Loss: 2.326825 (2.309707) Accuracy: 0.062500 (0.096311)\n",
      "Epoch: 24/100, Train Loss: 2.2836, Train Acc: 0.1539, Val. Loss: 2.3100, Val. Acc: 0.0969\n",
      "[0/251] Loss: 2.277653 (2.277653) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.313219 (2.290791) Accuracy: 0.125000 (0.142045)\n",
      "[20/251] Loss: 2.309973 (2.281718) Accuracy: 0.093750 (0.154762)\n",
      "[30/251] Loss: 2.258930 (2.280965) Accuracy: 0.187500 (0.156250)\n",
      "[40/251] Loss: 2.292248 (2.280522) Accuracy: 0.125000 (0.156250)\n",
      "[50/251] Loss: 2.307533 (2.281968) Accuracy: 0.125000 (0.154412)\n",
      "[60/251] Loss: 2.273259 (2.283855) Accuracy: 0.187500 (0.151639)\n",
      "[70/251] Loss: 2.305800 (2.284044) Accuracy: 0.093750 (0.152289)\n",
      "[80/251] Loss: 2.303199 (2.285381) Accuracy: 0.062500 (0.149306)\n",
      "[90/251] Loss: 2.254404 (2.284015) Accuracy: 0.187500 (0.152816)\n",
      "[100/251] Loss: 2.214519 (2.282613) Accuracy: 0.312500 (0.155631)\n",
      "[110/251] Loss: 2.294807 (2.281859) Accuracy: 0.062500 (0.155687)\n",
      "[120/251] Loss: 2.252522 (2.282069) Accuracy: 0.156250 (0.155733)\n",
      "[130/251] Loss: 2.271724 (2.282024) Accuracy: 0.125000 (0.156011)\n",
      "[140/251] Loss: 2.293019 (2.281770) Accuracy: 0.093750 (0.156028)\n",
      "[150/251] Loss: 2.301404 (2.282264) Accuracy: 0.156250 (0.156043)\n",
      "[160/251] Loss: 2.297497 (2.282647) Accuracy: 0.156250 (0.155085)\n",
      "[170/251] Loss: 2.222282 (2.282236) Accuracy: 0.312500 (0.157164)\n",
      "[180/251] Loss: 2.249972 (2.282449) Accuracy: 0.218750 (0.157804)\n",
      "[190/251] Loss: 2.295910 (2.282727) Accuracy: 0.125000 (0.156250)\n",
      "[200/251] Loss: 2.268008 (2.283076) Accuracy: 0.187500 (0.155784)\n",
      "[210/251] Loss: 2.277265 (2.283121) Accuracy: 0.218750 (0.155806)\n",
      "[220/251] Loss: 2.245611 (2.283365) Accuracy: 0.187500 (0.155402)\n",
      "[230/251] Loss: 2.280639 (2.282856) Accuracy: 0.156250 (0.156115)\n",
      "[240/251] Loss: 2.270272 (2.282792) Accuracy: 0.218750 (0.157417)\n",
      "[250/251] Loss: 2.389521 (2.283219) Accuracy: 0.000000 (0.157246)\n",
      "[0/63] Loss: 2.292416 (2.292416) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.296920 (2.311218) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.324061 (2.309759) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307104 (2.308872) Accuracy: 0.093750 (0.101815)\n",
      "[40/63] Loss: 2.296250 (2.308430) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.306277 (2.309319) Accuracy: 0.156250 (0.099265)\n",
      "[60/63] Loss: 2.326668 (2.309791) Accuracy: 0.062500 (0.096824)\n",
      "Epoch: 25/100, Train Loss: 2.2832, Train Acc: 0.1572, Val. Loss: 2.3101, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.288973 (2.288973) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.300588 (2.295922) Accuracy: 0.125000 (0.133523)\n",
      "[20/251] Loss: 2.273332 (2.291348) Accuracy: 0.156250 (0.142857)\n",
      "[30/251] Loss: 2.304986 (2.290318) Accuracy: 0.093750 (0.144153)\n",
      "[40/251] Loss: 2.271226 (2.286393) Accuracy: 0.218750 (0.153963)\n",
      "[50/251] Loss: 2.299254 (2.286273) Accuracy: 0.187500 (0.155637)\n",
      "[60/251] Loss: 2.273581 (2.285509) Accuracy: 0.125000 (0.157275)\n",
      "[70/251] Loss: 2.290018 (2.284852) Accuracy: 0.125000 (0.158011)\n",
      "[80/251] Loss: 2.272659 (2.283823) Accuracy: 0.156250 (0.160494)\n",
      "[90/251] Loss: 2.300378 (2.284051) Accuracy: 0.125000 (0.160371)\n",
      "[100/251] Loss: 2.310218 (2.283412) Accuracy: 0.156250 (0.160891)\n",
      "[110/251] Loss: 2.301422 (2.282786) Accuracy: 0.093750 (0.160191)\n",
      "[120/251] Loss: 2.268750 (2.283095) Accuracy: 0.187500 (0.159866)\n",
      "[130/251] Loss: 2.296208 (2.282524) Accuracy: 0.156250 (0.161021)\n",
      "[140/251] Loss: 2.309721 (2.282557) Accuracy: 0.156250 (0.160904)\n",
      "[150/251] Loss: 2.300203 (2.282408) Accuracy: 0.125000 (0.161217)\n",
      "[160/251] Loss: 2.304760 (2.282598) Accuracy: 0.031250 (0.160520)\n",
      "[170/251] Loss: 2.301255 (2.283446) Accuracy: 0.093750 (0.158077)\n",
      "[180/251] Loss: 2.256216 (2.283926) Accuracy: 0.187500 (0.156768)\n",
      "[190/251] Loss: 2.279245 (2.282928) Accuracy: 0.187500 (0.157723)\n",
      "[200/251] Loss: 2.291570 (2.282992) Accuracy: 0.125000 (0.158116)\n",
      "[210/251] Loss: 2.278649 (2.282345) Accuracy: 0.156250 (0.158916)\n",
      "[220/251] Loss: 2.242882 (2.282069) Accuracy: 0.250000 (0.159219)\n",
      "[230/251] Loss: 2.336893 (2.282600) Accuracy: 0.062500 (0.158415)\n",
      "[240/251] Loss: 2.245685 (2.282383) Accuracy: 0.250000 (0.158584)\n",
      "[250/251] Loss: 1.893405 (2.280978) Accuracy: 1.000000 (0.161355)\n",
      "[0/63] Loss: 2.294030 (2.294030) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294732 (2.311326) Accuracy: 0.125000 (0.096591)\n",
      "[20/63] Loss: 2.325375 (2.309877) Accuracy: 0.062500 (0.098214)\n",
      "[30/63] Loss: 2.307430 (2.309032) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298903 (2.308716) Accuracy: 0.093750 (0.102896)\n",
      "[50/63] Loss: 2.308041 (2.309560) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.329122 (2.310040) Accuracy: 0.062500 (0.097336)\n",
      "Epoch: 26/100, Train Loss: 2.2810, Train Acc: 0.1614, Val. Loss: 2.3103, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.293692 (2.293692) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.299263 (2.284902) Accuracy: 0.156250 (0.156250)\n",
      "[20/251] Loss: 2.277572 (2.281756) Accuracy: 0.187500 (0.165179)\n",
      "[30/251] Loss: 2.304514 (2.283250) Accuracy: 0.156250 (0.159274)\n",
      "[40/251] Loss: 2.236103 (2.280521) Accuracy: 0.218750 (0.163872)\n",
      "[50/251] Loss: 2.261195 (2.277372) Accuracy: 0.187500 (0.170956)\n",
      "[60/251] Loss: 2.279060 (2.275151) Accuracy: 0.156250 (0.177254)\n",
      "[70/251] Loss: 2.252325 (2.277997) Accuracy: 0.218750 (0.170775)\n",
      "[80/251] Loss: 2.309797 (2.278425) Accuracy: 0.125000 (0.169367)\n",
      "[90/251] Loss: 2.315685 (2.278727) Accuracy: 0.093750 (0.170330)\n",
      "[100/251] Loss: 2.282713 (2.278850) Accuracy: 0.156250 (0.168626)\n",
      "[110/251] Loss: 2.319324 (2.280575) Accuracy: 0.062500 (0.164414)\n",
      "[120/251] Loss: 2.290945 (2.281087) Accuracy: 0.093750 (0.163223)\n",
      "[130/251] Loss: 2.274177 (2.281057) Accuracy: 0.218750 (0.162929)\n",
      "[140/251] Loss: 2.277281 (2.281405) Accuracy: 0.218750 (0.162456)\n",
      "[150/251] Loss: 2.283210 (2.282137) Accuracy: 0.125000 (0.160803)\n",
      "[160/251] Loss: 2.256662 (2.281842) Accuracy: 0.281250 (0.162461)\n",
      "[170/251] Loss: 2.270895 (2.281951) Accuracy: 0.187500 (0.160819)\n",
      "[180/251] Loss: 2.256620 (2.281782) Accuracy: 0.187500 (0.159876)\n",
      "[190/251] Loss: 2.286786 (2.282428) Accuracy: 0.156250 (0.159359)\n",
      "[200/251] Loss: 2.277321 (2.283069) Accuracy: 0.156250 (0.157494)\n",
      "[210/251] Loss: 2.290879 (2.282940) Accuracy: 0.125000 (0.158323)\n",
      "[220/251] Loss: 2.316654 (2.282944) Accuracy: 0.125000 (0.157947)\n",
      "[230/251] Loss: 2.237709 (2.282750) Accuracy: 0.281250 (0.158956)\n",
      "[240/251] Loss: 2.286251 (2.282929) Accuracy: 0.125000 (0.158195)\n",
      "[250/251] Loss: 2.310886 (2.282233) Accuracy: 0.000000 (0.158367)\n",
      "[0/63] Loss: 2.293326 (2.293326) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294765 (2.311156) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.324006 (2.309693) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.307103 (2.308963) Accuracy: 0.093750 (0.102823)\n",
      "[40/63] Loss: 2.297739 (2.308555) Accuracy: 0.093750 (0.105183)\n",
      "[50/63] Loss: 2.308215 (2.309569) Accuracy: 0.125000 (0.102328)\n",
      "[60/63] Loss: 2.328765 (2.310012) Accuracy: 0.062500 (0.099385)\n",
      "Epoch: 27/100, Train Loss: 2.2822, Train Acc: 0.1584, Val. Loss: 2.3103, Val. Acc: 0.0993\n",
      "[0/251] Loss: 2.297600 (2.297600) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.259143 (2.282749) Accuracy: 0.281250 (0.164773)\n",
      "[20/251] Loss: 2.317335 (2.288196) Accuracy: 0.093750 (0.141369)\n",
      "[30/251] Loss: 2.242589 (2.283953) Accuracy: 0.250000 (0.149194)\n",
      "[40/251] Loss: 2.273241 (2.283338) Accuracy: 0.187500 (0.153963)\n",
      "[50/251] Loss: 2.320440 (2.283447) Accuracy: 0.031250 (0.153799)\n",
      "[60/251] Loss: 2.244771 (2.281118) Accuracy: 0.281250 (0.159836)\n",
      "[70/251] Loss: 2.268493 (2.278642) Accuracy: 0.187500 (0.165493)\n",
      "[80/251] Loss: 2.300015 (2.280141) Accuracy: 0.062500 (0.160108)\n",
      "[90/251] Loss: 2.272185 (2.278899) Accuracy: 0.218750 (0.163118)\n",
      "[100/251] Loss: 2.279145 (2.278916) Accuracy: 0.156250 (0.164295)\n",
      "[110/251] Loss: 2.306646 (2.279256) Accuracy: 0.093750 (0.162444)\n",
      "[120/251] Loss: 2.295481 (2.279545) Accuracy: 0.125000 (0.161157)\n",
      "[130/251] Loss: 2.280855 (2.280070) Accuracy: 0.187500 (0.159590)\n",
      "[140/251] Loss: 2.314489 (2.281368) Accuracy: 0.031250 (0.157580)\n",
      "[150/251] Loss: 2.277807 (2.281112) Accuracy: 0.125000 (0.158940)\n",
      "[160/251] Loss: 2.230498 (2.280493) Accuracy: 0.250000 (0.159744)\n",
      "[170/251] Loss: 2.223680 (2.279666) Accuracy: 0.250000 (0.162281)\n",
      "[180/251] Loss: 2.258264 (2.279836) Accuracy: 0.156250 (0.160566)\n",
      "[190/251] Loss: 2.298266 (2.280021) Accuracy: 0.093750 (0.160013)\n",
      "[200/251] Loss: 2.295465 (2.280299) Accuracy: 0.125000 (0.159670)\n",
      "[210/251] Loss: 2.270894 (2.280912) Accuracy: 0.250000 (0.158323)\n",
      "[220/251] Loss: 2.270911 (2.280999) Accuracy: 0.125000 (0.158088)\n",
      "[230/251] Loss: 2.256885 (2.280729) Accuracy: 0.218750 (0.158685)\n",
      "[240/251] Loss: 2.299185 (2.281634) Accuracy: 0.187500 (0.157936)\n",
      "[250/251] Loss: 2.302618 (2.281970) Accuracy: 0.000000 (0.156748)\n",
      "[0/63] Loss: 2.292867 (2.292867) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.295063 (2.311126) Accuracy: 0.125000 (0.102273)\n",
      "[20/63] Loss: 2.324391 (2.309695) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.307575 (2.308916) Accuracy: 0.062500 (0.102823)\n",
      "[40/63] Loss: 2.296563 (2.308464) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308002 (2.309511) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327949 (2.310006) Accuracy: 0.093750 (0.098873)\n",
      "Epoch: 28/100, Train Loss: 2.2820, Train Acc: 0.1567, Val. Loss: 2.3103, Val. Acc: 0.0993\n",
      "[0/251] Loss: 2.260013 (2.260013) Accuracy: 0.281250 (0.281250)\n",
      "[10/251] Loss: 2.326442 (2.286069) Accuracy: 0.062500 (0.150568)\n",
      "[20/251] Loss: 2.287567 (2.282781) Accuracy: 0.156250 (0.147321)\n",
      "[30/251] Loss: 2.322221 (2.282685) Accuracy: 0.093750 (0.154234)\n",
      "[40/251] Loss: 2.316138 (2.280455) Accuracy: 0.062500 (0.159299)\n",
      "[50/251] Loss: 2.307042 (2.282177) Accuracy: 0.093750 (0.156863)\n",
      "[60/251] Loss: 2.312120 (2.282716) Accuracy: 0.062500 (0.155738)\n",
      "[70/251] Loss: 2.279215 (2.280596) Accuracy: 0.156250 (0.158891)\n",
      "[80/251] Loss: 2.298265 (2.280425) Accuracy: 0.125000 (0.157793)\n",
      "[90/251] Loss: 2.296990 (2.281477) Accuracy: 0.156250 (0.155907)\n",
      "[100/251] Loss: 2.248142 (2.281108) Accuracy: 0.156250 (0.156250)\n",
      "[110/251] Loss: 2.259869 (2.280047) Accuracy: 0.281250 (0.157939)\n",
      "[120/251] Loss: 2.324935 (2.280792) Accuracy: 0.031250 (0.157025)\n",
      "[130/251] Loss: 2.272794 (2.281414) Accuracy: 0.187500 (0.155773)\n",
      "[140/251] Loss: 2.300655 (2.282097) Accuracy: 0.156250 (0.154255)\n",
      "[150/251] Loss: 2.220295 (2.280904) Accuracy: 0.250000 (0.156871)\n",
      "[160/251] Loss: 2.273338 (2.281009) Accuracy: 0.187500 (0.157997)\n",
      "[170/251] Loss: 2.308071 (2.281942) Accuracy: 0.093750 (0.156433)\n",
      "[180/251] Loss: 2.279757 (2.281622) Accuracy: 0.125000 (0.156768)\n",
      "[190/251] Loss: 2.298985 (2.281785) Accuracy: 0.156250 (0.156904)\n",
      "[200/251] Loss: 2.302807 (2.281075) Accuracy: 0.093750 (0.157649)\n",
      "[210/251] Loss: 2.254265 (2.281364) Accuracy: 0.187500 (0.157287)\n",
      "[220/251] Loss: 2.279518 (2.281246) Accuracy: 0.250000 (0.158371)\n",
      "[230/251] Loss: 2.296239 (2.281153) Accuracy: 0.093750 (0.158550)\n",
      "[240/251] Loss: 2.296346 (2.281345) Accuracy: 0.156250 (0.158584)\n",
      "[250/251] Loss: 2.126074 (2.281066) Accuracy: 1.000000 (0.161355)\n",
      "[0/63] Loss: 2.292866 (2.292866) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294808 (2.311191) Accuracy: 0.125000 (0.102273)\n",
      "[20/63] Loss: 2.324879 (2.309817) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.307088 (2.308961) Accuracy: 0.093750 (0.101815)\n",
      "[40/63] Loss: 2.296259 (2.308477) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308383 (2.309603) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.326153 (2.310121) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 29/100, Train Loss: 2.2811, Train Acc: 0.1614, Val. Loss: 2.3104, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.288716 (2.288716) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.294360 (2.283143) Accuracy: 0.156250 (0.173295)\n",
      "[20/251] Loss: 2.277828 (2.286759) Accuracy: 0.156250 (0.157738)\n",
      "[30/251] Loss: 2.241608 (2.282837) Accuracy: 0.218750 (0.160282)\n",
      "[40/251] Loss: 2.259864 (2.281848) Accuracy: 0.250000 (0.165396)\n",
      "[50/251] Loss: 2.280234 (2.283418) Accuracy: 0.156250 (0.158088)\n",
      "[60/251] Loss: 2.276361 (2.283316) Accuracy: 0.125000 (0.158299)\n",
      "[70/251] Loss: 2.267796 (2.283171) Accuracy: 0.125000 (0.158451)\n",
      "[80/251] Loss: 2.300056 (2.282581) Accuracy: 0.156250 (0.161265)\n",
      "[90/251] Loss: 2.318149 (2.282400) Accuracy: 0.093750 (0.160714)\n",
      "[100/251] Loss: 2.259117 (2.283413) Accuracy: 0.312500 (0.159963)\n",
      "[110/251] Loss: 2.267153 (2.282531) Accuracy: 0.218750 (0.161599)\n",
      "[120/251] Loss: 2.273746 (2.282236) Accuracy: 0.187500 (0.163223)\n",
      "[130/251] Loss: 2.236516 (2.282533) Accuracy: 0.218750 (0.161498)\n",
      "[140/251] Loss: 2.277206 (2.282798) Accuracy: 0.218750 (0.160461)\n",
      "[150/251] Loss: 2.287954 (2.282683) Accuracy: 0.125000 (0.159768)\n",
      "[160/251] Loss: 2.230435 (2.282150) Accuracy: 0.218750 (0.159744)\n",
      "[170/251] Loss: 2.278258 (2.282913) Accuracy: 0.218750 (0.158260)\n",
      "[180/251] Loss: 2.271940 (2.283134) Accuracy: 0.250000 (0.157286)\n",
      "[190/251] Loss: 2.306381 (2.283188) Accuracy: 0.125000 (0.156577)\n",
      "[200/251] Loss: 2.269409 (2.282876) Accuracy: 0.250000 (0.157649)\n",
      "[210/251] Loss: 2.251932 (2.282284) Accuracy: 0.312500 (0.158916)\n",
      "[220/251] Loss: 2.228406 (2.281987) Accuracy: 0.312500 (0.159644)\n",
      "[230/251] Loss: 2.271029 (2.282133) Accuracy: 0.156250 (0.158144)\n",
      "[240/251] Loss: 2.280056 (2.281929) Accuracy: 0.125000 (0.159232)\n",
      "[250/251] Loss: 2.362366 (2.281731) Accuracy: 0.000000 (0.159363)\n",
      "[0/63] Loss: 2.292916 (2.292916) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294533 (2.311141) Accuracy: 0.125000 (0.102273)\n",
      "[20/63] Loss: 2.324788 (2.309784) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.307381 (2.308999) Accuracy: 0.062500 (0.101815)\n",
      "[40/63] Loss: 2.297185 (2.308530) Accuracy: 0.093750 (0.102896)\n",
      "[50/63] Loss: 2.308286 (2.309648) Accuracy: 0.125000 (0.099265)\n",
      "[60/63] Loss: 2.327320 (2.310156) Accuracy: 0.093750 (0.098361)\n",
      "Epoch: 30/100, Train Loss: 2.2817, Train Acc: 0.1594, Val. Loss: 2.3105, Val. Acc: 0.0988\n",
      "[0/251] Loss: 2.281105 (2.281105) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.264573 (2.274215) Accuracy: 0.218750 (0.178977)\n",
      "[20/251] Loss: 2.283455 (2.272626) Accuracy: 0.187500 (0.187500)\n",
      "[30/251] Loss: 2.297689 (2.276830) Accuracy: 0.125000 (0.172379)\n",
      "[40/251] Loss: 2.240475 (2.276512) Accuracy: 0.218750 (0.171494)\n",
      "[50/251] Loss: 2.315722 (2.278976) Accuracy: 0.031250 (0.168505)\n",
      "[60/251] Loss: 2.298799 (2.279571) Accuracy: 0.125000 (0.164447)\n",
      "[70/251] Loss: 2.310184 (2.281106) Accuracy: 0.093750 (0.160651)\n",
      "[80/251] Loss: 2.306038 (2.280823) Accuracy: 0.125000 (0.162037)\n",
      "[90/251] Loss: 2.239340 (2.280533) Accuracy: 0.187500 (0.161058)\n",
      "[100/251] Loss: 2.285508 (2.280980) Accuracy: 0.156250 (0.159344)\n",
      "[110/251] Loss: 2.326828 (2.281352) Accuracy: 0.062500 (0.158502)\n",
      "[120/251] Loss: 2.256420 (2.280882) Accuracy: 0.187500 (0.160640)\n",
      "[130/251] Loss: 2.298714 (2.280421) Accuracy: 0.093750 (0.160305)\n",
      "[140/251] Loss: 2.329362 (2.281414) Accuracy: 0.062500 (0.158466)\n",
      "[150/251] Loss: 2.289550 (2.281264) Accuracy: 0.187500 (0.159147)\n",
      "[160/251] Loss: 2.291873 (2.281402) Accuracy: 0.156250 (0.158579)\n",
      "[170/251] Loss: 2.299139 (2.281065) Accuracy: 0.125000 (0.158991)\n",
      "[180/251] Loss: 2.312157 (2.281616) Accuracy: 0.093750 (0.157804)\n",
      "[190/251] Loss: 2.267090 (2.281790) Accuracy: 0.187500 (0.157723)\n",
      "[200/251] Loss: 2.304710 (2.281337) Accuracy: 0.125000 (0.158738)\n",
      "[210/251] Loss: 2.328272 (2.281863) Accuracy: 0.062500 (0.157731)\n",
      "[220/251] Loss: 2.274614 (2.281730) Accuracy: 0.187500 (0.157805)\n",
      "[230/251] Loss: 2.300814 (2.281764) Accuracy: 0.125000 (0.157738)\n",
      "[240/251] Loss: 2.288951 (2.281884) Accuracy: 0.156250 (0.157028)\n",
      "[250/251] Loss: 2.403054 (2.281683) Accuracy: 0.000000 (0.157993)\n",
      "[0/63] Loss: 2.292938 (2.292938) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294862 (2.311193) Accuracy: 0.125000 (0.102273)\n",
      "[20/63] Loss: 2.325004 (2.309891) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.307016 (2.309077) Accuracy: 0.062500 (0.102823)\n",
      "[40/63] Loss: 2.297642 (2.308580) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308361 (2.309700) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.326980 (2.310209) Accuracy: 0.093750 (0.098361)\n",
      "Epoch: 31/100, Train Loss: 2.2817, Train Acc: 0.1580, Val. Loss: 2.3105, Val. Acc: 0.0988\n",
      "[0/251] Loss: 2.292141 (2.292141) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.278956 (2.281858) Accuracy: 0.218750 (0.159091)\n",
      "[20/251] Loss: 2.268497 (2.284939) Accuracy: 0.187500 (0.157738)\n",
      "[30/251] Loss: 2.269893 (2.281263) Accuracy: 0.187500 (0.162298)\n",
      "[40/251] Loss: 2.236181 (2.277797) Accuracy: 0.281250 (0.173018)\n",
      "[50/251] Loss: 2.267111 (2.279810) Accuracy: 0.187500 (0.165441)\n",
      "[60/251] Loss: 2.290374 (2.280442) Accuracy: 0.093750 (0.165984)\n",
      "[70/251] Loss: 2.325505 (2.280413) Accuracy: 0.093750 (0.165053)\n",
      "[80/251] Loss: 2.294668 (2.279893) Accuracy: 0.156250 (0.166667)\n",
      "[90/251] Loss: 2.234117 (2.280791) Accuracy: 0.250000 (0.165522)\n",
      "[100/251] Loss: 2.281233 (2.279300) Accuracy: 0.218750 (0.170173)\n",
      "[110/251] Loss: 2.279301 (2.278918) Accuracy: 0.156250 (0.170608)\n",
      "[120/251] Loss: 2.301253 (2.279153) Accuracy: 0.093750 (0.167097)\n",
      "[130/251] Loss: 2.230205 (2.279064) Accuracy: 0.250000 (0.166985)\n",
      "[140/251] Loss: 2.306457 (2.279214) Accuracy: 0.187500 (0.166002)\n",
      "[150/251] Loss: 2.360631 (2.280588) Accuracy: 0.031250 (0.163907)\n",
      "[160/251] Loss: 2.282048 (2.280659) Accuracy: 0.125000 (0.163626)\n",
      "[170/251] Loss: 2.313863 (2.282698) Accuracy: 0.062500 (0.158991)\n",
      "[180/251] Loss: 2.304754 (2.282631) Accuracy: 0.093750 (0.159185)\n",
      "[190/251] Loss: 2.275379 (2.282761) Accuracy: 0.187500 (0.158704)\n",
      "[200/251] Loss: 2.213624 (2.281946) Accuracy: 0.406250 (0.160759)\n",
      "[210/251] Loss: 2.284925 (2.281851) Accuracy: 0.218750 (0.160989)\n",
      "[220/251] Loss: 2.303340 (2.281584) Accuracy: 0.156250 (0.161058)\n",
      "[230/251] Loss: 2.320612 (2.281352) Accuracy: 0.093750 (0.160850)\n",
      "[240/251] Loss: 2.265216 (2.281630) Accuracy: 0.218750 (0.160529)\n",
      "[250/251] Loss: 2.390009 (2.281486) Accuracy: 0.000000 (0.161230)\n",
      "[0/63] Loss: 2.292521 (2.292521) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294934 (2.311212) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.324956 (2.309881) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307346 (2.309121) Accuracy: 0.062500 (0.099798)\n",
      "[40/63] Loss: 2.297238 (2.308607) Accuracy: 0.093750 (0.101372)\n",
      "[50/63] Loss: 2.308080 (2.309737) Accuracy: 0.125000 (0.098039)\n",
      "[60/63] Loss: 2.327393 (2.310248) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 32/100, Train Loss: 2.2815, Train Acc: 0.1612, Val. Loss: 2.3106, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.285108 (2.285108) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.338483 (2.283043) Accuracy: 0.062500 (0.159091)\n",
      "[20/251] Loss: 2.285045 (2.282548) Accuracy: 0.062500 (0.147321)\n",
      "[30/251] Loss: 2.268490 (2.284714) Accuracy: 0.218750 (0.144153)\n",
      "[40/251] Loss: 2.255427 (2.285216) Accuracy: 0.250000 (0.146341)\n",
      "[50/251] Loss: 2.264624 (2.286183) Accuracy: 0.250000 (0.145221)\n",
      "[60/251] Loss: 2.293549 (2.283775) Accuracy: 0.125000 (0.149078)\n",
      "[70/251] Loss: 2.281059 (2.282750) Accuracy: 0.125000 (0.153169)\n",
      "[80/251] Loss: 2.268421 (2.282606) Accuracy: 0.187500 (0.153549)\n",
      "[90/251] Loss: 2.316983 (2.282956) Accuracy: 0.062500 (0.152473)\n",
      "[100/251] Loss: 2.279852 (2.282609) Accuracy: 0.187500 (0.155631)\n",
      "[110/251] Loss: 2.255020 (2.281790) Accuracy: 0.250000 (0.157095)\n",
      "[120/251] Loss: 2.252383 (2.281178) Accuracy: 0.187500 (0.158058)\n",
      "[130/251] Loss: 2.221319 (2.280951) Accuracy: 0.250000 (0.157920)\n",
      "[140/251] Loss: 2.249264 (2.280624) Accuracy: 0.250000 (0.158910)\n",
      "[150/251] Loss: 2.338770 (2.280135) Accuracy: 0.031250 (0.160389)\n",
      "[160/251] Loss: 2.259453 (2.279579) Accuracy: 0.250000 (0.161879)\n",
      "[170/251] Loss: 2.288141 (2.279935) Accuracy: 0.218750 (0.161550)\n",
      "[180/251] Loss: 2.272218 (2.280327) Accuracy: 0.125000 (0.160739)\n",
      "[190/251] Loss: 2.307269 (2.280790) Accuracy: 0.156250 (0.160668)\n",
      "[200/251] Loss: 2.240686 (2.280671) Accuracy: 0.218750 (0.160759)\n",
      "[210/251] Loss: 2.349390 (2.280468) Accuracy: 0.000000 (0.160841)\n",
      "[220/251] Loss: 2.288460 (2.280851) Accuracy: 0.125000 (0.160209)\n",
      "[230/251] Loss: 2.310886 (2.281393) Accuracy: 0.093750 (0.158956)\n",
      "[240/251] Loss: 2.288678 (2.281351) Accuracy: 0.093750 (0.158584)\n",
      "[250/251] Loss: 2.338625 (2.281108) Accuracy: 0.000000 (0.158989)\n",
      "[0/63] Loss: 2.292644 (2.292644) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294718 (2.311212) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325110 (2.309896) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307434 (2.309135) Accuracy: 0.062500 (0.100806)\n",
      "[40/63] Loss: 2.297026 (2.308626) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308329 (2.309767) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327184 (2.310280) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 33/100, Train Loss: 2.2811, Train Acc: 0.1590, Val. Loss: 2.3106, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.260552 (2.260552) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.248369 (2.274751) Accuracy: 0.156250 (0.159091)\n",
      "[20/251] Loss: 2.315871 (2.282456) Accuracy: 0.062500 (0.154762)\n",
      "[30/251] Loss: 2.290541 (2.280230) Accuracy: 0.062500 (0.164315)\n",
      "[40/251] Loss: 2.316141 (2.281773) Accuracy: 0.062500 (0.163872)\n",
      "[50/251] Loss: 2.315323 (2.281384) Accuracy: 0.125000 (0.162990)\n",
      "[60/251] Loss: 2.264873 (2.281206) Accuracy: 0.187500 (0.164447)\n",
      "[70/251] Loss: 2.236941 (2.280266) Accuracy: 0.187500 (0.164173)\n",
      "[80/251] Loss: 2.273661 (2.280634) Accuracy: 0.218750 (0.161265)\n",
      "[90/251] Loss: 2.317968 (2.280303) Accuracy: 0.062500 (0.160371)\n",
      "[100/251] Loss: 2.274194 (2.280628) Accuracy: 0.125000 (0.158725)\n",
      "[110/251] Loss: 2.275973 (2.279752) Accuracy: 0.250000 (0.161318)\n",
      "[120/251] Loss: 2.312296 (2.280450) Accuracy: 0.156250 (0.159607)\n",
      "[130/251] Loss: 2.304394 (2.281271) Accuracy: 0.156250 (0.158874)\n",
      "[140/251] Loss: 2.293638 (2.280281) Accuracy: 0.125000 (0.160239)\n",
      "[150/251] Loss: 2.275541 (2.280678) Accuracy: 0.187500 (0.160803)\n",
      "[160/251] Loss: 2.281367 (2.280965) Accuracy: 0.156250 (0.159744)\n",
      "[170/251] Loss: 2.293107 (2.281311) Accuracy: 0.125000 (0.158808)\n",
      "[180/251] Loss: 2.287404 (2.281257) Accuracy: 0.187500 (0.158840)\n",
      "[190/251] Loss: 2.265903 (2.280862) Accuracy: 0.156250 (0.159031)\n",
      "[200/251] Loss: 2.252918 (2.280814) Accuracy: 0.218750 (0.159515)\n",
      "[210/251] Loss: 2.247021 (2.280403) Accuracy: 0.250000 (0.161730)\n",
      "[220/251] Loss: 2.281098 (2.280344) Accuracy: 0.218750 (0.162189)\n",
      "[230/251] Loss: 2.255703 (2.280329) Accuracy: 0.218750 (0.161120)\n",
      "[240/251] Loss: 2.275897 (2.280452) Accuracy: 0.187500 (0.160918)\n",
      "[250/251] Loss: 2.370388 (2.281079) Accuracy: 0.000000 (0.159861)\n",
      "[0/63] Loss: 2.292483 (2.292483) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294774 (2.311303) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325351 (2.309974) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307641 (2.309227) Accuracy: 0.062500 (0.099798)\n",
      "[40/63] Loss: 2.297209 (2.308722) Accuracy: 0.093750 (0.101372)\n",
      "[50/63] Loss: 2.308165 (2.309815) Accuracy: 0.125000 (0.098039)\n",
      "[60/63] Loss: 2.327568 (2.310323) Accuracy: 0.093750 (0.097336)\n",
      "Epoch: 34/100, Train Loss: 2.2811, Train Acc: 0.1599, Val. Loss: 2.3106, Val. Acc: 0.0979\n",
      "[0/251] Loss: 2.300040 (2.300040) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.264211 (2.285455) Accuracy: 0.187500 (0.130682)\n",
      "[20/251] Loss: 2.222260 (2.277384) Accuracy: 0.281250 (0.153274)\n",
      "[30/251] Loss: 2.300898 (2.282203) Accuracy: 0.156250 (0.150202)\n",
      "[40/251] Loss: 2.298222 (2.282613) Accuracy: 0.125000 (0.150152)\n",
      "[50/251] Loss: 2.266477 (2.283696) Accuracy: 0.125000 (0.150735)\n",
      "[60/251] Loss: 2.253844 (2.284184) Accuracy: 0.156250 (0.153176)\n",
      "[70/251] Loss: 2.239041 (2.281669) Accuracy: 0.250000 (0.155370)\n",
      "[80/251] Loss: 2.248316 (2.280664) Accuracy: 0.281250 (0.157407)\n",
      "[90/251] Loss: 2.291566 (2.280514) Accuracy: 0.093750 (0.157967)\n",
      "[100/251] Loss: 2.260664 (2.279552) Accuracy: 0.187500 (0.159344)\n",
      "[110/251] Loss: 2.253839 (2.279624) Accuracy: 0.218750 (0.160473)\n",
      "[120/251] Loss: 2.310736 (2.279804) Accuracy: 0.125000 (0.160899)\n",
      "[130/251] Loss: 2.254534 (2.280015) Accuracy: 0.218750 (0.160782)\n",
      "[140/251] Loss: 2.282732 (2.279658) Accuracy: 0.156250 (0.161569)\n",
      "[150/251] Loss: 2.298303 (2.279762) Accuracy: 0.156250 (0.161631)\n",
      "[160/251] Loss: 2.296429 (2.280632) Accuracy: 0.156250 (0.159550)\n",
      "[170/251] Loss: 2.287965 (2.280598) Accuracy: 0.125000 (0.159722)\n",
      "[180/251] Loss: 2.345752 (2.281259) Accuracy: 0.062500 (0.159012)\n",
      "[190/251] Loss: 2.280801 (2.281725) Accuracy: 0.187500 (0.159359)\n",
      "[200/251] Loss: 2.287849 (2.281548) Accuracy: 0.156250 (0.159981)\n",
      "[210/251] Loss: 2.257776 (2.281028) Accuracy: 0.187500 (0.160397)\n",
      "[220/251] Loss: 2.338194 (2.280968) Accuracy: 0.062500 (0.160916)\n",
      "[230/251] Loss: 2.286959 (2.280941) Accuracy: 0.187500 (0.160850)\n",
      "[240/251] Loss: 2.241536 (2.280536) Accuracy: 0.187500 (0.161696)\n",
      "[250/251] Loss: 2.296827 (2.280654) Accuracy: 0.000000 (0.159985)\n",
      "[0/63] Loss: 2.292012 (2.292012) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294836 (2.311319) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325347 (2.309992) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.307784 (2.309273) Accuracy: 0.093750 (0.101815)\n",
      "[40/63] Loss: 2.297084 (2.308749) Accuracy: 0.093750 (0.102896)\n",
      "[50/63] Loss: 2.307948 (2.309853) Accuracy: 0.125000 (0.099265)\n",
      "[60/63] Loss: 2.327428 (2.310351) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 35/100, Train Loss: 2.2807, Train Acc: 0.1600, Val. Loss: 2.3107, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.225182 (2.225182) Accuracy: 0.281250 (0.281250)\n",
      "[10/251] Loss: 2.311632 (2.287167) Accuracy: 0.156250 (0.178977)\n",
      "[20/251] Loss: 2.312492 (2.288893) Accuracy: 0.125000 (0.162202)\n",
      "[30/251] Loss: 2.299414 (2.285328) Accuracy: 0.093750 (0.156250)\n",
      "[40/251] Loss: 2.239021 (2.284509) Accuracy: 0.250000 (0.154726)\n",
      "[50/251] Loss: 2.280692 (2.284110) Accuracy: 0.250000 (0.156250)\n",
      "[60/251] Loss: 2.263880 (2.282222) Accuracy: 0.187500 (0.160348)\n",
      "[70/251] Loss: 2.267152 (2.282178) Accuracy: 0.218750 (0.161092)\n",
      "[80/251] Loss: 2.310502 (2.282144) Accuracy: 0.062500 (0.160108)\n",
      "[90/251] Loss: 2.265066 (2.282489) Accuracy: 0.125000 (0.157280)\n",
      "[100/251] Loss: 2.291623 (2.282729) Accuracy: 0.125000 (0.157797)\n",
      "[110/251] Loss: 2.265323 (2.282134) Accuracy: 0.281250 (0.159347)\n",
      "[120/251] Loss: 2.271819 (2.282864) Accuracy: 0.156250 (0.157283)\n",
      "[130/251] Loss: 2.312104 (2.282819) Accuracy: 0.093750 (0.157681)\n",
      "[140/251] Loss: 2.274567 (2.282236) Accuracy: 0.125000 (0.160239)\n",
      "[150/251] Loss: 2.307029 (2.282102) Accuracy: 0.062500 (0.160596)\n",
      "[160/251] Loss: 2.291742 (2.282168) Accuracy: 0.125000 (0.159938)\n",
      "[170/251] Loss: 2.320295 (2.281338) Accuracy: 0.125000 (0.161184)\n",
      "[180/251] Loss: 2.255108 (2.281164) Accuracy: 0.156250 (0.161775)\n",
      "[190/251] Loss: 2.318812 (2.281567) Accuracy: 0.062500 (0.160013)\n",
      "[200/251] Loss: 2.225765 (2.280613) Accuracy: 0.218750 (0.161070)\n",
      "[210/251] Loss: 2.327611 (2.280960) Accuracy: 0.156250 (0.159656)\n",
      "[220/251] Loss: 2.255154 (2.281369) Accuracy: 0.218750 (0.159502)\n",
      "[230/251] Loss: 2.283747 (2.281230) Accuracy: 0.156250 (0.158956)\n",
      "[240/251] Loss: 2.273007 (2.281001) Accuracy: 0.125000 (0.159232)\n",
      "[250/251] Loss: 2.312271 (2.280582) Accuracy: 0.000000 (0.159612)\n",
      "[0/63] Loss: 2.291810 (2.291810) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294804 (2.311347) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325592 (2.310030) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.308087 (2.309309) Accuracy: 0.093750 (0.102823)\n",
      "[40/63] Loss: 2.297109 (2.308775) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.307865 (2.309885) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327235 (2.310382) Accuracy: 0.093750 (0.097336)\n",
      "Epoch: 36/100, Train Loss: 2.2806, Train Acc: 0.1596, Val. Loss: 2.3107, Val. Acc: 0.0979\n",
      "[0/251] Loss: 2.272992 (2.272992) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.287240 (2.278983) Accuracy: 0.125000 (0.170455)\n",
      "[20/251] Loss: 2.296300 (2.283828) Accuracy: 0.125000 (0.157738)\n",
      "[30/251] Loss: 2.219443 (2.281447) Accuracy: 0.281250 (0.155242)\n",
      "[40/251] Loss: 2.303367 (2.285592) Accuracy: 0.125000 (0.151677)\n",
      "[50/251] Loss: 2.253234 (2.285055) Accuracy: 0.218750 (0.151961)\n",
      "[60/251] Loss: 2.279178 (2.285355) Accuracy: 0.156250 (0.151127)\n",
      "[70/251] Loss: 2.296460 (2.286435) Accuracy: 0.093750 (0.150968)\n",
      "[80/251] Loss: 2.312143 (2.284900) Accuracy: 0.093750 (0.153935)\n",
      "[90/251] Loss: 2.292930 (2.283277) Accuracy: 0.156250 (0.157280)\n",
      "[100/251] Loss: 2.252635 (2.283514) Accuracy: 0.187500 (0.159344)\n",
      "[110/251] Loss: 2.256302 (2.282222) Accuracy: 0.187500 (0.160191)\n",
      "[120/251] Loss: 2.284070 (2.280668) Accuracy: 0.156250 (0.164256)\n",
      "[130/251] Loss: 2.248201 (2.280941) Accuracy: 0.187500 (0.162452)\n",
      "[140/251] Loss: 2.231035 (2.280874) Accuracy: 0.218750 (0.162234)\n",
      "[150/251] Loss: 2.269388 (2.281157) Accuracy: 0.156250 (0.162045)\n",
      "[160/251] Loss: 2.284031 (2.280621) Accuracy: 0.125000 (0.162267)\n",
      "[170/251] Loss: 2.285055 (2.280456) Accuracy: 0.093750 (0.161184)\n",
      "[180/251] Loss: 2.286173 (2.280658) Accuracy: 0.156250 (0.161430)\n",
      "[190/251] Loss: 2.301363 (2.280770) Accuracy: 0.156250 (0.161322)\n",
      "[200/251] Loss: 2.306801 (2.280522) Accuracy: 0.187500 (0.161847)\n",
      "[210/251] Loss: 2.257345 (2.280382) Accuracy: 0.218750 (0.162026)\n",
      "[220/251] Loss: 2.245385 (2.279810) Accuracy: 0.250000 (0.163320)\n",
      "[230/251] Loss: 2.338443 (2.280230) Accuracy: 0.093750 (0.162338)\n",
      "[240/251] Loss: 2.310781 (2.280692) Accuracy: 0.156250 (0.161177)\n",
      "[250/251] Loss: 2.300646 (2.280427) Accuracy: 0.000000 (0.160981)\n",
      "[0/63] Loss: 2.291863 (2.291863) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294857 (2.311355) Accuracy: 0.125000 (0.102273)\n",
      "[20/63] Loss: 2.325439 (2.310042) Accuracy: 0.062500 (0.104167)\n",
      "[30/63] Loss: 2.308030 (2.309298) Accuracy: 0.062500 (0.101815)\n",
      "[40/63] Loss: 2.297121 (2.308769) Accuracy: 0.093750 (0.102896)\n",
      "[50/63] Loss: 2.307990 (2.309898) Accuracy: 0.125000 (0.099265)\n",
      "[60/63] Loss: 2.327162 (2.310394) Accuracy: 0.093750 (0.097336)\n",
      "Epoch: 37/100, Train Loss: 2.2804, Train Acc: 0.1610, Val. Loss: 2.3107, Val. Acc: 0.0979\n",
      "[0/251] Loss: 2.289015 (2.289015) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.298962 (2.294035) Accuracy: 0.156250 (0.136364)\n",
      "[20/251] Loss: 2.312003 (2.293452) Accuracy: 0.093750 (0.138393)\n",
      "[30/251] Loss: 2.243645 (2.288384) Accuracy: 0.218750 (0.147177)\n",
      "[40/251] Loss: 2.239331 (2.281972) Accuracy: 0.281250 (0.159299)\n",
      "[50/251] Loss: 2.259140 (2.279169) Accuracy: 0.218750 (0.164216)\n",
      "[60/251] Loss: 2.211196 (2.280551) Accuracy: 0.312500 (0.159324)\n",
      "[70/251] Loss: 2.274531 (2.281875) Accuracy: 0.156250 (0.157130)\n",
      "[80/251] Loss: 2.264585 (2.282102) Accuracy: 0.187500 (0.158565)\n",
      "[90/251] Loss: 2.257678 (2.283573) Accuracy: 0.156250 (0.154190)\n",
      "[100/251] Loss: 2.244641 (2.282517) Accuracy: 0.218750 (0.156559)\n",
      "[110/251] Loss: 2.187664 (2.281587) Accuracy: 0.250000 (0.157939)\n",
      "[120/251] Loss: 2.288199 (2.280923) Accuracy: 0.187500 (0.159091)\n",
      "[130/251] Loss: 2.267746 (2.281802) Accuracy: 0.156250 (0.157920)\n",
      "[140/251] Loss: 2.295038 (2.281158) Accuracy: 0.156250 (0.158688)\n",
      "[150/251] Loss: 2.285997 (2.280737) Accuracy: 0.156250 (0.159975)\n",
      "[160/251] Loss: 2.287728 (2.280921) Accuracy: 0.156250 (0.159938)\n",
      "[170/251] Loss: 2.273871 (2.281193) Accuracy: 0.218750 (0.159905)\n",
      "[180/251] Loss: 2.279952 (2.281190) Accuracy: 0.156250 (0.159703)\n",
      "[190/251] Loss: 2.280596 (2.281084) Accuracy: 0.156250 (0.160177)\n",
      "[200/251] Loss: 2.301069 (2.280837) Accuracy: 0.187500 (0.160137)\n",
      "[210/251] Loss: 2.295161 (2.281119) Accuracy: 0.125000 (0.159508)\n",
      "[220/251] Loss: 2.285393 (2.281137) Accuracy: 0.187500 (0.159502)\n",
      "[230/251] Loss: 2.270710 (2.280894) Accuracy: 0.250000 (0.160173)\n",
      "[240/251] Loss: 2.258641 (2.280849) Accuracy: 0.187500 (0.159492)\n",
      "[250/251] Loss: 2.026161 (2.279264) Accuracy: 1.000000 (0.164218)\n",
      "[0/63] Loss: 2.291660 (2.291660) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294988 (2.311461) Accuracy: 0.125000 (0.102273)\n",
      "[20/63] Loss: 2.325474 (2.310122) Accuracy: 0.062500 (0.104167)\n",
      "[30/63] Loss: 2.308094 (2.309405) Accuracy: 0.062500 (0.101815)\n",
      "[40/63] Loss: 2.296878 (2.308857) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.307950 (2.309955) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327264 (2.310459) Accuracy: 0.093750 (0.098361)\n",
      "Epoch: 38/100, Train Loss: 2.2793, Train Acc: 0.1642, Val. Loss: 2.3108, Val. Acc: 0.0988\n",
      "[0/251] Loss: 2.238484 (2.238484) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.235447 (2.259623) Accuracy: 0.281250 (0.198864)\n",
      "[20/251] Loss: 2.300802 (2.271287) Accuracy: 0.125000 (0.175595)\n",
      "[30/251] Loss: 2.231857 (2.274091) Accuracy: 0.250000 (0.179435)\n",
      "[40/251] Loss: 2.275729 (2.271861) Accuracy: 0.187500 (0.184451)\n",
      "[50/251] Loss: 2.283010 (2.272699) Accuracy: 0.156250 (0.185049)\n",
      "[60/251] Loss: 2.246978 (2.273224) Accuracy: 0.281250 (0.184426)\n",
      "[70/251] Loss: 2.246896 (2.275714) Accuracy: 0.250000 (0.179577)\n",
      "[80/251] Loss: 2.283578 (2.276466) Accuracy: 0.156250 (0.176312)\n",
      "[90/251] Loss: 2.292773 (2.277504) Accuracy: 0.093750 (0.172390)\n",
      "[100/251] Loss: 2.269336 (2.277466) Accuracy: 0.156250 (0.171720)\n",
      "[110/251] Loss: 2.257721 (2.276989) Accuracy: 0.187500 (0.173142)\n",
      "[120/251] Loss: 2.272855 (2.276945) Accuracy: 0.218750 (0.173037)\n",
      "[130/251] Loss: 2.257899 (2.277025) Accuracy: 0.218750 (0.172471)\n",
      "[140/251] Loss: 2.302012 (2.278027) Accuracy: 0.093750 (0.169770)\n",
      "[150/251] Loss: 2.265599 (2.277494) Accuracy: 0.218750 (0.169702)\n",
      "[160/251] Loss: 2.314296 (2.277937) Accuracy: 0.093750 (0.169837)\n",
      "[170/251] Loss: 2.319592 (2.279076) Accuracy: 0.093750 (0.165387)\n",
      "[180/251] Loss: 2.260569 (2.278836) Accuracy: 0.218750 (0.166436)\n",
      "[190/251] Loss: 2.325929 (2.278841) Accuracy: 0.093750 (0.166067)\n",
      "[200/251] Loss: 2.271666 (2.278655) Accuracy: 0.187500 (0.166511)\n",
      "[210/251] Loss: 2.311868 (2.279154) Accuracy: 0.125000 (0.165432)\n",
      "[220/251] Loss: 2.283074 (2.279572) Accuracy: 0.125000 (0.163462)\n",
      "[230/251] Loss: 2.327570 (2.279947) Accuracy: 0.093750 (0.162338)\n",
      "[240/251] Loss: 2.288540 (2.280266) Accuracy: 0.187500 (0.162344)\n",
      "[250/251] Loss: 2.149940 (2.279641) Accuracy: 1.000000 (0.165463)\n",
      "[0/63] Loss: 2.291567 (2.291567) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.295006 (2.311423) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325475 (2.310107) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.308104 (2.309391) Accuracy: 0.062500 (0.100806)\n",
      "[40/63] Loss: 2.297039 (2.308849) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308038 (2.309978) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327377 (2.310491) Accuracy: 0.093750 (0.097336)\n",
      "Epoch: 39/100, Train Loss: 2.2796, Train Acc: 0.1655, Val. Loss: 2.3108, Val. Acc: 0.0979\n",
      "[0/251] Loss: 2.261497 (2.261497) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.213988 (2.269573) Accuracy: 0.250000 (0.184659)\n",
      "[20/251] Loss: 2.298563 (2.273123) Accuracy: 0.187500 (0.181548)\n",
      "[30/251] Loss: 2.249937 (2.279331) Accuracy: 0.218750 (0.166331)\n",
      "[40/251] Loss: 2.319568 (2.282075) Accuracy: 0.031250 (0.160061)\n",
      "[50/251] Loss: 2.284419 (2.281904) Accuracy: 0.093750 (0.157475)\n",
      "[60/251] Loss: 2.243851 (2.280030) Accuracy: 0.218750 (0.158811)\n",
      "[70/251] Loss: 2.260035 (2.281620) Accuracy: 0.156250 (0.156250)\n",
      "[80/251] Loss: 2.280780 (2.282668) Accuracy: 0.250000 (0.157793)\n",
      "[90/251] Loss: 2.295810 (2.281036) Accuracy: 0.093750 (0.160027)\n",
      "[100/251] Loss: 2.277396 (2.280434) Accuracy: 0.218750 (0.161510)\n",
      "[110/251] Loss: 2.252496 (2.279898) Accuracy: 0.187500 (0.163570)\n",
      "[120/251] Loss: 2.274745 (2.278746) Accuracy: 0.187500 (0.164514)\n",
      "[130/251] Loss: 2.268604 (2.278397) Accuracy: 0.218750 (0.166746)\n",
      "[140/251] Loss: 2.289565 (2.278107) Accuracy: 0.125000 (0.166888)\n",
      "[150/251] Loss: 2.273533 (2.278636) Accuracy: 0.156250 (0.165149)\n",
      "[160/251] Loss: 2.303449 (2.279590) Accuracy: 0.125000 (0.163432)\n",
      "[170/251] Loss: 2.251985 (2.279505) Accuracy: 0.218750 (0.163743)\n",
      "[180/251] Loss: 2.257127 (2.280181) Accuracy: 0.218750 (0.161775)\n",
      "[190/251] Loss: 2.307309 (2.279763) Accuracy: 0.093750 (0.163122)\n",
      "[200/251] Loss: 2.251061 (2.279569) Accuracy: 0.187500 (0.163557)\n",
      "[210/251] Loss: 2.288850 (2.279833) Accuracy: 0.187500 (0.163211)\n",
      "[220/251] Loss: 2.254217 (2.280028) Accuracy: 0.156250 (0.162613)\n",
      "[230/251] Loss: 2.293839 (2.280026) Accuracy: 0.187500 (0.162338)\n",
      "[240/251] Loss: 2.253651 (2.280021) Accuracy: 0.156250 (0.162604)\n",
      "[250/251] Loss: 2.039152 (2.279135) Accuracy: 1.000000 (0.165588)\n",
      "[0/63] Loss: 2.291728 (2.291728) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294699 (2.311369) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325644 (2.310110) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.307392 (2.309420) Accuracy: 0.093750 (0.101815)\n",
      "[40/63] Loss: 2.297385 (2.308854) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308170 (2.309994) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327596 (2.310501) Accuracy: 0.093750 (0.097336)\n",
      "Epoch: 40/100, Train Loss: 2.2791, Train Acc: 0.1656, Val. Loss: 2.3108, Val. Acc: 0.0979\n",
      "[0/251] Loss: 2.291193 (2.291193) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.330213 (2.293641) Accuracy: 0.062500 (0.139205)\n",
      "[20/251] Loss: 2.300608 (2.286048) Accuracy: 0.125000 (0.142857)\n",
      "[30/251] Loss: 2.258786 (2.285518) Accuracy: 0.187500 (0.147177)\n",
      "[40/251] Loss: 2.270160 (2.287067) Accuracy: 0.187500 (0.146341)\n",
      "[50/251] Loss: 2.317664 (2.284813) Accuracy: 0.125000 (0.153799)\n",
      "[60/251] Loss: 2.297543 (2.284125) Accuracy: 0.125000 (0.155738)\n",
      "[70/251] Loss: 2.284763 (2.283573) Accuracy: 0.156250 (0.156250)\n",
      "[80/251] Loss: 2.268448 (2.280542) Accuracy: 0.187500 (0.162423)\n",
      "[90/251] Loss: 2.317540 (2.280868) Accuracy: 0.093750 (0.162775)\n",
      "[100/251] Loss: 2.244661 (2.281862) Accuracy: 0.218750 (0.158416)\n",
      "[110/251] Loss: 2.269409 (2.281519) Accuracy: 0.187500 (0.158784)\n",
      "[120/251] Loss: 2.265834 (2.280788) Accuracy: 0.187500 (0.159349)\n",
      "[130/251] Loss: 2.287825 (2.281233) Accuracy: 0.187500 (0.158874)\n",
      "[140/251] Loss: 2.306126 (2.280486) Accuracy: 0.156250 (0.162677)\n",
      "[150/251] Loss: 2.268215 (2.279896) Accuracy: 0.250000 (0.163493)\n",
      "[160/251] Loss: 2.297350 (2.280258) Accuracy: 0.156250 (0.163238)\n",
      "[170/251] Loss: 2.310268 (2.280407) Accuracy: 0.093750 (0.163560)\n",
      "[180/251] Loss: 2.190162 (2.279801) Accuracy: 0.312500 (0.163329)\n",
      "[190/251] Loss: 2.306137 (2.279497) Accuracy: 0.125000 (0.163449)\n",
      "[200/251] Loss: 2.235069 (2.278943) Accuracy: 0.250000 (0.164179)\n",
      "[210/251] Loss: 2.276625 (2.279367) Accuracy: 0.187500 (0.164100)\n",
      "[220/251] Loss: 2.345743 (2.280114) Accuracy: 0.031250 (0.162472)\n",
      "[230/251] Loss: 2.232006 (2.279357) Accuracy: 0.250000 (0.163826)\n",
      "[240/251] Loss: 2.305959 (2.279575) Accuracy: 0.062500 (0.163771)\n",
      "[250/251] Loss: 2.395821 (2.280438) Accuracy: 0.000000 (0.161977)\n",
      "[0/63] Loss: 2.291754 (2.291754) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294724 (2.311397) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325700 (2.310144) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.307400 (2.309438) Accuracy: 0.093750 (0.101815)\n",
      "[40/63] Loss: 2.297619 (2.308876) Accuracy: 0.093750 (0.102896)\n",
      "[50/63] Loss: 2.308223 (2.310013) Accuracy: 0.125000 (0.099265)\n",
      "[60/63] Loss: 2.327694 (2.310520) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 41/100, Train Loss: 2.2804, Train Acc: 0.1620, Val. Loss: 2.3108, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.213907 (2.213907) Accuracy: 0.281250 (0.281250)\n",
      "[10/251] Loss: 2.286583 (2.266309) Accuracy: 0.125000 (0.173295)\n",
      "[20/251] Loss: 2.265604 (2.273714) Accuracy: 0.187500 (0.163690)\n",
      "[30/251] Loss: 2.289608 (2.279517) Accuracy: 0.125000 (0.156250)\n",
      "[40/251] Loss: 2.254048 (2.281128) Accuracy: 0.250000 (0.154726)\n",
      "[50/251] Loss: 2.280109 (2.279596) Accuracy: 0.156250 (0.159926)\n",
      "[60/251] Loss: 2.302217 (2.280469) Accuracy: 0.156250 (0.158299)\n",
      "[70/251] Loss: 2.292217 (2.281841) Accuracy: 0.156250 (0.155370)\n",
      "[80/251] Loss: 2.315705 (2.282198) Accuracy: 0.031250 (0.152392)\n",
      "[90/251] Loss: 2.268827 (2.282153) Accuracy: 0.218750 (0.155220)\n",
      "[100/251] Loss: 2.296796 (2.282781) Accuracy: 0.125000 (0.153156)\n",
      "[110/251] Loss: 2.329262 (2.283857) Accuracy: 0.031250 (0.150619)\n",
      "[120/251] Loss: 2.252343 (2.281957) Accuracy: 0.250000 (0.154442)\n",
      "[130/251] Loss: 2.287503 (2.282048) Accuracy: 0.250000 (0.153387)\n",
      "[140/251] Loss: 2.232914 (2.281157) Accuracy: 0.218750 (0.154920)\n",
      "[150/251] Loss: 2.250645 (2.280867) Accuracy: 0.187500 (0.156457)\n",
      "[160/251] Loss: 2.316715 (2.280791) Accuracy: 0.093750 (0.157026)\n",
      "[170/251] Loss: 2.296731 (2.280461) Accuracy: 0.125000 (0.158443)\n",
      "[180/251] Loss: 2.303694 (2.280488) Accuracy: 0.187500 (0.158149)\n",
      "[190/251] Loss: 2.261554 (2.280855) Accuracy: 0.156250 (0.157395)\n",
      "[200/251] Loss: 2.291557 (2.281223) Accuracy: 0.125000 (0.157183)\n",
      "[210/251] Loss: 2.243027 (2.281542) Accuracy: 0.281250 (0.156694)\n",
      "[220/251] Loss: 2.283564 (2.280682) Accuracy: 0.093750 (0.159361)\n",
      "[230/251] Loss: 2.241514 (2.280077) Accuracy: 0.218750 (0.160579)\n",
      "[240/251] Loss: 2.254117 (2.279904) Accuracy: 0.281250 (0.161437)\n",
      "[250/251] Loss: 2.332115 (2.280146) Accuracy: 0.000000 (0.161106)\n",
      "[0/63] Loss: 2.291703 (2.291703) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294749 (2.311398) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325642 (2.310165) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.307366 (2.309454) Accuracy: 0.093750 (0.101815)\n",
      "[40/63] Loss: 2.297609 (2.308891) Accuracy: 0.093750 (0.102896)\n",
      "[50/63] Loss: 2.308349 (2.310027) Accuracy: 0.125000 (0.099265)\n",
      "[60/63] Loss: 2.327732 (2.310529) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 42/100, Train Loss: 2.2801, Train Acc: 0.1611, Val. Loss: 2.3108, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.266700 (2.266700) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.247982 (2.275403) Accuracy: 0.250000 (0.178977)\n",
      "[20/251] Loss: 2.269755 (2.273251) Accuracy: 0.187500 (0.188988)\n",
      "[30/251] Loss: 2.256909 (2.274166) Accuracy: 0.187500 (0.186492)\n",
      "[40/251] Loss: 2.209812 (2.273390) Accuracy: 0.250000 (0.185213)\n",
      "[50/251] Loss: 2.254477 (2.276026) Accuracy: 0.218750 (0.178922)\n",
      "[60/251] Loss: 2.272748 (2.278876) Accuracy: 0.250000 (0.175205)\n",
      "[70/251] Loss: 2.277179 (2.279589) Accuracy: 0.156250 (0.173856)\n",
      "[80/251] Loss: 2.307760 (2.281585) Accuracy: 0.125000 (0.168596)\n",
      "[90/251] Loss: 2.311233 (2.281766) Accuracy: 0.062500 (0.166552)\n",
      "[100/251] Loss: 2.250947 (2.281152) Accuracy: 0.218750 (0.166460)\n",
      "[110/251] Loss: 2.269171 (2.281189) Accuracy: 0.218750 (0.164414)\n",
      "[120/251] Loss: 2.299194 (2.279602) Accuracy: 0.156250 (0.167097)\n",
      "[130/251] Loss: 2.260847 (2.279309) Accuracy: 0.218750 (0.166746)\n",
      "[140/251] Loss: 2.287010 (2.279692) Accuracy: 0.125000 (0.164894)\n",
      "[150/251] Loss: 2.296711 (2.280017) Accuracy: 0.156250 (0.164114)\n",
      "[160/251] Loss: 2.257345 (2.278922) Accuracy: 0.250000 (0.166149)\n",
      "[170/251] Loss: 2.306483 (2.279568) Accuracy: 0.062500 (0.164839)\n",
      "[180/251] Loss: 2.318576 (2.279782) Accuracy: 0.062500 (0.165228)\n",
      "[190/251] Loss: 2.254135 (2.279992) Accuracy: 0.187500 (0.164431)\n",
      "[200/251] Loss: 2.292821 (2.279974) Accuracy: 0.156250 (0.164490)\n",
      "[210/251] Loss: 2.292330 (2.279925) Accuracy: 0.156250 (0.163063)\n",
      "[220/251] Loss: 2.266955 (2.279315) Accuracy: 0.156250 (0.164451)\n",
      "[230/251] Loss: 2.283299 (2.279773) Accuracy: 0.156250 (0.163149)\n",
      "[240/251] Loss: 2.243556 (2.279898) Accuracy: 0.250000 (0.162474)\n",
      "[250/251] Loss: 2.399426 (2.280345) Accuracy: 0.000000 (0.161728)\n",
      "[0/63] Loss: 2.291674 (2.291674) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294728 (2.311401) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325697 (2.310180) Accuracy: 0.062500 (0.102679)\n",
      "[30/63] Loss: 2.307319 (2.309461) Accuracy: 0.093750 (0.101815)\n",
      "[40/63] Loss: 2.297611 (2.308897) Accuracy: 0.093750 (0.102896)\n",
      "[50/63] Loss: 2.308344 (2.310035) Accuracy: 0.125000 (0.099265)\n",
      "[60/63] Loss: 2.327674 (2.310535) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 43/100, Train Loss: 2.2803, Train Acc: 0.1617, Val. Loss: 2.3108, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.278939 (2.278939) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.294039 (2.282140) Accuracy: 0.125000 (0.153409)\n",
      "[20/251] Loss: 2.330246 (2.287801) Accuracy: 0.031250 (0.148810)\n",
      "[30/251] Loss: 2.288485 (2.284020) Accuracy: 0.156250 (0.155242)\n",
      "[40/251] Loss: 2.339967 (2.282713) Accuracy: 0.062500 (0.160823)\n",
      "[50/251] Loss: 2.243276 (2.279211) Accuracy: 0.218750 (0.168505)\n",
      "[60/251] Loss: 2.222845 (2.280949) Accuracy: 0.343750 (0.161885)\n",
      "[70/251] Loss: 2.267317 (2.280675) Accuracy: 0.156250 (0.161532)\n",
      "[80/251] Loss: 2.301449 (2.279972) Accuracy: 0.093750 (0.164738)\n",
      "[90/251] Loss: 2.267183 (2.279457) Accuracy: 0.187500 (0.165179)\n",
      "[100/251] Loss: 2.267460 (2.280318) Accuracy: 0.156250 (0.162748)\n",
      "[110/251] Loss: 2.265465 (2.278752) Accuracy: 0.250000 (0.165259)\n",
      "[120/251] Loss: 2.320589 (2.279534) Accuracy: 0.062500 (0.164256)\n",
      "[130/251] Loss: 2.272333 (2.278729) Accuracy: 0.187500 (0.165315)\n",
      "[140/251] Loss: 2.220979 (2.278165) Accuracy: 0.250000 (0.168218)\n",
      "[150/251] Loss: 2.256195 (2.278472) Accuracy: 0.156250 (0.165977)\n",
      "[160/251] Loss: 2.289584 (2.278860) Accuracy: 0.125000 (0.164984)\n",
      "[170/251] Loss: 2.298579 (2.278825) Accuracy: 0.125000 (0.164291)\n",
      "[180/251] Loss: 2.281721 (2.278631) Accuracy: 0.125000 (0.164883)\n",
      "[190/251] Loss: 2.294272 (2.279179) Accuracy: 0.093750 (0.162958)\n",
      "[200/251] Loss: 2.293946 (2.279753) Accuracy: 0.093750 (0.162002)\n",
      "[210/251] Loss: 2.267463 (2.279502) Accuracy: 0.187500 (0.161286)\n",
      "[220/251] Loss: 2.286239 (2.279334) Accuracy: 0.187500 (0.162472)\n",
      "[230/251] Loss: 2.276764 (2.279470) Accuracy: 0.187500 (0.162744)\n",
      "[240/251] Loss: 2.308789 (2.279516) Accuracy: 0.156250 (0.163252)\n",
      "[250/251] Loss: 2.239177 (2.279672) Accuracy: 0.000000 (0.162102)\n",
      "[0/63] Loss: 2.291649 (2.291649) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294878 (2.311430) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325552 (2.310210) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307376 (2.309465) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297536 (2.308888) Accuracy: 0.093750 (0.101372)\n",
      "[50/63] Loss: 2.308529 (2.310048) Accuracy: 0.125000 (0.098039)\n",
      "[60/63] Loss: 2.327665 (2.310546) Accuracy: 0.093750 (0.096311)\n",
      "Epoch: 44/100, Train Loss: 2.2797, Train Acc: 0.1621, Val. Loss: 2.3109, Val. Acc: 0.0969\n",
      "[0/251] Loss: 2.275690 (2.275690) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.231841 (2.281845) Accuracy: 0.187500 (0.147727)\n",
      "[20/251] Loss: 2.278962 (2.281784) Accuracy: 0.156250 (0.145833)\n",
      "[30/251] Loss: 2.293788 (2.280478) Accuracy: 0.156250 (0.153226)\n",
      "[40/251] Loss: 2.293561 (2.280394) Accuracy: 0.156250 (0.152439)\n",
      "[50/251] Loss: 2.305298 (2.280362) Accuracy: 0.093750 (0.154412)\n",
      "[60/251] Loss: 2.289563 (2.281082) Accuracy: 0.156250 (0.152664)\n",
      "[70/251] Loss: 2.240663 (2.279036) Accuracy: 0.250000 (0.158011)\n",
      "[80/251] Loss: 2.271357 (2.279662) Accuracy: 0.218750 (0.158565)\n",
      "[90/251] Loss: 2.276204 (2.279973) Accuracy: 0.156250 (0.157967)\n",
      "[100/251] Loss: 2.252980 (2.278827) Accuracy: 0.218750 (0.161200)\n",
      "[110/251] Loss: 2.262850 (2.278902) Accuracy: 0.250000 (0.160755)\n",
      "[120/251] Loss: 2.266134 (2.278683) Accuracy: 0.093750 (0.160899)\n",
      "[130/251] Loss: 2.248682 (2.277903) Accuracy: 0.187500 (0.161021)\n",
      "[140/251] Loss: 2.293663 (2.278072) Accuracy: 0.125000 (0.161791)\n",
      "[150/251] Loss: 2.261248 (2.277433) Accuracy: 0.218750 (0.163907)\n",
      "[160/251] Loss: 2.310267 (2.278255) Accuracy: 0.093750 (0.162461)\n",
      "[170/251] Loss: 2.248356 (2.277870) Accuracy: 0.343750 (0.164839)\n",
      "[180/251] Loss: 2.280945 (2.278529) Accuracy: 0.156250 (0.163501)\n",
      "[190/251] Loss: 2.318292 (2.279232) Accuracy: 0.093750 (0.162140)\n",
      "[200/251] Loss: 2.226432 (2.279360) Accuracy: 0.281250 (0.162469)\n",
      "[210/251] Loss: 2.296927 (2.279841) Accuracy: 0.093750 (0.162026)\n",
      "[220/251] Loss: 2.284028 (2.279841) Accuracy: 0.218750 (0.162472)\n",
      "[230/251] Loss: 2.270461 (2.280032) Accuracy: 0.125000 (0.161932)\n",
      "[240/251] Loss: 2.251374 (2.280000) Accuracy: 0.250000 (0.161826)\n",
      "[250/251] Loss: 2.389212 (2.280197) Accuracy: 0.000000 (0.161728)\n",
      "[0/63] Loss: 2.291677 (2.291677) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294861 (2.311436) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325597 (2.310218) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307388 (2.309468) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297562 (2.308890) Accuracy: 0.093750 (0.101372)\n",
      "[50/63] Loss: 2.308503 (2.310050) Accuracy: 0.125000 (0.098039)\n",
      "[60/63] Loss: 2.327719 (2.310552) Accuracy: 0.093750 (0.096311)\n",
      "Epoch: 45/100, Train Loss: 2.2802, Train Acc: 0.1617, Val. Loss: 2.3109, Val. Acc: 0.0969\n",
      "[0/251] Loss: 2.298708 (2.298708) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.312590 (2.275989) Accuracy: 0.156250 (0.178977)\n",
      "[20/251] Loss: 2.284428 (2.274155) Accuracy: 0.156250 (0.183036)\n",
      "[30/251] Loss: 2.282868 (2.273177) Accuracy: 0.093750 (0.178427)\n",
      "[40/251] Loss: 2.220848 (2.276560) Accuracy: 0.218750 (0.169207)\n",
      "[50/251] Loss: 2.260737 (2.279217) Accuracy: 0.218750 (0.165441)\n",
      "[60/251] Loss: 2.265170 (2.279591) Accuracy: 0.281250 (0.168545)\n",
      "[70/251] Loss: 2.274948 (2.281214) Accuracy: 0.218750 (0.165933)\n",
      "[80/251] Loss: 2.276330 (2.279432) Accuracy: 0.156250 (0.167438)\n",
      "[90/251] Loss: 2.284479 (2.277807) Accuracy: 0.156250 (0.172047)\n",
      "[100/251] Loss: 2.313065 (2.279069) Accuracy: 0.093750 (0.169554)\n",
      "[110/251] Loss: 2.330313 (2.278995) Accuracy: 0.031250 (0.169764)\n",
      "[120/251] Loss: 2.268707 (2.279281) Accuracy: 0.218750 (0.169163)\n",
      "[130/251] Loss: 2.276421 (2.278570) Accuracy: 0.156250 (0.169370)\n",
      "[140/251] Loss: 2.312876 (2.279233) Accuracy: 0.093750 (0.167110)\n",
      "[150/251] Loss: 2.267966 (2.279026) Accuracy: 0.187500 (0.166184)\n",
      "[160/251] Loss: 2.258312 (2.279269) Accuracy: 0.218750 (0.165179)\n",
      "[170/251] Loss: 2.332634 (2.279389) Accuracy: 0.062500 (0.164291)\n",
      "[180/251] Loss: 2.259828 (2.279050) Accuracy: 0.187500 (0.164883)\n",
      "[190/251] Loss: 2.282260 (2.279113) Accuracy: 0.125000 (0.163776)\n",
      "[200/251] Loss: 2.286878 (2.278534) Accuracy: 0.125000 (0.164490)\n",
      "[210/251] Loss: 2.280627 (2.278837) Accuracy: 0.187500 (0.164248)\n",
      "[220/251] Loss: 2.297070 (2.279374) Accuracy: 0.093750 (0.163744)\n",
      "[230/251] Loss: 2.267185 (2.279502) Accuracy: 0.187500 (0.163420)\n",
      "[240/251] Loss: 2.307379 (2.280001) Accuracy: 0.125000 (0.162604)\n",
      "[250/251] Loss: 2.342416 (2.279979) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291744 (2.291744) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294688 (2.311431) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325787 (2.310224) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307383 (2.309482) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297737 (2.308909) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308489 (2.310065) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327805 (2.310568) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 46/100, Train Loss: 2.2800, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.294142 (2.294142) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.286225 (2.282274) Accuracy: 0.187500 (0.156250)\n",
      "[20/251] Loss: 2.299956 (2.283981) Accuracy: 0.125000 (0.156250)\n",
      "[30/251] Loss: 2.261965 (2.281019) Accuracy: 0.187500 (0.163306)\n",
      "[40/251] Loss: 2.259562 (2.282585) Accuracy: 0.250000 (0.157012)\n",
      "[50/251] Loss: 2.232893 (2.280390) Accuracy: 0.312500 (0.166054)\n",
      "[60/251] Loss: 2.249612 (2.281672) Accuracy: 0.281250 (0.167008)\n",
      "[70/251] Loss: 2.277008 (2.278616) Accuracy: 0.187500 (0.172095)\n",
      "[80/251] Loss: 2.307298 (2.278377) Accuracy: 0.125000 (0.170139)\n",
      "[90/251] Loss: 2.320972 (2.279564) Accuracy: 0.093750 (0.167582)\n",
      "[100/251] Loss: 2.302774 (2.279385) Accuracy: 0.125000 (0.167698)\n",
      "[110/251] Loss: 2.318749 (2.280264) Accuracy: 0.062500 (0.164977)\n",
      "[120/251] Loss: 2.306509 (2.279984) Accuracy: 0.125000 (0.167097)\n",
      "[130/251] Loss: 2.301700 (2.280532) Accuracy: 0.125000 (0.166985)\n",
      "[140/251] Loss: 2.273675 (2.280179) Accuracy: 0.156250 (0.166888)\n",
      "[150/251] Loss: 2.292670 (2.281357) Accuracy: 0.031250 (0.163286)\n",
      "[160/251] Loss: 2.285050 (2.281812) Accuracy: 0.156250 (0.163043)\n",
      "[170/251] Loss: 2.261609 (2.281113) Accuracy: 0.156250 (0.164656)\n",
      "[180/251] Loss: 2.251510 (2.281158) Accuracy: 0.187500 (0.163847)\n",
      "[190/251] Loss: 2.255588 (2.280559) Accuracy: 0.218750 (0.164758)\n",
      "[200/251] Loss: 2.303559 (2.280863) Accuracy: 0.031250 (0.163091)\n",
      "[210/251] Loss: 2.291018 (2.280794) Accuracy: 0.156250 (0.162767)\n",
      "[220/251] Loss: 2.256862 (2.280578) Accuracy: 0.156250 (0.162613)\n",
      "[230/251] Loss: 2.258024 (2.279780) Accuracy: 0.218750 (0.163555)\n",
      "[240/251] Loss: 2.284721 (2.280302) Accuracy: 0.125000 (0.161826)\n",
      "[250/251] Loss: 2.376092 (2.280065) Accuracy: 0.000000 (0.162102)\n",
      "[0/63] Loss: 2.291725 (2.291725) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294649 (2.311431) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325833 (2.310230) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307357 (2.309487) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297736 (2.308914) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308493 (2.310071) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327805 (2.310572) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 47/100, Train Loss: 2.2801, Train Acc: 0.1621, Val. Loss: 2.3109, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.321591 (2.321591) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.288252 (2.285994) Accuracy: 0.187500 (0.150568)\n",
      "[20/251] Loss: 2.288292 (2.281772) Accuracy: 0.125000 (0.160714)\n",
      "[30/251] Loss: 2.269836 (2.282424) Accuracy: 0.187500 (0.159274)\n",
      "[40/251] Loss: 2.295927 (2.281712) Accuracy: 0.156250 (0.163110)\n",
      "[50/251] Loss: 2.223657 (2.279780) Accuracy: 0.312500 (0.166667)\n",
      "[60/251] Loss: 2.309125 (2.278908) Accuracy: 0.125000 (0.165984)\n",
      "[70/251] Loss: 2.272481 (2.279340) Accuracy: 0.218750 (0.165493)\n",
      "[80/251] Loss: 2.250192 (2.279657) Accuracy: 0.156250 (0.163194)\n",
      "[90/251] Loss: 2.321559 (2.280551) Accuracy: 0.062500 (0.160714)\n",
      "[100/251] Loss: 2.283261 (2.281381) Accuracy: 0.187500 (0.157797)\n",
      "[110/251] Loss: 2.249364 (2.280115) Accuracy: 0.312500 (0.160191)\n",
      "[120/251] Loss: 2.275599 (2.279146) Accuracy: 0.187500 (0.161415)\n",
      "[130/251] Loss: 2.288348 (2.279620) Accuracy: 0.125000 (0.161021)\n",
      "[140/251] Loss: 2.280777 (2.279042) Accuracy: 0.187500 (0.160904)\n",
      "[150/251] Loss: 2.240002 (2.279306) Accuracy: 0.218750 (0.160182)\n",
      "[160/251] Loss: 2.275047 (2.279749) Accuracy: 0.156250 (0.159550)\n",
      "[170/251] Loss: 2.247029 (2.280006) Accuracy: 0.156250 (0.159905)\n",
      "[180/251] Loss: 2.294658 (2.280179) Accuracy: 0.125000 (0.160739)\n",
      "[190/251] Loss: 2.285793 (2.280477) Accuracy: 0.093750 (0.160504)\n",
      "[200/251] Loss: 2.299334 (2.280462) Accuracy: 0.125000 (0.161070)\n",
      "[210/251] Loss: 2.281595 (2.280329) Accuracy: 0.187500 (0.160693)\n",
      "[220/251] Loss: 2.225599 (2.280006) Accuracy: 0.281250 (0.161623)\n",
      "[230/251] Loss: 2.256323 (2.279495) Accuracy: 0.218750 (0.162202)\n",
      "[240/251] Loss: 2.307163 (2.279511) Accuracy: 0.093750 (0.161826)\n",
      "[250/251] Loss: 2.401737 (2.280129) Accuracy: 0.000000 (0.161853)\n",
      "[0/63] Loss: 2.291736 (2.291736) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294619 (2.311430) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325877 (2.310234) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307353 (2.309490) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297791 (2.308919) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308498 (2.310076) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327809 (2.310578) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 48/100, Train Loss: 2.2801, Train Acc: 0.1619, Val. Loss: 2.3109, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.311505 (2.311505) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.306191 (2.288532) Accuracy: 0.125000 (0.144886)\n",
      "[20/251] Loss: 2.295465 (2.275250) Accuracy: 0.125000 (0.180060)\n",
      "[30/251] Loss: 2.307449 (2.278177) Accuracy: 0.093750 (0.172379)\n",
      "[40/251] Loss: 2.285120 (2.280443) Accuracy: 0.125000 (0.168445)\n",
      "[50/251] Loss: 2.266096 (2.281928) Accuracy: 0.281250 (0.165441)\n",
      "[60/251] Loss: 2.231411 (2.282244) Accuracy: 0.187500 (0.163422)\n",
      "[70/251] Loss: 2.313599 (2.283164) Accuracy: 0.062500 (0.159771)\n",
      "[80/251] Loss: 2.297738 (2.282694) Accuracy: 0.125000 (0.160494)\n",
      "[90/251] Loss: 2.267405 (2.281957) Accuracy: 0.218750 (0.161745)\n",
      "[100/251] Loss: 2.216870 (2.281564) Accuracy: 0.250000 (0.161510)\n",
      "[110/251] Loss: 2.256667 (2.280613) Accuracy: 0.218750 (0.162444)\n",
      "[120/251] Loss: 2.271127 (2.279845) Accuracy: 0.218750 (0.164514)\n",
      "[130/251] Loss: 2.277232 (2.279767) Accuracy: 0.156250 (0.165076)\n",
      "[140/251] Loss: 2.230281 (2.278756) Accuracy: 0.218750 (0.164894)\n",
      "[150/251] Loss: 2.271921 (2.277908) Accuracy: 0.218750 (0.167632)\n",
      "[160/251] Loss: 2.303757 (2.278285) Accuracy: 0.093750 (0.167120)\n",
      "[170/251] Loss: 2.366060 (2.278626) Accuracy: 0.031250 (0.166484)\n",
      "[180/251] Loss: 2.292451 (2.279083) Accuracy: 0.156250 (0.165573)\n",
      "[190/251] Loss: 2.307185 (2.280123) Accuracy: 0.125000 (0.163449)\n",
      "[200/251] Loss: 2.279673 (2.280373) Accuracy: 0.125000 (0.162780)\n",
      "[210/251] Loss: 2.230698 (2.280179) Accuracy: 0.250000 (0.162322)\n",
      "[220/251] Loss: 2.310735 (2.279998) Accuracy: 0.156250 (0.163320)\n",
      "[230/251] Loss: 2.242852 (2.279645) Accuracy: 0.187500 (0.163555)\n",
      "[240/251] Loss: 2.281568 (2.279571) Accuracy: 0.156250 (0.162474)\n",
      "[250/251] Loss: 2.380094 (2.280014) Accuracy: 0.000000 (0.162102)\n",
      "[0/63] Loss: 2.291744 (2.291744) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294602 (2.311430) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325923 (2.310241) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307318 (2.309493) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297887 (2.308922) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308500 (2.310080) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327836 (2.310582) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 49/100, Train Loss: 2.2800, Train Acc: 0.1621, Val. Loss: 2.3109, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.290831 (2.290831) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.298448 (2.279818) Accuracy: 0.125000 (0.161932)\n",
      "[20/251] Loss: 2.263620 (2.280888) Accuracy: 0.250000 (0.159226)\n",
      "[30/251] Loss: 2.258020 (2.280809) Accuracy: 0.125000 (0.158266)\n",
      "[40/251] Loss: 2.296980 (2.281156) Accuracy: 0.156250 (0.160061)\n",
      "[50/251] Loss: 2.276535 (2.282088) Accuracy: 0.187500 (0.158088)\n",
      "[60/251] Loss: 2.276465 (2.280337) Accuracy: 0.218750 (0.162910)\n",
      "[70/251] Loss: 2.304923 (2.281777) Accuracy: 0.156250 (0.161092)\n",
      "[80/251] Loss: 2.326093 (2.281911) Accuracy: 0.062500 (0.158179)\n",
      "[90/251] Loss: 2.324332 (2.281412) Accuracy: 0.093750 (0.158310)\n",
      "[100/251] Loss: 2.294525 (2.283747) Accuracy: 0.156250 (0.154084)\n",
      "[110/251] Loss: 2.285829 (2.282538) Accuracy: 0.125000 (0.154561)\n",
      "[120/251] Loss: 2.291316 (2.281735) Accuracy: 0.156250 (0.156767)\n",
      "[130/251] Loss: 2.268379 (2.280985) Accuracy: 0.218750 (0.158397)\n",
      "[140/251] Loss: 2.249533 (2.281200) Accuracy: 0.218750 (0.158688)\n",
      "[150/251] Loss: 2.249698 (2.280588) Accuracy: 0.281250 (0.160596)\n",
      "[160/251] Loss: 2.307039 (2.280254) Accuracy: 0.125000 (0.161297)\n",
      "[170/251] Loss: 2.280382 (2.280982) Accuracy: 0.218750 (0.160088)\n",
      "[180/251] Loss: 2.258113 (2.280949) Accuracy: 0.218750 (0.159876)\n",
      "[190/251] Loss: 2.265432 (2.281088) Accuracy: 0.187500 (0.159195)\n",
      "[200/251] Loss: 2.268683 (2.281110) Accuracy: 0.156250 (0.159204)\n",
      "[210/251] Loss: 2.268081 (2.281041) Accuracy: 0.156250 (0.158768)\n",
      "[220/251] Loss: 2.255354 (2.280379) Accuracy: 0.250000 (0.160351)\n",
      "[230/251] Loss: 2.258756 (2.279981) Accuracy: 0.250000 (0.161255)\n",
      "[240/251] Loss: 2.226765 (2.279379) Accuracy: 0.312500 (0.163511)\n",
      "[250/251] Loss: 2.096469 (2.278890) Accuracy: 1.000000 (0.166086)\n",
      "[0/63] Loss: 2.291692 (2.291692) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294756 (2.311458) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325884 (2.310268) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307317 (2.309505) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297976 (2.308942) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308584 (2.310086) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327840 (2.310581) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 50/100, Train Loss: 2.2789, Train Acc: 0.1661, Val. Loss: 2.3109, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.299022 (2.299022) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.286436 (2.278352) Accuracy: 0.125000 (0.167614)\n",
      "[20/251] Loss: 2.307641 (2.277625) Accuracy: 0.187500 (0.172619)\n",
      "[30/251] Loss: 2.299211 (2.273986) Accuracy: 0.125000 (0.176411)\n",
      "[40/251] Loss: 2.241309 (2.272916) Accuracy: 0.187500 (0.178354)\n",
      "[50/251] Loss: 2.346632 (2.276294) Accuracy: 0.031250 (0.168505)\n",
      "[60/251] Loss: 2.290799 (2.276419) Accuracy: 0.062500 (0.166496)\n",
      "[70/251] Loss: 2.273086 (2.275468) Accuracy: 0.218750 (0.169454)\n",
      "[80/251] Loss: 2.244750 (2.275284) Accuracy: 0.281250 (0.170910)\n",
      "[90/251] Loss: 2.300170 (2.276378) Accuracy: 0.156250 (0.169986)\n",
      "[100/251] Loss: 2.286743 (2.276351) Accuracy: 0.093750 (0.169864)\n",
      "[110/251] Loss: 2.247638 (2.276098) Accuracy: 0.312500 (0.169482)\n",
      "[120/251] Loss: 2.270902 (2.276404) Accuracy: 0.156250 (0.168905)\n",
      "[130/251] Loss: 2.264370 (2.276502) Accuracy: 0.218750 (0.168177)\n",
      "[140/251] Loss: 2.309568 (2.277253) Accuracy: 0.125000 (0.166002)\n",
      "[150/251] Loss: 2.311117 (2.278030) Accuracy: 0.125000 (0.164942)\n",
      "[160/251] Loss: 2.337810 (2.278199) Accuracy: 0.062500 (0.164014)\n",
      "[170/251] Loss: 2.322225 (2.278084) Accuracy: 0.093750 (0.164291)\n",
      "[180/251] Loss: 2.291885 (2.278164) Accuracy: 0.125000 (0.163847)\n",
      "[190/251] Loss: 2.308017 (2.278435) Accuracy: 0.062500 (0.163122)\n",
      "[200/251] Loss: 2.220512 (2.277694) Accuracy: 0.343750 (0.164490)\n",
      "[210/251] Loss: 2.303952 (2.278134) Accuracy: 0.125000 (0.164100)\n",
      "[220/251] Loss: 2.279248 (2.278711) Accuracy: 0.156250 (0.163886)\n",
      "[230/251] Loss: 2.270606 (2.279406) Accuracy: 0.187500 (0.162879)\n",
      "[240/251] Loss: 2.270973 (2.279468) Accuracy: 0.156250 (0.162993)\n",
      "[250/251] Loss: 2.242958 (2.279422) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291659 (2.291659) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294737 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325952 (2.310280) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307300 (2.309506) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297938 (2.308936) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308622 (2.310084) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327835 (2.310580) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 51/100, Train Loss: 2.2794, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.286292 (2.286292) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.283169 (2.270262) Accuracy: 0.125000 (0.164773)\n",
      "[20/251] Loss: 2.212452 (2.263637) Accuracy: 0.250000 (0.183036)\n",
      "[30/251] Loss: 2.250554 (2.268078) Accuracy: 0.218750 (0.181452)\n",
      "[40/251] Loss: 2.220953 (2.269064) Accuracy: 0.312500 (0.181402)\n",
      "[50/251] Loss: 2.181878 (2.272543) Accuracy: 0.406250 (0.178309)\n",
      "[60/251] Loss: 2.263493 (2.272819) Accuracy: 0.156250 (0.178279)\n",
      "[70/251] Loss: 2.335061 (2.275879) Accuracy: 0.062500 (0.172095)\n",
      "[80/251] Loss: 2.287570 (2.276083) Accuracy: 0.156250 (0.172840)\n",
      "[90/251] Loss: 2.273665 (2.276569) Accuracy: 0.187500 (0.171016)\n",
      "[100/251] Loss: 2.305485 (2.276749) Accuracy: 0.093750 (0.169864)\n",
      "[110/251] Loss: 2.279430 (2.277316) Accuracy: 0.156250 (0.167793)\n",
      "[120/251] Loss: 2.286007 (2.277045) Accuracy: 0.125000 (0.167872)\n",
      "[130/251] Loss: 2.290431 (2.278652) Accuracy: 0.156250 (0.166269)\n",
      "[140/251] Loss: 2.303430 (2.279570) Accuracy: 0.156250 (0.165337)\n",
      "[150/251] Loss: 2.327271 (2.280314) Accuracy: 0.062500 (0.164321)\n",
      "[160/251] Loss: 2.276035 (2.280237) Accuracy: 0.125000 (0.163043)\n",
      "[170/251] Loss: 2.304309 (2.279862) Accuracy: 0.093750 (0.163194)\n",
      "[180/251] Loss: 2.301263 (2.280031) Accuracy: 0.125000 (0.161948)\n",
      "[190/251] Loss: 2.275707 (2.279939) Accuracy: 0.156250 (0.161649)\n",
      "[200/251] Loss: 2.269338 (2.279121) Accuracy: 0.187500 (0.163557)\n",
      "[210/251] Loss: 2.321948 (2.279800) Accuracy: 0.031250 (0.162026)\n",
      "[220/251] Loss: 2.304017 (2.279817) Accuracy: 0.125000 (0.162330)\n",
      "[230/251] Loss: 2.281209 (2.279524) Accuracy: 0.093750 (0.163285)\n",
      "[240/251] Loss: 2.289448 (2.279588) Accuracy: 0.125000 (0.163252)\n",
      "[250/251] Loss: 2.377664 (2.279921) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291631 (2.291631) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294723 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.325976 (2.310281) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307359 (2.309510) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297956 (2.308941) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308605 (2.310088) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327856 (2.310584) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 52/100, Train Loss: 2.2799, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.247589 (2.247589) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.317706 (2.275194) Accuracy: 0.062500 (0.156250)\n",
      "[20/251] Loss: 2.298553 (2.278956) Accuracy: 0.125000 (0.157738)\n",
      "[30/251] Loss: 2.284212 (2.276469) Accuracy: 0.093750 (0.159274)\n",
      "[40/251] Loss: 2.257432 (2.277292) Accuracy: 0.250000 (0.162348)\n",
      "[50/251] Loss: 2.255337 (2.277459) Accuracy: 0.218750 (0.162377)\n",
      "[60/251] Loss: 2.291464 (2.276226) Accuracy: 0.125000 (0.167008)\n",
      "[70/251] Loss: 2.286322 (2.276021) Accuracy: 0.218750 (0.166813)\n",
      "[80/251] Loss: 2.304479 (2.276843) Accuracy: 0.093750 (0.166667)\n",
      "[90/251] Loss: 2.339992 (2.277117) Accuracy: 0.031250 (0.166896)\n",
      "[100/251] Loss: 2.254490 (2.276003) Accuracy: 0.187500 (0.167079)\n",
      "[110/251] Loss: 2.300319 (2.277991) Accuracy: 0.125000 (0.164414)\n",
      "[120/251] Loss: 2.273574 (2.278719) Accuracy: 0.187500 (0.163481)\n",
      "[130/251] Loss: 2.291974 (2.278191) Accuracy: 0.125000 (0.163884)\n",
      "[140/251] Loss: 2.292311 (2.278419) Accuracy: 0.187500 (0.163785)\n",
      "[150/251] Loss: 2.302498 (2.279527) Accuracy: 0.156250 (0.162459)\n",
      "[160/251] Loss: 2.280805 (2.280311) Accuracy: 0.218750 (0.161297)\n",
      "[170/251] Loss: 2.247654 (2.278926) Accuracy: 0.187500 (0.163560)\n",
      "[180/251] Loss: 2.258137 (2.278653) Accuracy: 0.187500 (0.163674)\n",
      "[190/251] Loss: 2.288667 (2.279075) Accuracy: 0.187500 (0.163285)\n",
      "[200/251] Loss: 2.276492 (2.278897) Accuracy: 0.156250 (0.164335)\n",
      "[210/251] Loss: 2.299234 (2.279304) Accuracy: 0.093750 (0.163803)\n",
      "[220/251] Loss: 2.274167 (2.279383) Accuracy: 0.093750 (0.163886)\n",
      "[230/251] Loss: 2.290442 (2.279630) Accuracy: 0.125000 (0.163014)\n",
      "[240/251] Loss: 2.283388 (2.279180) Accuracy: 0.125000 (0.164030)\n",
      "[250/251] Loss: 2.314145 (2.279658) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291622 (2.291622) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294730 (2.311463) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326011 (2.310285) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307367 (2.309514) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297958 (2.308942) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308632 (2.310092) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327882 (2.310588) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 53/100, Train Loss: 2.2797, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.311750 (2.311750) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.261906 (2.289496) Accuracy: 0.187500 (0.153409)\n",
      "[20/251] Loss: 2.265143 (2.284049) Accuracy: 0.250000 (0.156250)\n",
      "[30/251] Loss: 2.311288 (2.285857) Accuracy: 0.156250 (0.152218)\n",
      "[40/251] Loss: 2.288509 (2.278761) Accuracy: 0.156250 (0.163872)\n",
      "[50/251] Loss: 2.291699 (2.278055) Accuracy: 0.156250 (0.166054)\n",
      "[60/251] Loss: 2.309496 (2.279794) Accuracy: 0.093750 (0.163422)\n",
      "[70/251] Loss: 2.330310 (2.282737) Accuracy: 0.093750 (0.155810)\n",
      "[80/251] Loss: 2.223392 (2.281431) Accuracy: 0.250000 (0.159722)\n",
      "[90/251] Loss: 2.216806 (2.280275) Accuracy: 0.218750 (0.160371)\n",
      "[100/251] Loss: 2.297276 (2.279044) Accuracy: 0.156250 (0.165223)\n",
      "[110/251] Loss: 2.229530 (2.278342) Accuracy: 0.312500 (0.167793)\n",
      "[120/251] Loss: 2.291780 (2.278664) Accuracy: 0.062500 (0.165806)\n",
      "[130/251] Loss: 2.273459 (2.278930) Accuracy: 0.156250 (0.164599)\n",
      "[140/251] Loss: 2.268266 (2.279282) Accuracy: 0.187500 (0.163342)\n",
      "[150/251] Loss: 2.226932 (2.279704) Accuracy: 0.312500 (0.163493)\n",
      "[160/251] Loss: 2.311491 (2.278794) Accuracy: 0.125000 (0.164984)\n",
      "[170/251] Loss: 2.266701 (2.278547) Accuracy: 0.218750 (0.166301)\n",
      "[180/251] Loss: 2.308748 (2.279299) Accuracy: 0.062500 (0.164710)\n",
      "[190/251] Loss: 2.283531 (2.279414) Accuracy: 0.187500 (0.165249)\n",
      "[200/251] Loss: 2.304958 (2.279576) Accuracy: 0.125000 (0.164335)\n",
      "[210/251] Loss: 2.266936 (2.279699) Accuracy: 0.187500 (0.163803)\n",
      "[220/251] Loss: 2.255558 (2.279463) Accuracy: 0.250000 (0.164451)\n",
      "[230/251] Loss: 2.235001 (2.279395) Accuracy: 0.187500 (0.163690)\n",
      "[240/251] Loss: 2.267905 (2.279527) Accuracy: 0.156250 (0.163122)\n",
      "[250/251] Loss: 2.240845 (2.279356) Accuracy: 0.000000 (0.162475)\n",
      "[0/63] Loss: 2.291622 (2.291622) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294718 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326021 (2.310286) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307385 (2.309515) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297917 (2.308945) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308609 (2.310094) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327874 (2.310590) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 54/100, Train Loss: 2.2794, Train Acc: 0.1625, Val. Loss: 2.3109, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.320100 (2.320100) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.319703 (2.293344) Accuracy: 0.093750 (0.139205)\n",
      "[20/251] Loss: 2.294261 (2.287965) Accuracy: 0.218750 (0.151786)\n",
      "[30/251] Loss: 2.271562 (2.286645) Accuracy: 0.187500 (0.153226)\n",
      "[40/251] Loss: 2.323369 (2.288468) Accuracy: 0.093750 (0.148628)\n",
      "[50/251] Loss: 2.288409 (2.287506) Accuracy: 0.093750 (0.147672)\n",
      "[60/251] Loss: 2.304393 (2.284099) Accuracy: 0.125000 (0.155225)\n",
      "[70/251] Loss: 2.284174 (2.282833) Accuracy: 0.093750 (0.155370)\n",
      "[80/251] Loss: 2.233859 (2.282940) Accuracy: 0.218750 (0.155478)\n",
      "[90/251] Loss: 2.268347 (2.282686) Accuracy: 0.187500 (0.153846)\n",
      "[100/251] Loss: 2.284905 (2.280445) Accuracy: 0.218750 (0.160891)\n",
      "[110/251] Loss: 2.287585 (2.280876) Accuracy: 0.218750 (0.160755)\n",
      "[120/251] Loss: 2.256713 (2.281103) Accuracy: 0.187500 (0.161157)\n",
      "[130/251] Loss: 2.297529 (2.281048) Accuracy: 0.156250 (0.160782)\n",
      "[140/251] Loss: 2.257337 (2.280946) Accuracy: 0.187500 (0.159796)\n",
      "[150/251] Loss: 2.297841 (2.280855) Accuracy: 0.125000 (0.159354)\n",
      "[160/251] Loss: 2.271482 (2.282047) Accuracy: 0.218750 (0.157415)\n",
      "[170/251] Loss: 2.258219 (2.281753) Accuracy: 0.281250 (0.158443)\n",
      "[180/251] Loss: 2.204229 (2.280734) Accuracy: 0.312500 (0.161084)\n",
      "[190/251] Loss: 2.310554 (2.280982) Accuracy: 0.093750 (0.160340)\n",
      "[200/251] Loss: 2.267196 (2.280762) Accuracy: 0.156250 (0.160137)\n",
      "[210/251] Loss: 2.260205 (2.280533) Accuracy: 0.187500 (0.159656)\n",
      "[220/251] Loss: 2.318788 (2.280102) Accuracy: 0.125000 (0.160916)\n",
      "[230/251] Loss: 2.307212 (2.280037) Accuracy: 0.125000 (0.161255)\n",
      "[240/251] Loss: 2.293057 (2.279901) Accuracy: 0.125000 (0.161955)\n",
      "[250/251] Loss: 2.333105 (2.279694) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291603 (2.291603) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294718 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326041 (2.310289) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307393 (2.309515) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297868 (2.308946) Accuracy: 0.093750 (0.102134)\n",
      "[50/63] Loss: 2.308628 (2.310096) Accuracy: 0.125000 (0.098652)\n",
      "[60/63] Loss: 2.327849 (2.310592) Accuracy: 0.093750 (0.096824)\n",
      "Epoch: 55/100, Train Loss: 2.2797, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0974\n",
      "[0/251] Loss: 2.310418 (2.310418) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.307225 (2.279822) Accuracy: 0.156250 (0.178977)\n",
      "[20/251] Loss: 2.229674 (2.278402) Accuracy: 0.250000 (0.177083)\n",
      "[30/251] Loss: 2.263429 (2.274834) Accuracy: 0.218750 (0.185484)\n",
      "[40/251] Loss: 2.294684 (2.278166) Accuracy: 0.125000 (0.175305)\n",
      "[50/251] Loss: 2.266228 (2.280110) Accuracy: 0.156250 (0.170956)\n",
      "[60/251] Loss: 2.331746 (2.280595) Accuracy: 0.093750 (0.167520)\n",
      "[70/251] Loss: 2.263674 (2.279969) Accuracy: 0.125000 (0.165493)\n",
      "[80/251] Loss: 2.307034 (2.279991) Accuracy: 0.093750 (0.165895)\n",
      "[90/251] Loss: 2.279498 (2.281025) Accuracy: 0.218750 (0.161745)\n",
      "[100/251] Loss: 2.292210 (2.280470) Accuracy: 0.218750 (0.162129)\n",
      "[110/251] Loss: 2.323427 (2.279565) Accuracy: 0.062500 (0.163851)\n",
      "[120/251] Loss: 2.313073 (2.279362) Accuracy: 0.062500 (0.162707)\n",
      "[130/251] Loss: 2.305178 (2.279047) Accuracy: 0.125000 (0.164122)\n",
      "[140/251] Loss: 2.282317 (2.278820) Accuracy: 0.187500 (0.163785)\n",
      "[150/251] Loss: 2.281603 (2.279167) Accuracy: 0.156250 (0.163286)\n",
      "[160/251] Loss: 2.298434 (2.279232) Accuracy: 0.062500 (0.163043)\n",
      "[170/251] Loss: 2.264446 (2.279887) Accuracy: 0.218750 (0.161915)\n",
      "[180/251] Loss: 2.249853 (2.278935) Accuracy: 0.187500 (0.164537)\n",
      "[190/251] Loss: 2.333937 (2.279302) Accuracy: 0.031250 (0.163776)\n",
      "[200/251] Loss: 2.307370 (2.278829) Accuracy: 0.125000 (0.164646)\n",
      "[210/251] Loss: 2.266671 (2.278939) Accuracy: 0.218750 (0.165284)\n",
      "[220/251] Loss: 2.280101 (2.279338) Accuracy: 0.125000 (0.163744)\n",
      "[230/251] Loss: 2.294817 (2.279230) Accuracy: 0.125000 (0.163555)\n",
      "[240/251] Loss: 2.285382 (2.279577) Accuracy: 0.156250 (0.162863)\n",
      "[250/251] Loss: 2.231067 (2.279286) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291634 (2.291634) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294685 (2.311466) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326084 (2.310293) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307404 (2.309522) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297941 (2.308954) Accuracy: 0.093750 (0.102896)\n",
      "[50/63] Loss: 2.308602 (2.310101) Accuracy: 0.125000 (0.099265)\n",
      "[60/63] Loss: 2.327853 (2.310595) Accuracy: 0.093750 (0.097336)\n",
      "Epoch: 56/100, Train Loss: 2.2793, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0979\n",
      "[0/251] Loss: 2.248317 (2.248317) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.259442 (2.274923) Accuracy: 0.218750 (0.184659)\n",
      "[20/251] Loss: 2.274440 (2.271210) Accuracy: 0.125000 (0.186012)\n",
      "[30/251] Loss: 2.291985 (2.273631) Accuracy: 0.156250 (0.179435)\n",
      "[40/251] Loss: 2.304887 (2.274970) Accuracy: 0.093750 (0.178354)\n",
      "[50/251] Loss: 2.270791 (2.275868) Accuracy: 0.125000 (0.172181)\n",
      "[60/251] Loss: 2.316520 (2.274540) Accuracy: 0.031250 (0.178791)\n",
      "[70/251] Loss: 2.312480 (2.277350) Accuracy: 0.062500 (0.171215)\n",
      "[80/251] Loss: 2.290869 (2.278125) Accuracy: 0.156250 (0.166667)\n",
      "[90/251] Loss: 2.246699 (2.277741) Accuracy: 0.218750 (0.168613)\n",
      "[100/251] Loss: 2.308630 (2.277738) Accuracy: 0.062500 (0.168317)\n",
      "[110/251] Loss: 2.324654 (2.277912) Accuracy: 0.093750 (0.166385)\n",
      "[120/251] Loss: 2.254039 (2.277804) Accuracy: 0.125000 (0.164514)\n",
      "[130/251] Loss: 2.291509 (2.278528) Accuracy: 0.125000 (0.163406)\n",
      "[140/251] Loss: 2.315089 (2.278737) Accuracy: 0.093750 (0.162899)\n",
      "[150/251] Loss: 2.256008 (2.278169) Accuracy: 0.218750 (0.164321)\n",
      "[160/251] Loss: 2.270608 (2.277926) Accuracy: 0.187500 (0.166537)\n",
      "[170/251] Loss: 2.290835 (2.278865) Accuracy: 0.156250 (0.164839)\n",
      "[180/251] Loss: 2.227796 (2.278679) Accuracy: 0.218750 (0.163847)\n",
      "[190/251] Loss: 2.290753 (2.279155) Accuracy: 0.187500 (0.162958)\n",
      "[200/251] Loss: 2.294368 (2.279566) Accuracy: 0.093750 (0.162158)\n",
      "[210/251] Loss: 2.236631 (2.279155) Accuracy: 0.281250 (0.164248)\n",
      "[220/251] Loss: 2.282630 (2.279031) Accuracy: 0.156250 (0.164734)\n",
      "[230/251] Loss: 2.303599 (2.279310) Accuracy: 0.062500 (0.163826)\n",
      "[240/251] Loss: 2.233549 (2.278965) Accuracy: 0.187500 (0.164289)\n",
      "[250/251] Loss: 2.397047 (2.279910) Accuracy: 0.000000 (0.162475)\n",
      "[0/63] Loss: 2.291626 (2.291626) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294684 (2.311462) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326096 (2.310293) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307390 (2.309521) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297976 (2.308953) Accuracy: 0.093750 (0.102896)\n",
      "[50/63] Loss: 2.308594 (2.310101) Accuracy: 0.125000 (0.099265)\n",
      "[60/63] Loss: 2.327844 (2.310596) Accuracy: 0.093750 (0.097336)\n",
      "Epoch: 57/100, Train Loss: 2.2799, Train Acc: 0.1625, Val. Loss: 2.3109, Val. Acc: 0.0979\n",
      "[0/251] Loss: 2.321031 (2.321031) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.265644 (2.277836) Accuracy: 0.187500 (0.153409)\n",
      "[20/251] Loss: 2.293014 (2.285000) Accuracy: 0.093750 (0.139881)\n",
      "[30/251] Loss: 2.255186 (2.278479) Accuracy: 0.187500 (0.160282)\n",
      "[40/251] Loss: 2.318681 (2.276491) Accuracy: 0.125000 (0.163872)\n",
      "[50/251] Loss: 2.282589 (2.277858) Accuracy: 0.125000 (0.157475)\n",
      "[60/251] Loss: 2.237083 (2.276409) Accuracy: 0.218750 (0.161885)\n",
      "[70/251] Loss: 2.295687 (2.277757) Accuracy: 0.187500 (0.160651)\n",
      "[80/251] Loss: 2.287276 (2.276351) Accuracy: 0.156250 (0.164352)\n",
      "[90/251] Loss: 2.303414 (2.278360) Accuracy: 0.125000 (0.161058)\n",
      "[100/251] Loss: 2.256659 (2.278901) Accuracy: 0.250000 (0.161200)\n",
      "[110/251] Loss: 2.296860 (2.278827) Accuracy: 0.187500 (0.162444)\n",
      "[120/251] Loss: 2.312215 (2.278445) Accuracy: 0.093750 (0.162448)\n",
      "[130/251] Loss: 2.319670 (2.278566) Accuracy: 0.093750 (0.163884)\n",
      "[140/251] Loss: 2.280112 (2.279030) Accuracy: 0.156250 (0.163121)\n",
      "[150/251] Loss: 2.296818 (2.278617) Accuracy: 0.125000 (0.165149)\n",
      "[160/251] Loss: 2.291330 (2.279171) Accuracy: 0.156250 (0.164208)\n",
      "[170/251] Loss: 2.303605 (2.279768) Accuracy: 0.156250 (0.162098)\n",
      "[180/251] Loss: 2.273210 (2.279729) Accuracy: 0.187500 (0.161948)\n",
      "[190/251] Loss: 2.239905 (2.278858) Accuracy: 0.218750 (0.163285)\n",
      "[200/251] Loss: 2.225616 (2.278856) Accuracy: 0.250000 (0.163713)\n",
      "[210/251] Loss: 2.323306 (2.278902) Accuracy: 0.062500 (0.163655)\n",
      "[220/251] Loss: 2.266734 (2.279284) Accuracy: 0.187500 (0.163603)\n",
      "[230/251] Loss: 2.235742 (2.278783) Accuracy: 0.187500 (0.164096)\n",
      "[240/251] Loss: 2.289850 (2.279211) Accuracy: 0.093750 (0.162863)\n",
      "[250/251] Loss: 2.381931 (2.279842) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291631 (2.291631) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294674 (2.311461) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326105 (2.310295) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307377 (2.309522) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297984 (2.308955) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308599 (2.310104) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327846 (2.310598) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 58/100, Train Loss: 2.2798, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.262074 (2.262074) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.317873 (2.302552) Accuracy: 0.062500 (0.116477)\n",
      "[20/251] Loss: 2.314848 (2.290136) Accuracy: 0.062500 (0.136905)\n",
      "[30/251] Loss: 2.256583 (2.284349) Accuracy: 0.281250 (0.155242)\n",
      "[40/251] Loss: 2.237538 (2.282563) Accuracy: 0.218750 (0.158537)\n",
      "[50/251] Loss: 2.329896 (2.283628) Accuracy: 0.062500 (0.155025)\n",
      "[60/251] Loss: 2.257468 (2.281450) Accuracy: 0.218750 (0.160348)\n",
      "[70/251] Loss: 2.258311 (2.281439) Accuracy: 0.187500 (0.159331)\n",
      "[80/251] Loss: 2.264115 (2.280128) Accuracy: 0.187500 (0.162423)\n",
      "[90/251] Loss: 2.257515 (2.279797) Accuracy: 0.218750 (0.165179)\n",
      "[100/251] Loss: 2.324339 (2.281071) Accuracy: 0.062500 (0.162129)\n",
      "[110/251] Loss: 2.242844 (2.281931) Accuracy: 0.281250 (0.161036)\n",
      "[120/251] Loss: 2.265106 (2.282960) Accuracy: 0.218750 (0.158316)\n",
      "[130/251] Loss: 2.236574 (2.282448) Accuracy: 0.250000 (0.158874)\n",
      "[140/251] Loss: 2.284199 (2.282617) Accuracy: 0.125000 (0.159353)\n",
      "[150/251] Loss: 2.318016 (2.282653) Accuracy: 0.062500 (0.159354)\n",
      "[160/251] Loss: 2.238911 (2.281187) Accuracy: 0.281250 (0.161685)\n",
      "[170/251] Loss: 2.252444 (2.280769) Accuracy: 0.218750 (0.161732)\n",
      "[180/251] Loss: 2.244860 (2.280532) Accuracy: 0.187500 (0.160739)\n",
      "[190/251] Loss: 2.267226 (2.280829) Accuracy: 0.187500 (0.160668)\n",
      "[200/251] Loss: 2.265345 (2.280975) Accuracy: 0.156250 (0.160914)\n",
      "[210/251] Loss: 2.309082 (2.280134) Accuracy: 0.125000 (0.161730)\n",
      "[220/251] Loss: 2.330201 (2.279395) Accuracy: 0.093750 (0.162755)\n",
      "[230/251] Loss: 2.258261 (2.279081) Accuracy: 0.250000 (0.163826)\n",
      "[240/251] Loss: 2.294670 (2.279421) Accuracy: 0.187500 (0.164030)\n",
      "[250/251] Loss: 2.369298 (2.279782) Accuracy: 0.000000 (0.162724)\n",
      "[0/63] Loss: 2.291624 (2.291624) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294668 (2.311461) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326114 (2.310297) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307355 (2.309523) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297974 (2.308954) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308595 (2.310105) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327852 (2.310599) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 59/100, Train Loss: 2.2798, Train Acc: 0.1627, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.225988 (2.225988) Accuracy: 0.281250 (0.281250)\n",
      "[10/251] Loss: 2.255086 (2.256107) Accuracy: 0.281250 (0.201705)\n",
      "[20/251] Loss: 2.291681 (2.274991) Accuracy: 0.093750 (0.160714)\n",
      "[30/251] Loss: 2.280326 (2.274797) Accuracy: 0.187500 (0.171371)\n",
      "[40/251] Loss: 2.299982 (2.274601) Accuracy: 0.125000 (0.169207)\n",
      "[50/251] Loss: 2.286770 (2.275460) Accuracy: 0.125000 (0.167892)\n",
      "[60/251] Loss: 2.276333 (2.279029) Accuracy: 0.187500 (0.162398)\n",
      "[70/251] Loss: 2.284724 (2.280314) Accuracy: 0.125000 (0.161532)\n",
      "[80/251] Loss: 2.315486 (2.280043) Accuracy: 0.062500 (0.162809)\n",
      "[90/251] Loss: 2.277917 (2.279783) Accuracy: 0.156250 (0.164492)\n",
      "[100/251] Loss: 2.251666 (2.279758) Accuracy: 0.218750 (0.163366)\n",
      "[110/251] Loss: 2.301368 (2.279599) Accuracy: 0.156250 (0.164414)\n",
      "[120/251] Loss: 2.235119 (2.279363) Accuracy: 0.312500 (0.164256)\n",
      "[130/251] Loss: 2.288131 (2.280001) Accuracy: 0.187500 (0.163645)\n",
      "[140/251] Loss: 2.298802 (2.279314) Accuracy: 0.125000 (0.162899)\n",
      "[150/251] Loss: 2.281774 (2.279922) Accuracy: 0.187500 (0.162666)\n",
      "[160/251] Loss: 2.291363 (2.279831) Accuracy: 0.187500 (0.164402)\n",
      "[170/251] Loss: 2.321964 (2.279347) Accuracy: 0.062500 (0.163560)\n",
      "[180/251] Loss: 2.292047 (2.278743) Accuracy: 0.093750 (0.164192)\n",
      "[190/251] Loss: 2.277431 (2.279380) Accuracy: 0.156250 (0.162631)\n",
      "[200/251] Loss: 2.294493 (2.279125) Accuracy: 0.156250 (0.162935)\n",
      "[210/251] Loss: 2.310617 (2.279804) Accuracy: 0.093750 (0.161434)\n",
      "[220/251] Loss: 2.202688 (2.279899) Accuracy: 0.250000 (0.161340)\n",
      "[230/251] Loss: 2.321191 (2.279866) Accuracy: 0.093750 (0.161526)\n",
      "[240/251] Loss: 2.276675 (2.279690) Accuracy: 0.218750 (0.161955)\n",
      "[250/251] Loss: 2.298246 (2.279497) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291613 (2.291613) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294673 (2.311457) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326125 (2.310298) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307334 (2.309523) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297958 (2.308954) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308586 (2.310105) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327848 (2.310599) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 60/100, Train Loss: 2.2795, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.323147 (2.323147) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.257096 (2.282908) Accuracy: 0.218750 (0.144886)\n",
      "[20/251] Loss: 2.210006 (2.278770) Accuracy: 0.312500 (0.163690)\n",
      "[30/251] Loss: 2.255302 (2.276007) Accuracy: 0.250000 (0.168347)\n",
      "[40/251] Loss: 2.309262 (2.278510) Accuracy: 0.125000 (0.163872)\n",
      "[50/251] Loss: 2.272522 (2.279354) Accuracy: 0.187500 (0.159926)\n",
      "[60/251] Loss: 2.289435 (2.280191) Accuracy: 0.093750 (0.157275)\n",
      "[70/251] Loss: 2.264203 (2.280216) Accuracy: 0.156250 (0.157570)\n",
      "[80/251] Loss: 2.285336 (2.280684) Accuracy: 0.156250 (0.155478)\n",
      "[90/251] Loss: 2.243938 (2.279145) Accuracy: 0.281250 (0.162088)\n",
      "[100/251] Loss: 2.296949 (2.279402) Accuracy: 0.125000 (0.160891)\n",
      "[110/251] Loss: 2.256085 (2.278906) Accuracy: 0.250000 (0.162162)\n",
      "[120/251] Loss: 2.226692 (2.278442) Accuracy: 0.281250 (0.162448)\n",
      "[130/251] Loss: 2.255666 (2.278711) Accuracy: 0.218750 (0.163406)\n",
      "[140/251] Loss: 2.307133 (2.278820) Accuracy: 0.093750 (0.163342)\n",
      "[150/251] Loss: 2.292649 (2.279167) Accuracy: 0.187500 (0.162666)\n",
      "[160/251] Loss: 2.239413 (2.279111) Accuracy: 0.250000 (0.163820)\n",
      "[170/251] Loss: 2.278090 (2.279980) Accuracy: 0.156250 (0.162646)\n",
      "[180/251] Loss: 2.317317 (2.279403) Accuracy: 0.062500 (0.163156)\n",
      "[190/251] Loss: 2.290609 (2.279958) Accuracy: 0.156250 (0.162467)\n",
      "[200/251] Loss: 2.298623 (2.280357) Accuracy: 0.125000 (0.161070)\n",
      "[210/251] Loss: 2.311002 (2.280563) Accuracy: 0.093750 (0.160841)\n",
      "[220/251] Loss: 2.302851 (2.280663) Accuracy: 0.125000 (0.161058)\n",
      "[230/251] Loss: 2.255710 (2.280096) Accuracy: 0.156250 (0.162067)\n",
      "[240/251] Loss: 2.262584 (2.280041) Accuracy: 0.218750 (0.162604)\n",
      "[250/251] Loss: 2.304321 (2.279511) Accuracy: 0.000000 (0.162600)\n",
      "[0/63] Loss: 2.291607 (2.291607) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294680 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326126 (2.310298) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307355 (2.309524) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297948 (2.308952) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308573 (2.310106) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327860 (2.310599) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 61/100, Train Loss: 2.2795, Train Acc: 0.1626, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.300394 (2.300394) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.249362 (2.261399) Accuracy: 0.250000 (0.218750)\n",
      "[20/251] Loss: 2.275817 (2.274241) Accuracy: 0.156250 (0.183036)\n",
      "[30/251] Loss: 2.253271 (2.273647) Accuracy: 0.250000 (0.185484)\n",
      "[40/251] Loss: 2.256475 (2.272283) Accuracy: 0.125000 (0.183689)\n",
      "[50/251] Loss: 2.290593 (2.275215) Accuracy: 0.156250 (0.177083)\n",
      "[60/251] Loss: 2.271131 (2.275926) Accuracy: 0.187500 (0.172131)\n",
      "[70/251] Loss: 2.320749 (2.277501) Accuracy: 0.093750 (0.167254)\n",
      "[80/251] Loss: 2.274190 (2.278410) Accuracy: 0.187500 (0.164738)\n",
      "[90/251] Loss: 2.315717 (2.278371) Accuracy: 0.093750 (0.164492)\n",
      "[100/251] Loss: 2.302186 (2.278152) Accuracy: 0.093750 (0.163985)\n",
      "[110/251] Loss: 2.322307 (2.278432) Accuracy: 0.093750 (0.163851)\n",
      "[120/251] Loss: 2.230185 (2.277906) Accuracy: 0.312500 (0.166839)\n",
      "[130/251] Loss: 2.300797 (2.278189) Accuracy: 0.125000 (0.166269)\n",
      "[140/251] Loss: 2.242589 (2.278844) Accuracy: 0.187500 (0.163342)\n",
      "[150/251] Loss: 2.285669 (2.279175) Accuracy: 0.156250 (0.161838)\n",
      "[160/251] Loss: 2.260288 (2.278162) Accuracy: 0.218750 (0.164402)\n",
      "[170/251] Loss: 2.304038 (2.278444) Accuracy: 0.093750 (0.163560)\n",
      "[180/251] Loss: 2.320547 (2.279098) Accuracy: 0.062500 (0.162465)\n",
      "[190/251] Loss: 2.318589 (2.279268) Accuracy: 0.125000 (0.162631)\n",
      "[200/251] Loss: 2.285428 (2.279284) Accuracy: 0.156250 (0.162313)\n",
      "[210/251] Loss: 2.306014 (2.279557) Accuracy: 0.093750 (0.161434)\n",
      "[220/251] Loss: 2.289078 (2.279357) Accuracy: 0.093750 (0.161906)\n",
      "[230/251] Loss: 2.332973 (2.279200) Accuracy: 0.062500 (0.163285)\n",
      "[240/251] Loss: 2.266879 (2.279067) Accuracy: 0.187500 (0.163122)\n",
      "[250/251] Loss: 1.807902 (2.277586) Accuracy: 1.000000 (0.166210)\n",
      "[0/63] Loss: 2.291626 (2.291626) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294631 (2.311452) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326142 (2.310297) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307343 (2.309524) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297971 (2.308956) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308594 (2.310109) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327855 (2.310601) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 62/100, Train Loss: 2.2776, Train Acc: 0.1662, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.291738 (2.291738) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.308805 (2.278019) Accuracy: 0.156250 (0.164773)\n",
      "[20/251] Loss: 2.276904 (2.276711) Accuracy: 0.218750 (0.169643)\n",
      "[30/251] Loss: 2.289257 (2.278310) Accuracy: 0.156250 (0.174395)\n",
      "[40/251] Loss: 2.258488 (2.276754) Accuracy: 0.218750 (0.173018)\n",
      "[50/251] Loss: 2.278110 (2.278892) Accuracy: 0.156250 (0.166667)\n",
      "[60/251] Loss: 2.313151 (2.277842) Accuracy: 0.156250 (0.172643)\n",
      "[70/251] Loss: 2.290359 (2.276403) Accuracy: 0.187500 (0.175616)\n",
      "[80/251] Loss: 2.274832 (2.277181) Accuracy: 0.125000 (0.170910)\n",
      "[90/251] Loss: 2.286302 (2.276962) Accuracy: 0.125000 (0.171016)\n",
      "[100/251] Loss: 2.216517 (2.275861) Accuracy: 0.218750 (0.172339)\n",
      "[110/251] Loss: 2.276713 (2.276726) Accuracy: 0.187500 (0.170608)\n",
      "[120/251] Loss: 2.277982 (2.276497) Accuracy: 0.125000 (0.168905)\n",
      "[130/251] Loss: 2.353361 (2.277485) Accuracy: 0.031250 (0.167939)\n",
      "[140/251] Loss: 2.234482 (2.276975) Accuracy: 0.343750 (0.169548)\n",
      "[150/251] Loss: 2.295631 (2.277194) Accuracy: 0.031250 (0.167012)\n",
      "[160/251] Loss: 2.229552 (2.277456) Accuracy: 0.250000 (0.166731)\n",
      "[170/251] Loss: 2.270144 (2.277496) Accuracy: 0.156250 (0.166849)\n",
      "[180/251] Loss: 2.330994 (2.277984) Accuracy: 0.062500 (0.165746)\n",
      "[190/251] Loss: 2.298221 (2.278262) Accuracy: 0.156250 (0.164431)\n",
      "[200/251] Loss: 2.292569 (2.277968) Accuracy: 0.156250 (0.165578)\n",
      "[210/251] Loss: 2.287935 (2.278020) Accuracy: 0.156250 (0.165581)\n",
      "[220/251] Loss: 2.307403 (2.278266) Accuracy: 0.062500 (0.165865)\n",
      "[230/251] Loss: 2.284818 (2.279063) Accuracy: 0.187500 (0.164232)\n",
      "[240/251] Loss: 2.284484 (2.279628) Accuracy: 0.125000 (0.162474)\n",
      "[250/251] Loss: 2.408134 (2.279894) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291628 (2.291628) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294628 (2.311453) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326147 (2.310299) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307331 (2.309526) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297985 (2.308958) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308596 (2.310110) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327856 (2.310602) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 63/100, Train Loss: 2.2799, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.217587 (2.217587) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.277491 (2.279743) Accuracy: 0.156250 (0.153409)\n",
      "[20/251] Loss: 2.321088 (2.278491) Accuracy: 0.062500 (0.154762)\n",
      "[30/251] Loss: 2.230970 (2.276245) Accuracy: 0.250000 (0.165323)\n",
      "[40/251] Loss: 2.316857 (2.278781) Accuracy: 0.093750 (0.163110)\n",
      "[50/251] Loss: 2.282032 (2.275672) Accuracy: 0.187500 (0.169730)\n",
      "[60/251] Loss: 2.287272 (2.275053) Accuracy: 0.156250 (0.170082)\n",
      "[70/251] Loss: 2.288811 (2.276509) Accuracy: 0.093750 (0.169014)\n",
      "[80/251] Loss: 2.280830 (2.274583) Accuracy: 0.156250 (0.172454)\n",
      "[90/251] Loss: 2.285465 (2.275476) Accuracy: 0.156250 (0.169643)\n",
      "[100/251] Loss: 2.324623 (2.275917) Accuracy: 0.062500 (0.169245)\n",
      "[110/251] Loss: 2.305411 (2.276395) Accuracy: 0.125000 (0.169764)\n",
      "[120/251] Loss: 2.317200 (2.276877) Accuracy: 0.093750 (0.168905)\n",
      "[130/251] Loss: 2.281711 (2.278003) Accuracy: 0.156250 (0.166508)\n",
      "[140/251] Loss: 2.252686 (2.278587) Accuracy: 0.218750 (0.165337)\n",
      "[150/251] Loss: 2.243295 (2.278617) Accuracy: 0.187500 (0.164528)\n",
      "[160/251] Loss: 2.308539 (2.278590) Accuracy: 0.093750 (0.164402)\n",
      "[170/251] Loss: 2.252081 (2.277909) Accuracy: 0.187500 (0.165753)\n",
      "[180/251] Loss: 2.277500 (2.277147) Accuracy: 0.187500 (0.166609)\n",
      "[190/251] Loss: 2.334060 (2.277876) Accuracy: 0.031250 (0.165903)\n",
      "[200/251] Loss: 2.315103 (2.278530) Accuracy: 0.093750 (0.164646)\n",
      "[210/251] Loss: 2.295396 (2.279010) Accuracy: 0.062500 (0.163507)\n",
      "[220/251] Loss: 2.288824 (2.278887) Accuracy: 0.125000 (0.163886)\n",
      "[230/251] Loss: 2.253765 (2.278889) Accuracy: 0.281250 (0.163826)\n",
      "[240/251] Loss: 2.310772 (2.279103) Accuracy: 0.125000 (0.164030)\n",
      "[250/251] Loss: 2.421002 (2.279939) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291621 (2.291621) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294627 (2.311453) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326147 (2.310300) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307328 (2.309526) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297979 (2.308958) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308599 (2.310111) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327854 (2.310602) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 64/100, Train Loss: 2.2799, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.295746 (2.295746) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.287742 (2.272863) Accuracy: 0.125000 (0.170455)\n",
      "[20/251] Loss: 2.294552 (2.278088) Accuracy: 0.156250 (0.160714)\n",
      "[30/251] Loss: 2.316682 (2.280908) Accuracy: 0.062500 (0.159274)\n",
      "[40/251] Loss: 2.294963 (2.280836) Accuracy: 0.062500 (0.159299)\n",
      "[50/251] Loss: 2.259314 (2.279589) Accuracy: 0.218750 (0.161765)\n",
      "[60/251] Loss: 2.266438 (2.278550) Accuracy: 0.218750 (0.163934)\n",
      "[70/251] Loss: 2.316748 (2.277768) Accuracy: 0.093750 (0.167254)\n",
      "[80/251] Loss: 2.282979 (2.279810) Accuracy: 0.125000 (0.164352)\n",
      "[90/251] Loss: 2.279896 (2.278132) Accuracy: 0.156250 (0.166209)\n",
      "[100/251] Loss: 2.232964 (2.278848) Accuracy: 0.218750 (0.163985)\n",
      "[110/251] Loss: 2.304223 (2.280096) Accuracy: 0.062500 (0.162444)\n",
      "[120/251] Loss: 2.248484 (2.278952) Accuracy: 0.250000 (0.166581)\n",
      "[130/251] Loss: 2.269564 (2.279264) Accuracy: 0.156250 (0.164599)\n",
      "[140/251] Loss: 2.263258 (2.278808) Accuracy: 0.187500 (0.166223)\n",
      "[150/251] Loss: 2.280163 (2.278674) Accuracy: 0.156250 (0.165977)\n",
      "[160/251] Loss: 2.292933 (2.279620) Accuracy: 0.156250 (0.165567)\n",
      "[170/251] Loss: 2.280917 (2.280144) Accuracy: 0.125000 (0.164291)\n",
      "[180/251] Loss: 2.251640 (2.279423) Accuracy: 0.187500 (0.164883)\n",
      "[190/251] Loss: 2.293855 (2.278995) Accuracy: 0.187500 (0.164758)\n",
      "[200/251] Loss: 2.276306 (2.278883) Accuracy: 0.187500 (0.164490)\n",
      "[210/251] Loss: 2.280682 (2.279047) Accuracy: 0.156250 (0.163507)\n",
      "[220/251] Loss: 2.282622 (2.279337) Accuracy: 0.156250 (0.164169)\n",
      "[230/251] Loss: 2.263854 (2.279361) Accuracy: 0.218750 (0.164367)\n",
      "[240/251] Loss: 2.307032 (2.279433) Accuracy: 0.062500 (0.163771)\n",
      "[250/251] Loss: 2.230481 (2.279198) Accuracy: 0.000000 (0.162475)\n",
      "[0/63] Loss: 2.291628 (2.291628) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294634 (2.311456) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326131 (2.310305) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307312 (2.309531) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297994 (2.308962) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308613 (2.310115) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327844 (2.310605) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 65/100, Train Loss: 2.2792, Train Acc: 0.1625, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.306850 (2.306850) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.301811 (2.285053) Accuracy: 0.093750 (0.153409)\n",
      "[20/251] Loss: 2.259503 (2.283908) Accuracy: 0.218750 (0.154762)\n",
      "[30/251] Loss: 2.283374 (2.283551) Accuracy: 0.156250 (0.159274)\n",
      "[40/251] Loss: 2.317400 (2.279089) Accuracy: 0.093750 (0.161585)\n",
      "[50/251] Loss: 2.326278 (2.282524) Accuracy: 0.125000 (0.157475)\n",
      "[60/251] Loss: 2.267840 (2.284828) Accuracy: 0.156250 (0.154201)\n",
      "[70/251] Loss: 2.277769 (2.284524) Accuracy: 0.156250 (0.155810)\n",
      "[80/251] Loss: 2.245509 (2.283453) Accuracy: 0.218750 (0.158565)\n",
      "[90/251] Loss: 2.255230 (2.282968) Accuracy: 0.281250 (0.158997)\n",
      "[100/251] Loss: 2.343507 (2.282391) Accuracy: 0.031250 (0.160891)\n",
      "[110/251] Loss: 2.275511 (2.282291) Accuracy: 0.125000 (0.159628)\n",
      "[120/251] Loss: 2.283935 (2.282351) Accuracy: 0.187500 (0.159091)\n",
      "[130/251] Loss: 2.278381 (2.282814) Accuracy: 0.093750 (0.157204)\n",
      "[140/251] Loss: 2.290303 (2.283021) Accuracy: 0.156250 (0.158245)\n",
      "[150/251] Loss: 2.302094 (2.283489) Accuracy: 0.187500 (0.157285)\n",
      "[160/251] Loss: 2.265000 (2.282559) Accuracy: 0.125000 (0.158579)\n",
      "[170/251] Loss: 2.282456 (2.281295) Accuracy: 0.156250 (0.161915)\n",
      "[180/251] Loss: 2.249835 (2.280898) Accuracy: 0.187500 (0.161257)\n",
      "[190/251] Loss: 2.178671 (2.280179) Accuracy: 0.375000 (0.163449)\n",
      "[200/251] Loss: 2.319308 (2.280195) Accuracy: 0.031250 (0.163246)\n",
      "[210/251] Loss: 2.270631 (2.280359) Accuracy: 0.156250 (0.162174)\n",
      "[220/251] Loss: 2.274910 (2.280263) Accuracy: 0.218750 (0.162330)\n",
      "[230/251] Loss: 2.238758 (2.279764) Accuracy: 0.281250 (0.163420)\n",
      "[240/251] Loss: 2.293420 (2.279226) Accuracy: 0.156250 (0.163771)\n",
      "[250/251] Loss: 2.406355 (2.279870) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291627 (2.291627) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294631 (2.311457) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326133 (2.310306) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307317 (2.309532) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297991 (2.308963) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308614 (2.310116) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327848 (2.310606) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 66/100, Train Loss: 2.2799, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.246359 (2.246359) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.323537 (2.285977) Accuracy: 0.093750 (0.173295)\n",
      "[20/251] Loss: 2.286452 (2.284332) Accuracy: 0.156250 (0.165179)\n",
      "[30/251] Loss: 2.306335 (2.280495) Accuracy: 0.093750 (0.168347)\n",
      "[40/251] Loss: 2.316272 (2.283035) Accuracy: 0.062500 (0.163110)\n",
      "[50/251] Loss: 2.308560 (2.280156) Accuracy: 0.062500 (0.167279)\n",
      "[60/251] Loss: 2.253352 (2.280675) Accuracy: 0.187500 (0.164447)\n",
      "[70/251] Loss: 2.263187 (2.281931) Accuracy: 0.156250 (0.159331)\n",
      "[80/251] Loss: 2.269498 (2.279642) Accuracy: 0.156250 (0.162809)\n",
      "[90/251] Loss: 2.247652 (2.279842) Accuracy: 0.218750 (0.162431)\n",
      "[100/251] Loss: 2.290633 (2.277708) Accuracy: 0.187500 (0.166151)\n",
      "[110/251] Loss: 2.252236 (2.276855) Accuracy: 0.187500 (0.166948)\n",
      "[120/251] Loss: 2.240684 (2.276458) Accuracy: 0.250000 (0.167614)\n",
      "[130/251] Loss: 2.316200 (2.277670) Accuracy: 0.062500 (0.166031)\n",
      "[140/251] Loss: 2.338063 (2.277949) Accuracy: 0.000000 (0.164894)\n",
      "[150/251] Loss: 2.263776 (2.278479) Accuracy: 0.156250 (0.163907)\n",
      "[160/251] Loss: 2.260324 (2.278539) Accuracy: 0.187500 (0.163238)\n",
      "[170/251] Loss: 2.272043 (2.279077) Accuracy: 0.218750 (0.162281)\n",
      "[180/251] Loss: 2.282202 (2.279194) Accuracy: 0.156250 (0.161430)\n",
      "[190/251] Loss: 2.262163 (2.279516) Accuracy: 0.187500 (0.161322)\n",
      "[200/251] Loss: 2.269140 (2.279684) Accuracy: 0.187500 (0.161692)\n",
      "[210/251] Loss: 2.237479 (2.279084) Accuracy: 0.281250 (0.162915)\n",
      "[220/251] Loss: 2.210268 (2.278926) Accuracy: 0.343750 (0.163603)\n",
      "[230/251] Loss: 2.248198 (2.279519) Accuracy: 0.187500 (0.162202)\n",
      "[240/251] Loss: 2.274799 (2.279355) Accuracy: 0.187500 (0.163122)\n",
      "[250/251] Loss: 2.381906 (2.279772) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291629 (2.291629) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294628 (2.311457) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326137 (2.310307) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307312 (2.309533) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297993 (2.308963) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308615 (2.310117) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327851 (2.310607) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 67/100, Train Loss: 2.2798, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.311876 (2.311876) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.255992 (2.282096) Accuracy: 0.218750 (0.173295)\n",
      "[20/251] Loss: 2.270612 (2.285466) Accuracy: 0.156250 (0.165179)\n",
      "[30/251] Loss: 2.268100 (2.280174) Accuracy: 0.218750 (0.170363)\n",
      "[40/251] Loss: 2.290588 (2.280557) Accuracy: 0.156250 (0.163872)\n",
      "[50/251] Loss: 2.246754 (2.280504) Accuracy: 0.312500 (0.163603)\n",
      "[60/251] Loss: 2.270648 (2.281024) Accuracy: 0.187500 (0.162910)\n",
      "[70/251] Loss: 2.279035 (2.283630) Accuracy: 0.125000 (0.157130)\n",
      "[80/251] Loss: 2.324872 (2.284415) Accuracy: 0.031250 (0.155093)\n",
      "[90/251] Loss: 2.283548 (2.285404) Accuracy: 0.218750 (0.156250)\n",
      "[100/251] Loss: 2.290461 (2.283744) Accuracy: 0.093750 (0.157797)\n",
      "[110/251] Loss: 2.219792 (2.282910) Accuracy: 0.218750 (0.158221)\n",
      "[120/251] Loss: 2.304802 (2.282824) Accuracy: 0.125000 (0.160382)\n",
      "[130/251] Loss: 2.271956 (2.281669) Accuracy: 0.218750 (0.162452)\n",
      "[140/251] Loss: 2.320789 (2.282409) Accuracy: 0.093750 (0.161126)\n",
      "[150/251] Loss: 2.278832 (2.282048) Accuracy: 0.156250 (0.161631)\n",
      "[160/251] Loss: 2.266895 (2.281677) Accuracy: 0.156250 (0.162073)\n",
      "[170/251] Loss: 2.286289 (2.281779) Accuracy: 0.156250 (0.160819)\n",
      "[180/251] Loss: 2.313118 (2.281892) Accuracy: 0.093750 (0.160394)\n",
      "[190/251] Loss: 2.249454 (2.281247) Accuracy: 0.218750 (0.160504)\n",
      "[200/251] Loss: 2.280458 (2.280832) Accuracy: 0.125000 (0.160759)\n",
      "[210/251] Loss: 2.272671 (2.280445) Accuracy: 0.125000 (0.160989)\n",
      "[220/251] Loss: 2.261262 (2.280122) Accuracy: 0.187500 (0.161340)\n",
      "[230/251] Loss: 2.293645 (2.279633) Accuracy: 0.093750 (0.162879)\n",
      "[240/251] Loss: 2.231755 (2.278912) Accuracy: 0.250000 (0.163252)\n",
      "[250/251] Loss: 2.394351 (2.279816) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291627 (2.291627) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294626 (2.311457) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326142 (2.310308) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307311 (2.309533) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.297992 (2.308964) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308615 (2.310117) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327851 (2.310607) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 68/100, Train Loss: 2.2798, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.208929 (2.208929) Accuracy: 0.312500 (0.312500)\n",
      "[10/251] Loss: 2.273519 (2.266035) Accuracy: 0.156250 (0.184659)\n",
      "[20/251] Loss: 2.306867 (2.269951) Accuracy: 0.187500 (0.178571)\n",
      "[30/251] Loss: 2.295628 (2.272342) Accuracy: 0.093750 (0.171371)\n",
      "[40/251] Loss: 2.275151 (2.275870) Accuracy: 0.125000 (0.163872)\n",
      "[50/251] Loss: 2.242009 (2.276553) Accuracy: 0.218750 (0.163603)\n",
      "[60/251] Loss: 2.286504 (2.277586) Accuracy: 0.125000 (0.161885)\n",
      "[70/251] Loss: 2.291897 (2.279150) Accuracy: 0.125000 (0.158451)\n",
      "[80/251] Loss: 2.270555 (2.278778) Accuracy: 0.156250 (0.158951)\n",
      "[90/251] Loss: 2.286834 (2.278538) Accuracy: 0.156250 (0.159684)\n",
      "[100/251] Loss: 2.265603 (2.278419) Accuracy: 0.187500 (0.162129)\n",
      "[110/251] Loss: 2.309567 (2.278159) Accuracy: 0.125000 (0.162725)\n",
      "[120/251] Loss: 2.335646 (2.279400) Accuracy: 0.093750 (0.160640)\n",
      "[130/251] Loss: 2.266213 (2.279804) Accuracy: 0.250000 (0.159590)\n",
      "[140/251] Loss: 2.227224 (2.280737) Accuracy: 0.250000 (0.156915)\n",
      "[150/251] Loss: 2.261268 (2.280182) Accuracy: 0.218750 (0.156871)\n",
      "[160/251] Loss: 2.288626 (2.280593) Accuracy: 0.125000 (0.156444)\n",
      "[170/251] Loss: 2.286494 (2.279960) Accuracy: 0.187500 (0.158443)\n",
      "[180/251] Loss: 2.278145 (2.279383) Accuracy: 0.218750 (0.160221)\n",
      "[190/251] Loss: 2.297501 (2.279445) Accuracy: 0.062500 (0.159686)\n",
      "[200/251] Loss: 2.265777 (2.279766) Accuracy: 0.218750 (0.160292)\n",
      "[210/251] Loss: 2.295047 (2.280232) Accuracy: 0.156250 (0.159508)\n",
      "[220/251] Loss: 2.210476 (2.279403) Accuracy: 0.312500 (0.161906)\n",
      "[230/251] Loss: 2.280528 (2.278705) Accuracy: 0.125000 (0.163555)\n",
      "[240/251] Loss: 2.277927 (2.279607) Accuracy: 0.187500 (0.162215)\n",
      "[250/251] Loss: 1.901703 (2.277910) Accuracy: 1.000000 (0.166086)\n",
      "[0/63] Loss: 2.291638 (2.291638) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294608 (2.311458) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326148 (2.310309) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307313 (2.309535) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298007 (2.308967) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308622 (2.310119) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327870 (2.310609) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 69/100, Train Loss: 2.2779, Train Acc: 0.1661, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.232808 (2.232808) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.297765 (2.260774) Accuracy: 0.156250 (0.207386)\n",
      "[20/251] Loss: 2.283075 (2.271838) Accuracy: 0.187500 (0.186012)\n",
      "[30/251] Loss: 2.333479 (2.273924) Accuracy: 0.062500 (0.174395)\n",
      "[40/251] Loss: 2.318402 (2.274257) Accuracy: 0.000000 (0.175305)\n",
      "[50/251] Loss: 2.310793 (2.276061) Accuracy: 0.031250 (0.168505)\n",
      "[60/251] Loss: 2.279993 (2.276023) Accuracy: 0.125000 (0.169057)\n",
      "[70/251] Loss: 2.241247 (2.277032) Accuracy: 0.156250 (0.165933)\n",
      "[80/251] Loss: 2.291887 (2.277201) Accuracy: 0.156250 (0.167824)\n",
      "[90/251] Loss: 2.314831 (2.278282) Accuracy: 0.125000 (0.166209)\n",
      "[100/251] Loss: 2.278381 (2.278657) Accuracy: 0.125000 (0.163676)\n",
      "[110/251] Loss: 2.286593 (2.279047) Accuracy: 0.093750 (0.162162)\n",
      "[120/251] Loss: 2.252294 (2.278851) Accuracy: 0.281250 (0.161674)\n",
      "[130/251] Loss: 2.274501 (2.279570) Accuracy: 0.125000 (0.160305)\n",
      "[140/251] Loss: 2.231368 (2.278928) Accuracy: 0.375000 (0.163342)\n",
      "[150/251] Loss: 2.261979 (2.278430) Accuracy: 0.218750 (0.164942)\n",
      "[160/251] Loss: 2.278112 (2.278745) Accuracy: 0.218750 (0.165567)\n",
      "[170/251] Loss: 2.242989 (2.278490) Accuracy: 0.250000 (0.165936)\n",
      "[180/251] Loss: 2.277725 (2.279663) Accuracy: 0.125000 (0.163674)\n",
      "[190/251] Loss: 2.256598 (2.279380) Accuracy: 0.250000 (0.164921)\n",
      "[200/251] Loss: 2.240034 (2.278700) Accuracy: 0.156250 (0.165267)\n",
      "[210/251] Loss: 2.263041 (2.278682) Accuracy: 0.156250 (0.164840)\n",
      "[220/251] Loss: 2.231655 (2.278317) Accuracy: 0.218750 (0.165724)\n",
      "[230/251] Loss: 2.238746 (2.278561) Accuracy: 0.156250 (0.164908)\n",
      "[240/251] Loss: 2.269802 (2.278998) Accuracy: 0.156250 (0.163771)\n",
      "[250/251] Loss: 2.082138 (2.278602) Accuracy: 1.000000 (0.165961)\n",
      "[0/63] Loss: 2.291646 (2.291646) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294609 (2.311458) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326149 (2.310311) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307300 (2.309536) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298018 (2.308967) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308620 (2.310120) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327867 (2.310610) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 70/100, Train Loss: 2.2786, Train Acc: 0.1660, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.271568 (2.271568) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.270998 (2.275058) Accuracy: 0.187500 (0.170455)\n",
      "[20/251] Loss: 2.261992 (2.279490) Accuracy: 0.156250 (0.153274)\n",
      "[30/251] Loss: 2.268201 (2.273576) Accuracy: 0.156250 (0.161290)\n",
      "[40/251] Loss: 2.264362 (2.277078) Accuracy: 0.125000 (0.157774)\n",
      "[50/251] Loss: 2.283065 (2.279276) Accuracy: 0.218750 (0.157475)\n",
      "[60/251] Loss: 2.261622 (2.279639) Accuracy: 0.156250 (0.156762)\n",
      "[70/251] Loss: 2.307252 (2.280961) Accuracy: 0.062500 (0.153169)\n",
      "[80/251] Loss: 2.266895 (2.281554) Accuracy: 0.187500 (0.151235)\n",
      "[90/251] Loss: 2.310416 (2.280595) Accuracy: 0.125000 (0.153159)\n",
      "[100/251] Loss: 2.296039 (2.281698) Accuracy: 0.187500 (0.152537)\n",
      "[110/251] Loss: 2.309405 (2.283179) Accuracy: 0.125000 (0.149775)\n",
      "[120/251] Loss: 2.295254 (2.282386) Accuracy: 0.156250 (0.152118)\n",
      "[130/251] Loss: 2.262665 (2.282340) Accuracy: 0.187500 (0.153149)\n",
      "[140/251] Loss: 2.273689 (2.280838) Accuracy: 0.218750 (0.156915)\n",
      "[150/251] Loss: 2.307760 (2.280526) Accuracy: 0.093750 (0.158320)\n",
      "[160/251] Loss: 2.234473 (2.279871) Accuracy: 0.218750 (0.159938)\n",
      "[170/251] Loss: 2.226661 (2.279694) Accuracy: 0.187500 (0.159174)\n",
      "[180/251] Loss: 2.342361 (2.280305) Accuracy: 0.031250 (0.159358)\n",
      "[190/251] Loss: 2.311414 (2.279478) Accuracy: 0.093750 (0.163122)\n",
      "[200/251] Loss: 2.252942 (2.279784) Accuracy: 0.218750 (0.161692)\n",
      "[210/251] Loss: 2.307713 (2.280707) Accuracy: 0.125000 (0.161137)\n",
      "[220/251] Loss: 2.290931 (2.279498) Accuracy: 0.156250 (0.163886)\n",
      "[230/251] Loss: 2.246880 (2.279498) Accuracy: 0.281250 (0.163285)\n",
      "[240/251] Loss: 2.266849 (2.279547) Accuracy: 0.093750 (0.162863)\n",
      "[250/251] Loss: 2.420380 (2.279905) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291650 (2.291650) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294605 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326155 (2.310312) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307299 (2.309536) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298025 (2.308968) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308620 (2.310121) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327872 (2.310611) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 71/100, Train Loss: 2.2799, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.301612 (2.301612) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.254201 (2.279051) Accuracy: 0.187500 (0.161932)\n",
      "[20/251] Loss: 2.273269 (2.279666) Accuracy: 0.156250 (0.168155)\n",
      "[30/251] Loss: 2.263135 (2.279132) Accuracy: 0.187500 (0.171371)\n",
      "[40/251] Loss: 2.291441 (2.279294) Accuracy: 0.093750 (0.166921)\n",
      "[50/251] Loss: 2.282817 (2.276677) Accuracy: 0.125000 (0.166054)\n",
      "[60/251] Loss: 2.301769 (2.278277) Accuracy: 0.125000 (0.164447)\n",
      "[70/251] Loss: 2.234491 (2.276653) Accuracy: 0.218750 (0.167694)\n",
      "[80/251] Loss: 2.268934 (2.275097) Accuracy: 0.156250 (0.170525)\n",
      "[90/251] Loss: 2.236084 (2.275574) Accuracy: 0.250000 (0.168613)\n",
      "[100/251] Loss: 2.257524 (2.274953) Accuracy: 0.156250 (0.167698)\n",
      "[110/251] Loss: 2.291708 (2.275731) Accuracy: 0.156250 (0.166385)\n",
      "[120/251] Loss: 2.285500 (2.276402) Accuracy: 0.125000 (0.164773)\n",
      "[130/251] Loss: 2.317045 (2.276212) Accuracy: 0.125000 (0.166269)\n",
      "[140/251] Loss: 2.291326 (2.276129) Accuracy: 0.125000 (0.166888)\n",
      "[150/251] Loss: 2.301788 (2.277082) Accuracy: 0.062500 (0.165977)\n",
      "[160/251] Loss: 2.282892 (2.277577) Accuracy: 0.093750 (0.164208)\n",
      "[170/251] Loss: 2.316357 (2.278326) Accuracy: 0.062500 (0.163743)\n",
      "[180/251] Loss: 2.307362 (2.278718) Accuracy: 0.125000 (0.162293)\n",
      "[190/251] Loss: 2.247060 (2.278674) Accuracy: 0.218750 (0.161976)\n",
      "[200/251] Loss: 2.296054 (2.278421) Accuracy: 0.125000 (0.162780)\n",
      "[210/251] Loss: 2.263300 (2.278657) Accuracy: 0.187500 (0.162470)\n",
      "[220/251] Loss: 2.261019 (2.279461) Accuracy: 0.218750 (0.160492)\n",
      "[230/251] Loss: 2.312894 (2.279769) Accuracy: 0.093750 (0.160038)\n",
      "[240/251] Loss: 2.257913 (2.279581) Accuracy: 0.281250 (0.161826)\n",
      "[250/251] Loss: 2.301613 (2.279444) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291645 (2.291645) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294606 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326160 (2.310312) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307308 (2.309535) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298020 (2.308967) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308628 (2.310121) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327871 (2.310611) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 72/100, Train Loss: 2.2794, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.260817 (2.260817) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.316336 (2.280954) Accuracy: 0.125000 (0.181818)\n",
      "[20/251] Loss: 2.263604 (2.275110) Accuracy: 0.156250 (0.174107)\n",
      "[30/251] Loss: 2.289504 (2.274194) Accuracy: 0.125000 (0.179435)\n",
      "[40/251] Loss: 2.223860 (2.277176) Accuracy: 0.187500 (0.168445)\n",
      "[50/251] Loss: 2.267385 (2.276140) Accuracy: 0.125000 (0.167279)\n",
      "[60/251] Loss: 2.274334 (2.277822) Accuracy: 0.218750 (0.167008)\n",
      "[70/251] Loss: 2.295015 (2.278885) Accuracy: 0.125000 (0.165053)\n",
      "[80/251] Loss: 2.269561 (2.277635) Accuracy: 0.125000 (0.167438)\n",
      "[90/251] Loss: 2.256365 (2.277323) Accuracy: 0.218750 (0.168613)\n",
      "[100/251] Loss: 2.268525 (2.276385) Accuracy: 0.187500 (0.170483)\n",
      "[110/251] Loss: 2.293205 (2.276430) Accuracy: 0.187500 (0.170045)\n",
      "[120/251] Loss: 2.278486 (2.275869) Accuracy: 0.187500 (0.170455)\n",
      "[130/251] Loss: 2.292920 (2.275306) Accuracy: 0.156250 (0.170324)\n",
      "[140/251] Loss: 2.338309 (2.277421) Accuracy: 0.000000 (0.165115)\n",
      "[150/251] Loss: 2.313455 (2.277065) Accuracy: 0.156250 (0.164942)\n",
      "[160/251] Loss: 2.270147 (2.277639) Accuracy: 0.156250 (0.163043)\n",
      "[170/251] Loss: 2.300056 (2.278151) Accuracy: 0.156250 (0.161367)\n",
      "[180/251] Loss: 2.318762 (2.277364) Accuracy: 0.093750 (0.163329)\n",
      "[190/251] Loss: 2.302540 (2.278587) Accuracy: 0.125000 (0.161486)\n",
      "[200/251] Loss: 2.263388 (2.278425) Accuracy: 0.218750 (0.162313)\n",
      "[210/251] Loss: 2.291457 (2.278213) Accuracy: 0.125000 (0.163655)\n",
      "[220/251] Loss: 2.282997 (2.278200) Accuracy: 0.125000 (0.164169)\n",
      "[230/251] Loss: 2.312510 (2.278348) Accuracy: 0.093750 (0.163690)\n",
      "[240/251] Loss: 2.317960 (2.278777) Accuracy: 0.062500 (0.163771)\n",
      "[250/251] Loss: 2.404303 (2.279838) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291643 (2.291643) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294608 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326160 (2.310312) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307306 (2.309535) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298016 (2.308967) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308629 (2.310121) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327868 (2.310611) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 73/100, Train Loss: 2.2798, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.296085 (2.296085) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.275728 (2.273077) Accuracy: 0.156250 (0.161932)\n",
      "[20/251] Loss: 2.299140 (2.272607) Accuracy: 0.093750 (0.166667)\n",
      "[30/251] Loss: 2.278423 (2.273587) Accuracy: 0.156250 (0.172379)\n",
      "[40/251] Loss: 2.279339 (2.276384) Accuracy: 0.156250 (0.170732)\n",
      "[50/251] Loss: 2.281866 (2.278104) Accuracy: 0.187500 (0.169118)\n",
      "[60/251] Loss: 2.244428 (2.278241) Accuracy: 0.218750 (0.165984)\n",
      "[70/251] Loss: 2.275612 (2.278909) Accuracy: 0.218750 (0.165493)\n",
      "[80/251] Loss: 2.304208 (2.278656) Accuracy: 0.093750 (0.165509)\n",
      "[90/251] Loss: 2.276290 (2.278067) Accuracy: 0.218750 (0.166209)\n",
      "[100/251] Loss: 2.271116 (2.277255) Accuracy: 0.250000 (0.167698)\n",
      "[110/251] Loss: 2.211190 (2.276236) Accuracy: 0.281250 (0.170608)\n",
      "[120/251] Loss: 2.254166 (2.277713) Accuracy: 0.187500 (0.167872)\n",
      "[130/251] Loss: 2.277534 (2.278562) Accuracy: 0.125000 (0.166985)\n",
      "[140/251] Loss: 2.288343 (2.279472) Accuracy: 0.156250 (0.165559)\n",
      "[150/251] Loss: 2.283041 (2.279354) Accuracy: 0.125000 (0.165563)\n",
      "[160/251] Loss: 2.218968 (2.279020) Accuracy: 0.343750 (0.166537)\n",
      "[170/251] Loss: 2.298189 (2.278806) Accuracy: 0.093750 (0.166484)\n",
      "[180/251] Loss: 2.225473 (2.279161) Accuracy: 0.281250 (0.165401)\n",
      "[190/251] Loss: 2.228637 (2.279166) Accuracy: 0.218750 (0.164594)\n",
      "[200/251] Loss: 2.267347 (2.278794) Accuracy: 0.156250 (0.166045)\n",
      "[210/251] Loss: 2.250730 (2.278323) Accuracy: 0.218750 (0.166321)\n",
      "[220/251] Loss: 2.244049 (2.278904) Accuracy: 0.218750 (0.164451)\n",
      "[230/251] Loss: 2.269725 (2.278984) Accuracy: 0.156250 (0.163690)\n",
      "[240/251] Loss: 2.243753 (2.279381) Accuracy: 0.218750 (0.162993)\n",
      "[250/251] Loss: 2.388776 (2.279776) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291641 (2.291641) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294607 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326164 (2.310312) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307307 (2.309536) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298016 (2.308967) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308629 (2.310122) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327870 (2.310611) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 74/100, Train Loss: 2.2798, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.322520 (2.322520) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.332867 (2.295353) Accuracy: 0.062500 (0.113636)\n",
      "[20/251] Loss: 2.277327 (2.286091) Accuracy: 0.218750 (0.136905)\n",
      "[30/251] Loss: 2.289741 (2.281920) Accuracy: 0.156250 (0.150202)\n",
      "[40/251] Loss: 2.309446 (2.280697) Accuracy: 0.156250 (0.160823)\n",
      "[50/251] Loss: 2.298399 (2.281448) Accuracy: 0.093750 (0.157475)\n",
      "[60/251] Loss: 2.329313 (2.281560) Accuracy: 0.062500 (0.158811)\n",
      "[70/251] Loss: 2.303519 (2.282305) Accuracy: 0.093750 (0.157570)\n",
      "[80/251] Loss: 2.286371 (2.279795) Accuracy: 0.156250 (0.163194)\n",
      "[90/251] Loss: 2.297899 (2.280165) Accuracy: 0.125000 (0.162088)\n",
      "[100/251] Loss: 2.258253 (2.280811) Accuracy: 0.281250 (0.160272)\n",
      "[110/251] Loss: 2.265763 (2.280943) Accuracy: 0.187500 (0.161036)\n",
      "[120/251] Loss: 2.267036 (2.281115) Accuracy: 0.218750 (0.163223)\n",
      "[130/251] Loss: 2.278483 (2.282373) Accuracy: 0.156250 (0.158874)\n",
      "[140/251] Loss: 2.280202 (2.281384) Accuracy: 0.156250 (0.161569)\n",
      "[150/251] Loss: 2.295180 (2.280894) Accuracy: 0.093750 (0.161424)\n",
      "[160/251] Loss: 2.304256 (2.281102) Accuracy: 0.125000 (0.160520)\n",
      "[170/251] Loss: 2.241544 (2.281541) Accuracy: 0.250000 (0.158443)\n",
      "[180/251] Loss: 2.301609 (2.281719) Accuracy: 0.062500 (0.157459)\n",
      "[190/251] Loss: 2.258758 (2.281208) Accuracy: 0.218750 (0.158377)\n",
      "[200/251] Loss: 2.325097 (2.280852) Accuracy: 0.031250 (0.159981)\n",
      "[210/251] Loss: 2.219175 (2.280780) Accuracy: 0.250000 (0.158916)\n",
      "[220/251] Loss: 2.273414 (2.280246) Accuracy: 0.156250 (0.160209)\n",
      "[230/251] Loss: 2.229620 (2.279809) Accuracy: 0.218750 (0.160850)\n",
      "[240/251] Loss: 2.271060 (2.279460) Accuracy: 0.218750 (0.161696)\n",
      "[250/251] Loss: 2.299628 (2.279430) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291639 (2.291639) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294605 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326165 (2.310313) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307308 (2.309537) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298018 (2.308968) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308629 (2.310123) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327871 (2.310612) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 75/100, Train Loss: 2.2794, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.306931 (2.306931) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.223019 (2.283882) Accuracy: 0.312500 (0.178977)\n",
      "[20/251] Loss: 2.334624 (2.276858) Accuracy: 0.031250 (0.180060)\n",
      "[30/251] Loss: 2.293683 (2.277107) Accuracy: 0.125000 (0.174395)\n",
      "[40/251] Loss: 2.314744 (2.275422) Accuracy: 0.156250 (0.172256)\n",
      "[50/251] Loss: 2.320997 (2.278238) Accuracy: 0.031250 (0.165441)\n",
      "[60/251] Loss: 2.283839 (2.277086) Accuracy: 0.187500 (0.167008)\n",
      "[70/251] Loss: 2.259802 (2.277317) Accuracy: 0.187500 (0.166373)\n",
      "[80/251] Loss: 2.269661 (2.276923) Accuracy: 0.187500 (0.167052)\n",
      "[90/251] Loss: 2.242789 (2.276053) Accuracy: 0.250000 (0.169299)\n",
      "[100/251] Loss: 2.313615 (2.276300) Accuracy: 0.093750 (0.169245)\n",
      "[110/251] Loss: 2.259281 (2.276574) Accuracy: 0.156250 (0.168919)\n",
      "[120/251] Loss: 2.262710 (2.277225) Accuracy: 0.250000 (0.168130)\n",
      "[130/251] Loss: 2.234750 (2.277317) Accuracy: 0.250000 (0.167939)\n",
      "[140/251] Loss: 2.344939 (2.278025) Accuracy: 0.062500 (0.165337)\n",
      "[150/251] Loss: 2.304228 (2.278388) Accuracy: 0.125000 (0.164735)\n",
      "[160/251] Loss: 2.301460 (2.278592) Accuracy: 0.093750 (0.164208)\n",
      "[170/251] Loss: 2.303145 (2.278560) Accuracy: 0.125000 (0.163925)\n",
      "[180/251] Loss: 2.253338 (2.277787) Accuracy: 0.218750 (0.165919)\n",
      "[190/251] Loss: 2.262685 (2.277788) Accuracy: 0.187500 (0.165412)\n",
      "[200/251] Loss: 2.294559 (2.278581) Accuracy: 0.156250 (0.164335)\n",
      "[210/251] Loss: 2.283096 (2.279029) Accuracy: 0.187500 (0.163655)\n",
      "[220/251] Loss: 2.262207 (2.279292) Accuracy: 0.187500 (0.162613)\n",
      "[230/251] Loss: 2.288590 (2.279506) Accuracy: 0.125000 (0.162473)\n",
      "[240/251] Loss: 2.295065 (2.279685) Accuracy: 0.125000 (0.162474)\n",
      "[250/251] Loss: 2.238372 (2.279191) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291637 (2.291637) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294607 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326167 (2.310314) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307308 (2.309538) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298021 (2.308969) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308627 (2.310123) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327872 (2.310612) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 76/100, Train Loss: 2.2792, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.275383 (2.275383) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.287366 (2.265919) Accuracy: 0.156250 (0.187500)\n",
      "[20/251] Loss: 2.306237 (2.271372) Accuracy: 0.093750 (0.169643)\n",
      "[30/251] Loss: 2.297045 (2.279846) Accuracy: 0.156250 (0.159274)\n",
      "[40/251] Loss: 2.295790 (2.280266) Accuracy: 0.187500 (0.166159)\n",
      "[50/251] Loss: 2.312589 (2.282601) Accuracy: 0.093750 (0.165441)\n",
      "[60/251] Loss: 2.269978 (2.283185) Accuracy: 0.125000 (0.160861)\n",
      "[70/251] Loss: 2.256984 (2.279710) Accuracy: 0.156250 (0.164173)\n",
      "[80/251] Loss: 2.272640 (2.280240) Accuracy: 0.218750 (0.166667)\n",
      "[90/251] Loss: 2.240951 (2.280282) Accuracy: 0.156250 (0.165179)\n",
      "[100/251] Loss: 2.260697 (2.278519) Accuracy: 0.156250 (0.168007)\n",
      "[110/251] Loss: 2.274148 (2.279247) Accuracy: 0.156250 (0.167511)\n",
      "[120/251] Loss: 2.324206 (2.279596) Accuracy: 0.093750 (0.167097)\n",
      "[130/251] Loss: 2.237990 (2.279303) Accuracy: 0.250000 (0.167462)\n",
      "[140/251] Loss: 2.252287 (2.279940) Accuracy: 0.250000 (0.165780)\n",
      "[150/251] Loss: 2.262093 (2.279562) Accuracy: 0.187500 (0.165356)\n",
      "[160/251] Loss: 2.311599 (2.280148) Accuracy: 0.093750 (0.164208)\n",
      "[170/251] Loss: 2.267768 (2.280029) Accuracy: 0.218750 (0.163925)\n",
      "[180/251] Loss: 2.301020 (2.280373) Accuracy: 0.062500 (0.162638)\n",
      "[190/251] Loss: 2.240284 (2.280109) Accuracy: 0.250000 (0.163122)\n",
      "[200/251] Loss: 2.336472 (2.279473) Accuracy: 0.031250 (0.163713)\n",
      "[210/251] Loss: 2.273763 (2.279905) Accuracy: 0.125000 (0.162915)\n",
      "[220/251] Loss: 2.292406 (2.280090) Accuracy: 0.125000 (0.162472)\n",
      "[230/251] Loss: 2.337043 (2.279770) Accuracy: 0.062500 (0.162744)\n",
      "[240/251] Loss: 2.341801 (2.279802) Accuracy: 0.000000 (0.162733)\n",
      "[250/251] Loss: 2.314335 (2.279482) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291635 (2.291635) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294605 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326171 (2.310314) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307308 (2.309538) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298023 (2.308969) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308627 (2.310123) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327871 (2.310612) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 77/100, Train Loss: 2.2795, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.315772 (2.315772) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.275450 (2.292354) Accuracy: 0.156250 (0.130682)\n",
      "[20/251] Loss: 2.241182 (2.285388) Accuracy: 0.218750 (0.154762)\n",
      "[30/251] Loss: 2.291165 (2.287663) Accuracy: 0.125000 (0.144153)\n",
      "[40/251] Loss: 2.268124 (2.285460) Accuracy: 0.156250 (0.146341)\n",
      "[50/251] Loss: 2.279199 (2.281505) Accuracy: 0.125000 (0.153799)\n",
      "[60/251] Loss: 2.286944 (2.280018) Accuracy: 0.156250 (0.159324)\n",
      "[70/251] Loss: 2.222758 (2.278696) Accuracy: 0.281250 (0.161972)\n",
      "[80/251] Loss: 2.303082 (2.280376) Accuracy: 0.125000 (0.160880)\n",
      "[90/251] Loss: 2.312113 (2.280946) Accuracy: 0.125000 (0.161745)\n",
      "[100/251] Loss: 2.285550 (2.281070) Accuracy: 0.156250 (0.163057)\n",
      "[110/251] Loss: 2.272430 (2.281905) Accuracy: 0.218750 (0.162725)\n",
      "[120/251] Loss: 2.255617 (2.282180) Accuracy: 0.218750 (0.162190)\n",
      "[130/251] Loss: 2.303793 (2.282315) Accuracy: 0.125000 (0.162691)\n",
      "[140/251] Loss: 2.300506 (2.281043) Accuracy: 0.093750 (0.164229)\n",
      "[150/251] Loss: 2.333688 (2.280261) Accuracy: 0.031250 (0.165149)\n",
      "[160/251] Loss: 2.272051 (2.280115) Accuracy: 0.187500 (0.165179)\n",
      "[170/251] Loss: 2.321556 (2.280236) Accuracy: 0.062500 (0.165570)\n",
      "[180/251] Loss: 2.286845 (2.280722) Accuracy: 0.125000 (0.163847)\n",
      "[190/251] Loss: 2.270503 (2.280388) Accuracy: 0.125000 (0.164267)\n",
      "[200/251] Loss: 2.253113 (2.280378) Accuracy: 0.125000 (0.162624)\n",
      "[210/251] Loss: 2.227705 (2.280238) Accuracy: 0.218750 (0.162767)\n",
      "[220/251] Loss: 2.278655 (2.279947) Accuracy: 0.156250 (0.163320)\n",
      "[230/251] Loss: 2.289839 (2.279653) Accuracy: 0.093750 (0.162608)\n",
      "[240/251] Loss: 2.234614 (2.279244) Accuracy: 0.250000 (0.163122)\n",
      "[250/251] Loss: 2.383509 (2.279748) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291636 (2.291636) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294604 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326171 (2.310314) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307304 (2.309538) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298023 (2.308969) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308629 (2.310123) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327869 (2.310613) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 78/100, Train Loss: 2.2797, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.278573 (2.278573) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.260516 (2.283837) Accuracy: 0.187500 (0.161932)\n",
      "[20/251] Loss: 2.242986 (2.280307) Accuracy: 0.250000 (0.163690)\n",
      "[30/251] Loss: 2.261238 (2.281385) Accuracy: 0.156250 (0.167339)\n",
      "[40/251] Loss: 2.261886 (2.280513) Accuracy: 0.187500 (0.163110)\n",
      "[50/251] Loss: 2.262814 (2.279533) Accuracy: 0.218750 (0.166054)\n",
      "[60/251] Loss: 2.290300 (2.278818) Accuracy: 0.156250 (0.168545)\n",
      "[70/251] Loss: 2.285263 (2.280344) Accuracy: 0.125000 (0.163732)\n",
      "[80/251] Loss: 2.297900 (2.280844) Accuracy: 0.125000 (0.162809)\n",
      "[90/251] Loss: 2.295302 (2.278841) Accuracy: 0.093750 (0.165179)\n",
      "[100/251] Loss: 2.291010 (2.278148) Accuracy: 0.187500 (0.165532)\n",
      "[110/251] Loss: 2.259649 (2.278720) Accuracy: 0.250000 (0.164414)\n",
      "[120/251] Loss: 2.241611 (2.279648) Accuracy: 0.250000 (0.163998)\n",
      "[130/251] Loss: 2.263306 (2.278671) Accuracy: 0.156250 (0.166031)\n",
      "[140/251] Loss: 2.277909 (2.277361) Accuracy: 0.156250 (0.167110)\n",
      "[150/251] Loss: 2.309098 (2.278149) Accuracy: 0.093750 (0.165563)\n",
      "[160/251] Loss: 2.256795 (2.278659) Accuracy: 0.125000 (0.163238)\n",
      "[170/251] Loss: 2.317031 (2.278775) Accuracy: 0.093750 (0.162463)\n",
      "[180/251] Loss: 2.302009 (2.278743) Accuracy: 0.156250 (0.163329)\n",
      "[190/251] Loss: 2.323482 (2.279278) Accuracy: 0.062500 (0.161486)\n",
      "[200/251] Loss: 2.303909 (2.279561) Accuracy: 0.187500 (0.161692)\n",
      "[210/251] Loss: 2.270055 (2.279307) Accuracy: 0.250000 (0.162470)\n",
      "[220/251] Loss: 2.288674 (2.279682) Accuracy: 0.187500 (0.162613)\n",
      "[230/251] Loss: 2.293918 (2.278707) Accuracy: 0.125000 (0.164367)\n",
      "[240/251] Loss: 2.283442 (2.279319) Accuracy: 0.125000 (0.162733)\n",
      "[250/251] Loss: 2.397296 (2.279800) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291639 (2.291639) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294602 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326173 (2.310314) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307303 (2.309538) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298028 (2.308969) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308630 (2.310124) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327870 (2.310613) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 79/100, Train Loss: 2.2798, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.306411 (2.306411) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.235776 (2.280381) Accuracy: 0.218750 (0.144886)\n",
      "[20/251] Loss: 2.255555 (2.279327) Accuracy: 0.187500 (0.159226)\n",
      "[30/251] Loss: 2.268014 (2.280113) Accuracy: 0.218750 (0.166331)\n",
      "[40/251] Loss: 2.271356 (2.278515) Accuracy: 0.125000 (0.164634)\n",
      "[50/251] Loss: 2.272185 (2.279492) Accuracy: 0.218750 (0.166667)\n",
      "[60/251] Loss: 2.290932 (2.281752) Accuracy: 0.156250 (0.161885)\n",
      "[70/251] Loss: 2.274659 (2.281606) Accuracy: 0.187500 (0.161092)\n",
      "[80/251] Loss: 2.279752 (2.280913) Accuracy: 0.156250 (0.162037)\n",
      "[90/251] Loss: 2.250641 (2.280445) Accuracy: 0.156250 (0.161058)\n",
      "[100/251] Loss: 2.249806 (2.279538) Accuracy: 0.281250 (0.163057)\n",
      "[110/251] Loss: 2.284934 (2.279832) Accuracy: 0.125000 (0.161318)\n",
      "[120/251] Loss: 2.250583 (2.279362) Accuracy: 0.250000 (0.163481)\n",
      "[130/251] Loss: 2.295213 (2.279523) Accuracy: 0.156250 (0.162452)\n",
      "[140/251] Loss: 2.306160 (2.279089) Accuracy: 0.093750 (0.162899)\n",
      "[150/251] Loss: 2.280244 (2.279221) Accuracy: 0.062500 (0.161631)\n",
      "[160/251] Loss: 2.300343 (2.279623) Accuracy: 0.187500 (0.160326)\n",
      "[170/251] Loss: 2.263545 (2.279144) Accuracy: 0.187500 (0.162463)\n",
      "[180/251] Loss: 2.275733 (2.278688) Accuracy: 0.187500 (0.162983)\n",
      "[190/251] Loss: 2.244625 (2.279274) Accuracy: 0.250000 (0.161649)\n",
      "[200/251] Loss: 2.273100 (2.279345) Accuracy: 0.187500 (0.162780)\n",
      "[210/251] Loss: 2.255369 (2.279729) Accuracy: 0.250000 (0.162767)\n",
      "[220/251] Loss: 2.265302 (2.280093) Accuracy: 0.250000 (0.162189)\n",
      "[230/251] Loss: 2.264073 (2.279917) Accuracy: 0.156250 (0.161391)\n",
      "[240/251] Loss: 2.285711 (2.279943) Accuracy: 0.156250 (0.161696)\n",
      "[250/251] Loss: 2.408587 (2.279842) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291638 (2.291638) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294600 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326174 (2.310314) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307302 (2.309538) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298028 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308630 (2.310124) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327872 (2.310613) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 80/100, Train Loss: 2.2798, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.281211 (2.281211) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.298792 (2.285766) Accuracy: 0.218750 (0.164773)\n",
      "[20/251] Loss: 2.274853 (2.285284) Accuracy: 0.187500 (0.156250)\n",
      "[30/251] Loss: 2.233039 (2.281759) Accuracy: 0.250000 (0.159274)\n",
      "[40/251] Loss: 2.238406 (2.281433) Accuracy: 0.218750 (0.156250)\n",
      "[50/251] Loss: 2.268094 (2.280684) Accuracy: 0.093750 (0.158088)\n",
      "[60/251] Loss: 2.279794 (2.277387) Accuracy: 0.156250 (0.161885)\n",
      "[70/251] Loss: 2.277398 (2.277159) Accuracy: 0.218750 (0.164613)\n",
      "[80/251] Loss: 2.288165 (2.277246) Accuracy: 0.125000 (0.164738)\n",
      "[90/251] Loss: 2.301960 (2.277498) Accuracy: 0.125000 (0.165179)\n",
      "[100/251] Loss: 2.299179 (2.279060) Accuracy: 0.093750 (0.162748)\n",
      "[110/251] Loss: 2.240235 (2.280815) Accuracy: 0.218750 (0.159347)\n",
      "[120/251] Loss: 2.318385 (2.280681) Accuracy: 0.062500 (0.160124)\n",
      "[130/251] Loss: 2.276932 (2.281076) Accuracy: 0.250000 (0.159113)\n",
      "[140/251] Loss: 2.284946 (2.281123) Accuracy: 0.156250 (0.158466)\n",
      "[150/251] Loss: 2.311601 (2.281380) Accuracy: 0.093750 (0.158940)\n",
      "[160/251] Loss: 2.303768 (2.281411) Accuracy: 0.062500 (0.158773)\n",
      "[170/251] Loss: 2.242074 (2.281126) Accuracy: 0.250000 (0.159357)\n",
      "[180/251] Loss: 2.262485 (2.280977) Accuracy: 0.250000 (0.159876)\n",
      "[190/251] Loss: 2.211843 (2.280695) Accuracy: 0.312500 (0.160831)\n",
      "[200/251] Loss: 2.287630 (2.280643) Accuracy: 0.187500 (0.160914)\n",
      "[210/251] Loss: 2.286266 (2.280717) Accuracy: 0.187500 (0.161286)\n",
      "[220/251] Loss: 2.278501 (2.279972) Accuracy: 0.125000 (0.162472)\n",
      "[230/251] Loss: 2.287737 (2.279452) Accuracy: 0.156250 (0.163420)\n",
      "[240/251] Loss: 2.288621 (2.279404) Accuracy: 0.093750 (0.163382)\n",
      "[250/251] Loss: 2.299681 (2.279421) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291638 (2.291638) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294600 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326174 (2.310315) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307305 (2.309538) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298029 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308630 (2.310124) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327873 (2.310613) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 81/100, Train Loss: 2.2794, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.223560 (2.223560) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.315412 (2.266532) Accuracy: 0.062500 (0.176136)\n",
      "[20/251] Loss: 2.280869 (2.268698) Accuracy: 0.156250 (0.175595)\n",
      "[30/251] Loss: 2.305983 (2.271273) Accuracy: 0.125000 (0.174395)\n",
      "[40/251] Loss: 2.264323 (2.271351) Accuracy: 0.218750 (0.179116)\n",
      "[50/251] Loss: 2.277976 (2.273554) Accuracy: 0.156250 (0.173407)\n",
      "[60/251] Loss: 2.259120 (2.276567) Accuracy: 0.250000 (0.166496)\n",
      "[70/251] Loss: 2.243987 (2.277329) Accuracy: 0.250000 (0.162852)\n",
      "[80/251] Loss: 2.305719 (2.279223) Accuracy: 0.156250 (0.158951)\n",
      "[90/251] Loss: 2.283630 (2.279645) Accuracy: 0.156250 (0.158997)\n",
      "[100/251] Loss: 2.298840 (2.281083) Accuracy: 0.093750 (0.155012)\n",
      "[110/251] Loss: 2.301488 (2.280028) Accuracy: 0.187500 (0.157658)\n",
      "[120/251] Loss: 2.284746 (2.280287) Accuracy: 0.125000 (0.156767)\n",
      "[130/251] Loss: 2.265983 (2.281095) Accuracy: 0.156250 (0.156250)\n",
      "[140/251] Loss: 2.233706 (2.281207) Accuracy: 0.218750 (0.156250)\n",
      "[150/251] Loss: 2.295774 (2.281302) Accuracy: 0.156250 (0.156250)\n",
      "[160/251] Loss: 2.291449 (2.280872) Accuracy: 0.125000 (0.157415)\n",
      "[170/251] Loss: 2.286530 (2.280198) Accuracy: 0.093750 (0.158260)\n",
      "[180/251] Loss: 2.247418 (2.279227) Accuracy: 0.187500 (0.159876)\n",
      "[190/251] Loss: 2.263696 (2.279323) Accuracy: 0.218750 (0.159686)\n",
      "[200/251] Loss: 2.244961 (2.279355) Accuracy: 0.218750 (0.160603)\n",
      "[210/251] Loss: 2.266183 (2.278848) Accuracy: 0.187500 (0.163359)\n",
      "[220/251] Loss: 2.231106 (2.278822) Accuracy: 0.281250 (0.162896)\n",
      "[230/251] Loss: 2.309786 (2.279122) Accuracy: 0.156250 (0.162338)\n",
      "[240/251] Loss: 2.264964 (2.279431) Accuracy: 0.187500 (0.162215)\n",
      "[250/251] Loss: 2.382446 (2.279739) Accuracy: 0.000000 (0.162351)\n",
      "[0/63] Loss: 2.291637 (2.291637) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294600 (2.311459) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326175 (2.310315) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307306 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298028 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308630 (2.310124) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327872 (2.310614) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 82/100, Train Loss: 2.2797, Train Acc: 0.1624, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.308620 (2.308620) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.265164 (2.264213) Accuracy: 0.156250 (0.176136)\n",
      "[20/251] Loss: 2.239772 (2.270853) Accuracy: 0.187500 (0.174107)\n",
      "[30/251] Loss: 2.290564 (2.276648) Accuracy: 0.156250 (0.164315)\n",
      "[40/251] Loss: 2.288539 (2.278437) Accuracy: 0.156250 (0.163110)\n",
      "[50/251] Loss: 2.278772 (2.278125) Accuracy: 0.156250 (0.164216)\n",
      "[60/251] Loss: 2.266341 (2.275043) Accuracy: 0.250000 (0.168033)\n",
      "[70/251] Loss: 2.307736 (2.275820) Accuracy: 0.125000 (0.167694)\n",
      "[80/251] Loss: 2.261699 (2.275202) Accuracy: 0.218750 (0.170139)\n",
      "[90/251] Loss: 2.310852 (2.275812) Accuracy: 0.125000 (0.167582)\n",
      "[100/251] Loss: 2.315262 (2.277126) Accuracy: 0.093750 (0.166151)\n",
      "[110/251] Loss: 2.291082 (2.278025) Accuracy: 0.187500 (0.164977)\n",
      "[120/251] Loss: 2.331471 (2.278169) Accuracy: 0.125000 (0.166064)\n",
      "[130/251] Loss: 2.213357 (2.277762) Accuracy: 0.281250 (0.165792)\n",
      "[140/251] Loss: 2.341203 (2.277226) Accuracy: 0.062500 (0.167775)\n",
      "[150/251] Loss: 2.245712 (2.277933) Accuracy: 0.218750 (0.166391)\n",
      "[160/251] Loss: 2.301585 (2.278991) Accuracy: 0.125000 (0.164402)\n",
      "[170/251] Loss: 2.305998 (2.279268) Accuracy: 0.062500 (0.163012)\n",
      "[180/251] Loss: 2.271370 (2.279285) Accuracy: 0.187500 (0.162983)\n",
      "[190/251] Loss: 2.306874 (2.278866) Accuracy: 0.062500 (0.164431)\n",
      "[200/251] Loss: 2.273606 (2.278756) Accuracy: 0.156250 (0.164646)\n",
      "[210/251] Loss: 2.246603 (2.278843) Accuracy: 0.187500 (0.164100)\n",
      "[220/251] Loss: 2.248687 (2.279158) Accuracy: 0.156250 (0.163320)\n",
      "[230/251] Loss: 2.297078 (2.279149) Accuracy: 0.156250 (0.163149)\n",
      "[240/251] Loss: 2.273400 (2.279048) Accuracy: 0.156250 (0.163511)\n",
      "[250/251] Loss: 2.007614 (2.278292) Accuracy: 1.000000 (0.166086)\n",
      "[0/63] Loss: 2.291640 (2.291640) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294596 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326178 (2.310315) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307310 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298030 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308631 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327878 (2.310614) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 83/100, Train Loss: 2.2783, Train Acc: 0.1661, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.250806 (2.250806) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.251274 (2.277386) Accuracy: 0.156250 (0.159091)\n",
      "[20/251] Loss: 2.336428 (2.279923) Accuracy: 0.062500 (0.156250)\n",
      "[30/251] Loss: 2.297055 (2.279178) Accuracy: 0.125000 (0.160282)\n",
      "[40/251] Loss: 2.297364 (2.283787) Accuracy: 0.062500 (0.149390)\n",
      "[50/251] Loss: 2.257493 (2.282618) Accuracy: 0.187500 (0.149510)\n",
      "[60/251] Loss: 2.287039 (2.281831) Accuracy: 0.125000 (0.151127)\n",
      "[70/251] Loss: 2.245759 (2.281915) Accuracy: 0.156250 (0.150528)\n",
      "[80/251] Loss: 2.280815 (2.280904) Accuracy: 0.156250 (0.155093)\n",
      "[90/251] Loss: 2.271003 (2.282328) Accuracy: 0.093750 (0.151442)\n",
      "[100/251] Loss: 2.208621 (2.279969) Accuracy: 0.343750 (0.157797)\n",
      "[110/251] Loss: 2.253987 (2.279747) Accuracy: 0.250000 (0.159065)\n",
      "[120/251] Loss: 2.308987 (2.279779) Accuracy: 0.093750 (0.159607)\n",
      "[130/251] Loss: 2.284688 (2.280254) Accuracy: 0.218750 (0.159351)\n",
      "[140/251] Loss: 2.277366 (2.279430) Accuracy: 0.187500 (0.160904)\n",
      "[150/251] Loss: 2.258453 (2.279422) Accuracy: 0.218750 (0.161217)\n",
      "[160/251] Loss: 2.306778 (2.280243) Accuracy: 0.125000 (0.160714)\n",
      "[170/251] Loss: 2.263785 (2.279641) Accuracy: 0.187500 (0.162281)\n",
      "[180/251] Loss: 2.260082 (2.279660) Accuracy: 0.187500 (0.162638)\n",
      "[190/251] Loss: 2.271006 (2.279293) Accuracy: 0.187500 (0.163122)\n",
      "[200/251] Loss: 2.269203 (2.279592) Accuracy: 0.187500 (0.162935)\n",
      "[210/251] Loss: 2.266719 (2.279354) Accuracy: 0.187500 (0.163063)\n",
      "[220/251] Loss: 2.289835 (2.279122) Accuracy: 0.093750 (0.163744)\n",
      "[230/251] Loss: 2.308442 (2.279182) Accuracy: 0.125000 (0.162879)\n",
      "[240/251] Loss: 2.287351 (2.279089) Accuracy: 0.218750 (0.163641)\n",
      "[250/251] Loss: 2.380512 (2.279730) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291640 (2.291640) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294595 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326179 (2.310316) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307307 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298029 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308632 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327876 (2.310614) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 84/100, Train Loss: 2.2797, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.278560 (2.278560) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.326622 (2.287054) Accuracy: 0.062500 (0.156250)\n",
      "[20/251] Loss: 2.248619 (2.281252) Accuracy: 0.250000 (0.168155)\n",
      "[30/251] Loss: 2.276772 (2.280681) Accuracy: 0.156250 (0.155242)\n",
      "[40/251] Loss: 2.268521 (2.278196) Accuracy: 0.187500 (0.160823)\n",
      "[50/251] Loss: 2.226584 (2.280746) Accuracy: 0.250000 (0.154412)\n",
      "[60/251] Loss: 2.321254 (2.280024) Accuracy: 0.062500 (0.153689)\n",
      "[70/251] Loss: 2.288399 (2.281141) Accuracy: 0.125000 (0.153609)\n",
      "[80/251] Loss: 2.331390 (2.281793) Accuracy: 0.062500 (0.155864)\n",
      "[90/251] Loss: 2.265893 (2.281303) Accuracy: 0.156250 (0.156593)\n",
      "[100/251] Loss: 2.196784 (2.279500) Accuracy: 0.343750 (0.158416)\n",
      "[110/251] Loss: 2.238841 (2.278215) Accuracy: 0.250000 (0.162725)\n",
      "[120/251] Loss: 2.264573 (2.277858) Accuracy: 0.156250 (0.164514)\n",
      "[130/251] Loss: 2.283760 (2.277791) Accuracy: 0.125000 (0.164599)\n",
      "[140/251] Loss: 2.279656 (2.278443) Accuracy: 0.156250 (0.164007)\n",
      "[150/251] Loss: 2.272617 (2.279091) Accuracy: 0.156250 (0.161217)\n",
      "[160/251] Loss: 2.339257 (2.279501) Accuracy: 0.062500 (0.161685)\n",
      "[170/251] Loss: 2.269902 (2.278590) Accuracy: 0.156250 (0.162829)\n",
      "[180/251] Loss: 2.245022 (2.278089) Accuracy: 0.281250 (0.163674)\n",
      "[190/251] Loss: 2.316578 (2.278440) Accuracy: 0.125000 (0.163285)\n",
      "[200/251] Loss: 2.274184 (2.278608) Accuracy: 0.156250 (0.163557)\n",
      "[210/251] Loss: 2.272222 (2.278646) Accuracy: 0.187500 (0.163211)\n",
      "[220/251] Loss: 2.287792 (2.278484) Accuracy: 0.187500 (0.164310)\n",
      "[230/251] Loss: 2.246870 (2.278851) Accuracy: 0.218750 (0.164232)\n",
      "[240/251] Loss: 2.303465 (2.278903) Accuracy: 0.062500 (0.163641)\n",
      "[250/251] Loss: 2.316895 (2.279484) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291642 (2.291642) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294594 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326178 (2.310316) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307307 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298029 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308633 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327876 (2.310614) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 85/100, Train Loss: 2.2795, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.287525 (2.287525) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.314915 (2.281052) Accuracy: 0.125000 (0.153409)\n",
      "[20/251] Loss: 2.305904 (2.279027) Accuracy: 0.125000 (0.163690)\n",
      "[30/251] Loss: 2.308291 (2.278918) Accuracy: 0.062500 (0.166331)\n",
      "[40/251] Loss: 2.319931 (2.277476) Accuracy: 0.000000 (0.166921)\n",
      "[50/251] Loss: 2.320519 (2.279121) Accuracy: 0.093750 (0.162990)\n",
      "[60/251] Loss: 2.219805 (2.279759) Accuracy: 0.218750 (0.158811)\n",
      "[70/251] Loss: 2.311382 (2.277563) Accuracy: 0.093750 (0.164613)\n",
      "[80/251] Loss: 2.315386 (2.279231) Accuracy: 0.093750 (0.158565)\n",
      "[90/251] Loss: 2.232176 (2.279937) Accuracy: 0.218750 (0.157967)\n",
      "[100/251] Loss: 2.207470 (2.278408) Accuracy: 0.281250 (0.161819)\n",
      "[110/251] Loss: 2.244064 (2.278545) Accuracy: 0.250000 (0.162725)\n",
      "[120/251] Loss: 2.297729 (2.276887) Accuracy: 0.218750 (0.167614)\n",
      "[130/251] Loss: 2.309883 (2.277559) Accuracy: 0.031250 (0.165792)\n",
      "[140/251] Loss: 2.347921 (2.277735) Accuracy: 0.031250 (0.166223)\n",
      "[150/251] Loss: 2.250341 (2.278021) Accuracy: 0.218750 (0.165977)\n",
      "[160/251] Loss: 2.288964 (2.276867) Accuracy: 0.125000 (0.168284)\n",
      "[170/251] Loss: 2.271747 (2.277807) Accuracy: 0.125000 (0.166118)\n",
      "[180/251] Loss: 2.283748 (2.278203) Accuracy: 0.187500 (0.164537)\n",
      "[190/251] Loss: 2.221590 (2.278582) Accuracy: 0.312500 (0.164594)\n",
      "[200/251] Loss: 2.286101 (2.278727) Accuracy: 0.125000 (0.164646)\n",
      "[210/251] Loss: 2.290814 (2.278250) Accuracy: 0.156250 (0.165136)\n",
      "[220/251] Loss: 2.274146 (2.278774) Accuracy: 0.187500 (0.164451)\n",
      "[230/251] Loss: 2.270205 (2.279144) Accuracy: 0.156250 (0.164096)\n",
      "[240/251] Loss: 2.300023 (2.279513) Accuracy: 0.125000 (0.163252)\n",
      "[250/251] Loss: 2.399751 (2.279803) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291641 (2.291641) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294594 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326178 (2.310316) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307306 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298029 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308633 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327876 (2.310614) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 86/100, Train Loss: 2.2798, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.252018 (2.252018) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.313469 (2.282365) Accuracy: 0.156250 (0.173295)\n",
      "[20/251] Loss: 2.287710 (2.280112) Accuracy: 0.156250 (0.166667)\n",
      "[30/251] Loss: 2.278556 (2.277571) Accuracy: 0.156250 (0.168347)\n",
      "[40/251] Loss: 2.210205 (2.276182) Accuracy: 0.312500 (0.169970)\n",
      "[50/251] Loss: 2.298483 (2.278533) Accuracy: 0.187500 (0.167892)\n",
      "[60/251] Loss: 2.270494 (2.281148) Accuracy: 0.187500 (0.161373)\n",
      "[70/251] Loss: 2.262647 (2.281709) Accuracy: 0.187500 (0.162852)\n",
      "[80/251] Loss: 2.284562 (2.281346) Accuracy: 0.156250 (0.162809)\n",
      "[90/251] Loss: 2.281831 (2.281013) Accuracy: 0.218750 (0.164148)\n",
      "[100/251] Loss: 2.231622 (2.279890) Accuracy: 0.343750 (0.165532)\n",
      "[110/251] Loss: 2.236253 (2.279862) Accuracy: 0.250000 (0.163851)\n",
      "[120/251] Loss: 2.279406 (2.279839) Accuracy: 0.187500 (0.163740)\n",
      "[130/251] Loss: 2.265876 (2.280022) Accuracy: 0.187500 (0.162214)\n",
      "[140/251] Loss: 2.301061 (2.279992) Accuracy: 0.093750 (0.162234)\n",
      "[150/251] Loss: 2.264856 (2.279649) Accuracy: 0.218750 (0.163493)\n",
      "[160/251] Loss: 2.298814 (2.279272) Accuracy: 0.156250 (0.164208)\n",
      "[170/251] Loss: 2.243531 (2.279251) Accuracy: 0.250000 (0.164108)\n",
      "[180/251] Loss: 2.266413 (2.279864) Accuracy: 0.125000 (0.162293)\n",
      "[190/251] Loss: 2.287359 (2.279625) Accuracy: 0.187500 (0.162631)\n",
      "[200/251] Loss: 2.288083 (2.279240) Accuracy: 0.125000 (0.162469)\n",
      "[210/251] Loss: 2.302370 (2.279432) Accuracy: 0.125000 (0.162470)\n",
      "[220/251] Loss: 2.256610 (2.279519) Accuracy: 0.250000 (0.162896)\n",
      "[230/251] Loss: 2.257118 (2.279547) Accuracy: 0.281250 (0.163826)\n",
      "[240/251] Loss: 2.290962 (2.278884) Accuracy: 0.156250 (0.164160)\n",
      "[250/251] Loss: 2.327376 (2.279523) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291641 (2.291641) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294593 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326179 (2.310316) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307304 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298030 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308633 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327876 (2.310614) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 87/100, Train Loss: 2.2795, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.275105 (2.275105) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.248302 (2.271147) Accuracy: 0.218750 (0.153409)\n",
      "[20/251] Loss: 2.274409 (2.278104) Accuracy: 0.156250 (0.150298)\n",
      "[30/251] Loss: 2.312609 (2.279717) Accuracy: 0.062500 (0.151210)\n",
      "[40/251] Loss: 2.234836 (2.278982) Accuracy: 0.250000 (0.152439)\n",
      "[50/251] Loss: 2.261671 (2.275748) Accuracy: 0.218750 (0.159314)\n",
      "[60/251] Loss: 2.322423 (2.276076) Accuracy: 0.062500 (0.160861)\n",
      "[70/251] Loss: 2.305284 (2.277477) Accuracy: 0.093750 (0.158011)\n",
      "[80/251] Loss: 2.271401 (2.277714) Accuracy: 0.187500 (0.159722)\n",
      "[90/251] Loss: 2.290404 (2.276816) Accuracy: 0.156250 (0.163462)\n",
      "[100/251] Loss: 2.272804 (2.277355) Accuracy: 0.156250 (0.163676)\n",
      "[110/251] Loss: 2.284744 (2.277368) Accuracy: 0.187500 (0.164133)\n",
      "[120/251] Loss: 2.249942 (2.277837) Accuracy: 0.250000 (0.163998)\n",
      "[130/251] Loss: 2.300132 (2.278766) Accuracy: 0.093750 (0.163406)\n",
      "[140/251] Loss: 2.314217 (2.278524) Accuracy: 0.156250 (0.165115)\n",
      "[150/251] Loss: 2.317558 (2.278385) Accuracy: 0.093750 (0.165149)\n",
      "[160/251] Loss: 2.271332 (2.279039) Accuracy: 0.187500 (0.164402)\n",
      "[170/251] Loss: 2.301891 (2.279232) Accuracy: 0.093750 (0.164474)\n",
      "[180/251] Loss: 2.281353 (2.278467) Accuracy: 0.187500 (0.165919)\n",
      "[190/251] Loss: 2.282601 (2.278208) Accuracy: 0.125000 (0.165903)\n",
      "[200/251] Loss: 2.250451 (2.278872) Accuracy: 0.281250 (0.164956)\n",
      "[210/251] Loss: 2.256712 (2.278554) Accuracy: 0.218750 (0.164396)\n",
      "[220/251] Loss: 2.257551 (2.278266) Accuracy: 0.156250 (0.165441)\n",
      "[230/251] Loss: 2.278447 (2.277994) Accuracy: 0.125000 (0.166126)\n",
      "[240/251] Loss: 2.284837 (2.278813) Accuracy: 0.125000 (0.164289)\n",
      "[250/251] Loss: 2.354812 (2.279628) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291641 (2.291641) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294592 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326180 (2.310316) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307304 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298030 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308633 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327877 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 88/100, Train Loss: 2.2796, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.287976 (2.287976) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.306058 (2.274108) Accuracy: 0.093750 (0.178977)\n",
      "[20/251] Loss: 2.261456 (2.281159) Accuracy: 0.250000 (0.163690)\n",
      "[30/251] Loss: 2.295734 (2.275436) Accuracy: 0.093750 (0.172379)\n",
      "[40/251] Loss: 2.273681 (2.275420) Accuracy: 0.187500 (0.173018)\n",
      "[50/251] Loss: 2.299330 (2.275667) Accuracy: 0.093750 (0.173407)\n",
      "[60/251] Loss: 2.300270 (2.276842) Accuracy: 0.062500 (0.169570)\n",
      "[70/251] Loss: 2.252345 (2.277367) Accuracy: 0.250000 (0.169014)\n",
      "[80/251] Loss: 2.276877 (2.277201) Accuracy: 0.156250 (0.168981)\n",
      "[90/251] Loss: 2.266617 (2.278659) Accuracy: 0.250000 (0.165522)\n",
      "[100/251] Loss: 2.284868 (2.280056) Accuracy: 0.156250 (0.165842)\n",
      "[110/251] Loss: 2.278322 (2.279497) Accuracy: 0.187500 (0.166667)\n",
      "[120/251] Loss: 2.293438 (2.278518) Accuracy: 0.093750 (0.168388)\n",
      "[130/251] Loss: 2.275926 (2.278121) Accuracy: 0.187500 (0.168416)\n",
      "[140/251] Loss: 2.276279 (2.277917) Accuracy: 0.187500 (0.170434)\n",
      "[150/251] Loss: 2.251103 (2.278222) Accuracy: 0.218750 (0.169081)\n",
      "[160/251] Loss: 2.232865 (2.277497) Accuracy: 0.218750 (0.169255)\n",
      "[170/251] Loss: 2.281580 (2.277717) Accuracy: 0.156250 (0.168677)\n",
      "[180/251] Loss: 2.271122 (2.278965) Accuracy: 0.125000 (0.165055)\n",
      "[190/251] Loss: 2.270136 (2.279129) Accuracy: 0.187500 (0.164103)\n",
      "[200/251] Loss: 2.290538 (2.279268) Accuracy: 0.187500 (0.163246)\n",
      "[210/251] Loss: 2.266350 (2.279286) Accuracy: 0.156250 (0.162767)\n",
      "[220/251] Loss: 2.241886 (2.279028) Accuracy: 0.218750 (0.162755)\n",
      "[230/251] Loss: 2.241459 (2.278834) Accuracy: 0.187500 (0.163420)\n",
      "[240/251] Loss: 2.286666 (2.278651) Accuracy: 0.156250 (0.164289)\n",
      "[250/251] Loss: 2.383047 (2.279737) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291642 (2.291642) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294591 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326181 (2.310316) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307304 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298030 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308633 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327876 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 89/100, Train Loss: 2.2797, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.295349 (2.295349) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.312551 (2.271785) Accuracy: 0.125000 (0.187500)\n",
      "[20/251] Loss: 2.328032 (2.273378) Accuracy: 0.062500 (0.181548)\n",
      "[30/251] Loss: 2.249263 (2.273491) Accuracy: 0.250000 (0.180444)\n",
      "[40/251] Loss: 2.290131 (2.274727) Accuracy: 0.218750 (0.186738)\n",
      "[50/251] Loss: 2.260590 (2.275199) Accuracy: 0.218750 (0.183211)\n",
      "[60/251] Loss: 2.324166 (2.276577) Accuracy: 0.031250 (0.177254)\n",
      "[70/251] Loss: 2.222256 (2.277650) Accuracy: 0.281250 (0.172535)\n",
      "[80/251] Loss: 2.320226 (2.278423) Accuracy: 0.125000 (0.171682)\n",
      "[90/251] Loss: 2.274857 (2.278438) Accuracy: 0.187500 (0.171016)\n",
      "[100/251] Loss: 2.316292 (2.279723) Accuracy: 0.093750 (0.167389)\n",
      "[110/251] Loss: 2.234950 (2.279735) Accuracy: 0.187500 (0.164977)\n",
      "[120/251] Loss: 2.265481 (2.280572) Accuracy: 0.187500 (0.163998)\n",
      "[130/251] Loss: 2.306492 (2.279598) Accuracy: 0.062500 (0.164361)\n",
      "[140/251] Loss: 2.268018 (2.279475) Accuracy: 0.218750 (0.165780)\n",
      "[150/251] Loss: 2.263319 (2.278433) Accuracy: 0.281250 (0.168253)\n",
      "[160/251] Loss: 2.253266 (2.277830) Accuracy: 0.281250 (0.169061)\n",
      "[170/251] Loss: 2.255555 (2.278041) Accuracy: 0.156250 (0.167032)\n",
      "[180/251] Loss: 2.230719 (2.278210) Accuracy: 0.250000 (0.165919)\n",
      "[190/251] Loss: 2.302031 (2.278379) Accuracy: 0.093750 (0.164594)\n",
      "[200/251] Loss: 2.218535 (2.278175) Accuracy: 0.343750 (0.164956)\n",
      "[210/251] Loss: 2.240769 (2.277519) Accuracy: 0.250000 (0.166914)\n",
      "[220/251] Loss: 2.335664 (2.277961) Accuracy: 0.062500 (0.165724)\n",
      "[230/251] Loss: 2.319754 (2.278680) Accuracy: 0.125000 (0.164096)\n",
      "[240/251] Loss: 2.306848 (2.279316) Accuracy: 0.062500 (0.162215)\n",
      "[250/251] Loss: 2.400772 (2.279805) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291641 (2.291641) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294591 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326181 (2.310316) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307304 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298030 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308633 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327876 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 90/100, Train Loss: 2.2798, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.313249 (2.313249) Accuracy: 0.093750 (0.093750)\n",
      "[10/251] Loss: 2.301033 (2.277150) Accuracy: 0.093750 (0.167614)\n",
      "[20/251] Loss: 2.305048 (2.280276) Accuracy: 0.156250 (0.156250)\n",
      "[30/251] Loss: 2.276110 (2.275500) Accuracy: 0.156250 (0.169355)\n",
      "[40/251] Loss: 2.255687 (2.271868) Accuracy: 0.250000 (0.178354)\n",
      "[50/251] Loss: 2.308722 (2.277185) Accuracy: 0.093750 (0.169730)\n",
      "[60/251] Loss: 2.278889 (2.274211) Accuracy: 0.156250 (0.173668)\n",
      "[70/251] Loss: 2.288770 (2.276046) Accuracy: 0.156250 (0.171215)\n",
      "[80/251] Loss: 2.247080 (2.275331) Accuracy: 0.250000 (0.172840)\n",
      "[90/251] Loss: 2.264735 (2.276366) Accuracy: 0.156250 (0.169299)\n",
      "[100/251] Loss: 2.284020 (2.276515) Accuracy: 0.187500 (0.169245)\n",
      "[110/251] Loss: 2.293962 (2.276788) Accuracy: 0.125000 (0.170045)\n",
      "[120/251] Loss: 2.295944 (2.277106) Accuracy: 0.187500 (0.166839)\n",
      "[130/251] Loss: 2.300172 (2.276388) Accuracy: 0.125000 (0.169370)\n",
      "[140/251] Loss: 2.305412 (2.276435) Accuracy: 0.125000 (0.169548)\n",
      "[150/251] Loss: 2.293183 (2.277224) Accuracy: 0.125000 (0.168460)\n",
      "[160/251] Loss: 2.325610 (2.277648) Accuracy: 0.062500 (0.167120)\n",
      "[170/251] Loss: 2.334866 (2.278716) Accuracy: 0.093750 (0.165570)\n",
      "[180/251] Loss: 2.315919 (2.279842) Accuracy: 0.062500 (0.162465)\n",
      "[190/251] Loss: 2.295090 (2.279724) Accuracy: 0.187500 (0.162140)\n",
      "[200/251] Loss: 2.290282 (2.280215) Accuracy: 0.093750 (0.160448)\n",
      "[210/251] Loss: 2.283188 (2.280067) Accuracy: 0.093750 (0.160693)\n",
      "[220/251] Loss: 2.299968 (2.280048) Accuracy: 0.187500 (0.161340)\n",
      "[230/251] Loss: 2.260457 (2.279661) Accuracy: 0.187500 (0.162067)\n",
      "[240/251] Loss: 2.270330 (2.279624) Accuracy: 0.156250 (0.162085)\n",
      "[250/251] Loss: 2.214805 (2.279087) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291641 (2.291641) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294592 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326180 (2.310316) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307305 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298028 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308633 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327876 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 91/100, Train Loss: 2.2791, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.293805 (2.293805) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.234208 (2.276484) Accuracy: 0.218750 (0.164773)\n",
      "[20/251] Loss: 2.265896 (2.275691) Accuracy: 0.187500 (0.162202)\n",
      "[30/251] Loss: 2.279248 (2.281062) Accuracy: 0.156250 (0.157258)\n",
      "[40/251] Loss: 2.305273 (2.281673) Accuracy: 0.125000 (0.153963)\n",
      "[50/251] Loss: 2.297302 (2.280434) Accuracy: 0.156250 (0.158088)\n",
      "[60/251] Loss: 2.262395 (2.282993) Accuracy: 0.218750 (0.154201)\n",
      "[70/251] Loss: 2.311048 (2.280919) Accuracy: 0.125000 (0.157130)\n",
      "[80/251] Loss: 2.305648 (2.279368) Accuracy: 0.156250 (0.160494)\n",
      "[90/251] Loss: 2.290016 (2.280437) Accuracy: 0.125000 (0.160714)\n",
      "[100/251] Loss: 2.267880 (2.281667) Accuracy: 0.218750 (0.157797)\n",
      "[110/251] Loss: 2.256235 (2.281485) Accuracy: 0.187500 (0.159347)\n",
      "[120/251] Loss: 2.286732 (2.281926) Accuracy: 0.156250 (0.159091)\n",
      "[130/251] Loss: 2.321530 (2.282319) Accuracy: 0.125000 (0.157681)\n",
      "[140/251] Loss: 2.251314 (2.281364) Accuracy: 0.187500 (0.159796)\n",
      "[150/251] Loss: 2.314798 (2.281786) Accuracy: 0.187500 (0.159147)\n",
      "[160/251] Loss: 2.289485 (2.281756) Accuracy: 0.156250 (0.160132)\n",
      "[170/251] Loss: 2.254335 (2.280980) Accuracy: 0.250000 (0.161550)\n",
      "[180/251] Loss: 2.282175 (2.280031) Accuracy: 0.125000 (0.161948)\n",
      "[190/251] Loss: 2.247172 (2.280613) Accuracy: 0.281250 (0.161158)\n",
      "[200/251] Loss: 2.305697 (2.280405) Accuracy: 0.093750 (0.162158)\n",
      "[210/251] Loss: 2.262746 (2.280438) Accuracy: 0.187500 (0.160841)\n",
      "[220/251] Loss: 2.253778 (2.280002) Accuracy: 0.281250 (0.161482)\n",
      "[230/251] Loss: 2.242433 (2.279820) Accuracy: 0.218750 (0.161526)\n",
      "[240/251] Loss: 2.297243 (2.279555) Accuracy: 0.156250 (0.162604)\n",
      "[250/251] Loss: 1.925014 (2.277968) Accuracy: 1.000000 (0.166086)\n",
      "[0/63] Loss: 2.291640 (2.291640) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294593 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326180 (2.310316) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307306 (2.309539) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298030 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308632 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327878 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 92/100, Train Loss: 2.2780, Train Acc: 0.1661, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.328415 (2.328415) Accuracy: 0.062500 (0.062500)\n",
      "[10/251] Loss: 2.314596 (2.278816) Accuracy: 0.093750 (0.173295)\n",
      "[20/251] Loss: 2.299016 (2.274448) Accuracy: 0.156250 (0.178571)\n",
      "[30/251] Loss: 2.277439 (2.280072) Accuracy: 0.187500 (0.161290)\n",
      "[40/251] Loss: 2.259586 (2.280694) Accuracy: 0.125000 (0.160061)\n",
      "[50/251] Loss: 2.192292 (2.277292) Accuracy: 0.343750 (0.166054)\n",
      "[60/251] Loss: 2.298868 (2.276444) Accuracy: 0.093750 (0.167008)\n",
      "[70/251] Loss: 2.301608 (2.277716) Accuracy: 0.125000 (0.164613)\n",
      "[80/251] Loss: 2.232339 (2.276143) Accuracy: 0.281250 (0.168596)\n",
      "[90/251] Loss: 2.256643 (2.276792) Accuracy: 0.218750 (0.166896)\n",
      "[100/251] Loss: 2.254494 (2.277677) Accuracy: 0.218750 (0.165532)\n",
      "[110/251] Loss: 2.269355 (2.276748) Accuracy: 0.187500 (0.168074)\n",
      "[120/251] Loss: 2.268770 (2.278587) Accuracy: 0.125000 (0.165289)\n",
      "[130/251] Loss: 2.314635 (2.278279) Accuracy: 0.093750 (0.166031)\n",
      "[140/251] Loss: 2.305501 (2.278703) Accuracy: 0.156250 (0.166002)\n",
      "[150/251] Loss: 2.225213 (2.277825) Accuracy: 0.312500 (0.168460)\n",
      "[160/251] Loss: 2.297195 (2.277857) Accuracy: 0.125000 (0.168478)\n",
      "[170/251] Loss: 2.309254 (2.279345) Accuracy: 0.125000 (0.165387)\n",
      "[180/251] Loss: 2.256163 (2.280155) Accuracy: 0.187500 (0.163501)\n",
      "[190/251] Loss: 2.241227 (2.281029) Accuracy: 0.312500 (0.162467)\n",
      "[200/251] Loss: 2.280180 (2.280969) Accuracy: 0.125000 (0.162002)\n",
      "[210/251] Loss: 2.320685 (2.280471) Accuracy: 0.093750 (0.162174)\n",
      "[220/251] Loss: 2.284068 (2.279451) Accuracy: 0.125000 (0.164310)\n",
      "[230/251] Loss: 2.292822 (2.279204) Accuracy: 0.187500 (0.164502)\n",
      "[240/251] Loss: 2.261394 (2.279275) Accuracy: 0.187500 (0.163382)\n",
      "[250/251] Loss: 2.148822 (2.278831) Accuracy: 1.000000 (0.166086)\n",
      "[0/63] Loss: 2.291639 (2.291639) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294594 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326181 (2.310317) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307304 (2.309540) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298030 (2.308970) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308632 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327877 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 93/100, Train Loss: 2.2788, Train Acc: 0.1661, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.292999 (2.292999) Accuracy: 0.156250 (0.156250)\n",
      "[10/251] Loss: 2.316279 (2.285970) Accuracy: 0.093750 (0.144886)\n",
      "[20/251] Loss: 2.260102 (2.282071) Accuracy: 0.218750 (0.160714)\n",
      "[30/251] Loss: 2.302329 (2.285127) Accuracy: 0.093750 (0.147177)\n",
      "[40/251] Loss: 2.289228 (2.285221) Accuracy: 0.125000 (0.149390)\n",
      "[50/251] Loss: 2.251767 (2.282589) Accuracy: 0.281250 (0.156863)\n",
      "[60/251] Loss: 2.277689 (2.285659) Accuracy: 0.125000 (0.148053)\n",
      "[70/251] Loss: 2.240460 (2.286839) Accuracy: 0.218750 (0.145246)\n",
      "[80/251] Loss: 2.267812 (2.285890) Accuracy: 0.187500 (0.146219)\n",
      "[90/251] Loss: 2.257226 (2.285885) Accuracy: 0.218750 (0.146291)\n",
      "[100/251] Loss: 2.276181 (2.284415) Accuracy: 0.156250 (0.151918)\n",
      "[110/251] Loss: 2.298196 (2.283337) Accuracy: 0.125000 (0.154842)\n",
      "[120/251] Loss: 2.250968 (2.283122) Accuracy: 0.218750 (0.156508)\n",
      "[130/251] Loss: 2.297899 (2.283718) Accuracy: 0.187500 (0.155296)\n",
      "[140/251] Loss: 2.273284 (2.283003) Accuracy: 0.125000 (0.156472)\n",
      "[150/251] Loss: 2.245537 (2.281998) Accuracy: 0.187500 (0.157078)\n",
      "[160/251] Loss: 2.288624 (2.281324) Accuracy: 0.156250 (0.158579)\n",
      "[170/251] Loss: 2.299937 (2.281460) Accuracy: 0.125000 (0.159539)\n",
      "[180/251] Loss: 2.256190 (2.281120) Accuracy: 0.218750 (0.161084)\n",
      "[190/251] Loss: 2.271132 (2.280800) Accuracy: 0.187500 (0.161649)\n",
      "[200/251] Loss: 2.224965 (2.279430) Accuracy: 0.312500 (0.163868)\n",
      "[210/251] Loss: 2.342182 (2.279668) Accuracy: 0.031250 (0.162767)\n",
      "[220/251] Loss: 2.280914 (2.279403) Accuracy: 0.218750 (0.163603)\n",
      "[230/251] Loss: 2.294223 (2.279531) Accuracy: 0.093750 (0.163285)\n",
      "[240/251] Loss: 2.267635 (2.279257) Accuracy: 0.218750 (0.163252)\n",
      "[250/251] Loss: 1.892920 (2.277843) Accuracy: 1.000000 (0.166086)\n",
      "[0/63] Loss: 2.291639 (2.291639) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294593 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326182 (2.310317) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307302 (2.309540) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298031 (2.308971) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308632 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327878 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 94/100, Train Loss: 2.2778, Train Acc: 0.1661, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.202015 (2.202015) Accuracy: 0.343750 (0.343750)\n",
      "[10/251] Loss: 2.278849 (2.273684) Accuracy: 0.187500 (0.187500)\n",
      "[20/251] Loss: 2.292096 (2.281423) Accuracy: 0.156250 (0.159226)\n",
      "[30/251] Loss: 2.274913 (2.277828) Accuracy: 0.218750 (0.166331)\n",
      "[40/251] Loss: 2.262285 (2.282463) Accuracy: 0.218750 (0.156250)\n",
      "[50/251] Loss: 2.294972 (2.279846) Accuracy: 0.156250 (0.161152)\n",
      "[60/251] Loss: 2.272527 (2.280193) Accuracy: 0.156250 (0.159324)\n",
      "[70/251] Loss: 2.281337 (2.278957) Accuracy: 0.156250 (0.162852)\n",
      "[80/251] Loss: 2.254464 (2.279641) Accuracy: 0.187500 (0.162037)\n",
      "[90/251] Loss: 2.286231 (2.280709) Accuracy: 0.156250 (0.160714)\n",
      "[100/251] Loss: 2.296489 (2.279686) Accuracy: 0.187500 (0.164295)\n",
      "[110/251] Loss: 2.263365 (2.278776) Accuracy: 0.218750 (0.166948)\n",
      "[120/251] Loss: 2.262758 (2.278849) Accuracy: 0.156250 (0.167614)\n",
      "[130/251] Loss: 2.270858 (2.279025) Accuracy: 0.156250 (0.166508)\n",
      "[140/251] Loss: 2.326101 (2.278859) Accuracy: 0.031250 (0.165559)\n",
      "[150/251] Loss: 2.340489 (2.279387) Accuracy: 0.031250 (0.163493)\n",
      "[160/251] Loss: 2.255752 (2.279426) Accuracy: 0.187500 (0.163432)\n",
      "[170/251] Loss: 2.278814 (2.278991) Accuracy: 0.187500 (0.165022)\n",
      "[180/251] Loss: 2.272323 (2.279375) Accuracy: 0.156250 (0.164365)\n",
      "[190/251] Loss: 2.244051 (2.279000) Accuracy: 0.281250 (0.165576)\n",
      "[200/251] Loss: 2.307978 (2.279175) Accuracy: 0.093750 (0.164490)\n",
      "[210/251] Loss: 2.198427 (2.278751) Accuracy: 0.250000 (0.165136)\n",
      "[220/251] Loss: 2.288154 (2.279202) Accuracy: 0.125000 (0.164169)\n",
      "[230/251] Loss: 2.265106 (2.279232) Accuracy: 0.218750 (0.164096)\n",
      "[240/251] Loss: 2.233083 (2.279392) Accuracy: 0.218750 (0.162733)\n",
      "[250/251] Loss: 2.119641 (2.278718) Accuracy: 1.000000 (0.166086)\n",
      "[0/63] Loss: 2.291639 (2.291639) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294594 (2.311461) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326182 (2.310317) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307302 (2.309540) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298031 (2.308971) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308632 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327878 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 95/100, Train Loss: 2.2787, Train Acc: 0.1661, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.236965 (2.236965) Accuracy: 0.343750 (0.343750)\n",
      "[10/251] Loss: 2.266582 (2.284652) Accuracy: 0.218750 (0.153409)\n",
      "[20/251] Loss: 2.280611 (2.291538) Accuracy: 0.156250 (0.139881)\n",
      "[30/251] Loss: 2.255047 (2.284472) Accuracy: 0.281250 (0.156250)\n",
      "[40/251] Loss: 2.262335 (2.280300) Accuracy: 0.156250 (0.163110)\n",
      "[50/251] Loss: 2.280369 (2.280618) Accuracy: 0.156250 (0.162990)\n",
      "[60/251] Loss: 2.317280 (2.280185) Accuracy: 0.125000 (0.161373)\n",
      "[70/251] Loss: 2.243289 (2.281118) Accuracy: 0.187500 (0.158451)\n",
      "[80/251] Loss: 2.307892 (2.280828) Accuracy: 0.125000 (0.158565)\n",
      "[90/251] Loss: 2.213660 (2.281287) Accuracy: 0.250000 (0.157624)\n",
      "[100/251] Loss: 2.312103 (2.281193) Accuracy: 0.062500 (0.158106)\n",
      "[110/251] Loss: 2.303774 (2.280983) Accuracy: 0.125000 (0.158502)\n",
      "[120/251] Loss: 2.282880 (2.280194) Accuracy: 0.218750 (0.162448)\n",
      "[130/251] Loss: 2.306136 (2.280457) Accuracy: 0.125000 (0.161260)\n",
      "[140/251] Loss: 2.295269 (2.280066) Accuracy: 0.156250 (0.164229)\n",
      "[150/251] Loss: 2.296544 (2.280218) Accuracy: 0.125000 (0.163493)\n",
      "[160/251] Loss: 2.311756 (2.279957) Accuracy: 0.031250 (0.163820)\n",
      "[170/251] Loss: 2.271479 (2.279340) Accuracy: 0.156250 (0.164839)\n",
      "[180/251] Loss: 2.268680 (2.279728) Accuracy: 0.125000 (0.163847)\n",
      "[190/251] Loss: 2.319943 (2.279834) Accuracy: 0.125000 (0.163285)\n",
      "[200/251] Loss: 2.243743 (2.279867) Accuracy: 0.218750 (0.162780)\n",
      "[210/251] Loss: 2.261637 (2.279731) Accuracy: 0.156250 (0.162322)\n",
      "[220/251] Loss: 2.286112 (2.279489) Accuracy: 0.218750 (0.162472)\n",
      "[230/251] Loss: 2.304969 (2.279358) Accuracy: 0.156250 (0.163014)\n",
      "[240/251] Loss: 2.295819 (2.279672) Accuracy: 0.156250 (0.162474)\n",
      "[250/251] Loss: 2.077186 (2.278554) Accuracy: 1.000000 (0.166086)\n",
      "[0/63] Loss: 2.291639 (2.291639) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294593 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326182 (2.310317) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307302 (2.309540) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298032 (2.308971) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308632 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327878 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 96/100, Train Loss: 2.2786, Train Acc: 0.1661, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.271716 (2.271716) Accuracy: 0.187500 (0.187500)\n",
      "[10/251] Loss: 2.298913 (2.276045) Accuracy: 0.125000 (0.176136)\n",
      "[20/251] Loss: 2.280694 (2.280439) Accuracy: 0.156250 (0.163690)\n",
      "[30/251] Loss: 2.274904 (2.282843) Accuracy: 0.187500 (0.155242)\n",
      "[40/251] Loss: 2.257932 (2.280713) Accuracy: 0.250000 (0.157774)\n",
      "[50/251] Loss: 2.251548 (2.282703) Accuracy: 0.281250 (0.155637)\n",
      "[60/251] Loss: 2.228467 (2.281257) Accuracy: 0.281250 (0.161885)\n",
      "[70/251] Loss: 2.271570 (2.280278) Accuracy: 0.156250 (0.159771)\n",
      "[80/251] Loss: 2.300128 (2.280707) Accuracy: 0.156250 (0.162037)\n",
      "[90/251] Loss: 2.241593 (2.279913) Accuracy: 0.218750 (0.164148)\n",
      "[100/251] Loss: 2.310098 (2.278846) Accuracy: 0.062500 (0.163676)\n",
      "[110/251] Loss: 2.300493 (2.279046) Accuracy: 0.187500 (0.163570)\n",
      "[120/251] Loss: 2.287984 (2.278891) Accuracy: 0.093750 (0.163740)\n",
      "[130/251] Loss: 2.261818 (2.279537) Accuracy: 0.156250 (0.163406)\n",
      "[140/251] Loss: 2.291835 (2.279821) Accuracy: 0.093750 (0.163785)\n",
      "[150/251] Loss: 2.302688 (2.279109) Accuracy: 0.125000 (0.164321)\n",
      "[160/251] Loss: 2.282115 (2.278393) Accuracy: 0.125000 (0.165761)\n",
      "[170/251] Loss: 2.269080 (2.278235) Accuracy: 0.187500 (0.165570)\n",
      "[180/251] Loss: 2.310689 (2.278084) Accuracy: 0.062500 (0.164883)\n",
      "[190/251] Loss: 2.273971 (2.278597) Accuracy: 0.125000 (0.163940)\n",
      "[200/251] Loss: 2.247850 (2.279259) Accuracy: 0.281250 (0.162313)\n",
      "[210/251] Loss: 2.267881 (2.279129) Accuracy: 0.187500 (0.162470)\n",
      "[220/251] Loss: 2.293371 (2.278549) Accuracy: 0.125000 (0.163603)\n",
      "[230/251] Loss: 2.334831 (2.279394) Accuracy: 0.062500 (0.162067)\n",
      "[240/251] Loss: 2.309216 (2.279541) Accuracy: 0.093750 (0.162085)\n",
      "[250/251] Loss: 2.284720 (2.279355) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291639 (2.291639) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294593 (2.311461) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326182 (2.310317) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307301 (2.309540) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298031 (2.308971) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308632 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327878 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 97/100, Train Loss: 2.2794, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.255902 (2.255902) Accuracy: 0.250000 (0.250000)\n",
      "[10/251] Loss: 2.298309 (2.269369) Accuracy: 0.187500 (0.198864)\n",
      "[20/251] Loss: 2.299741 (2.276150) Accuracy: 0.062500 (0.181548)\n",
      "[30/251] Loss: 2.264519 (2.278277) Accuracy: 0.156250 (0.172379)\n",
      "[40/251] Loss: 2.297482 (2.276393) Accuracy: 0.125000 (0.172256)\n",
      "[50/251] Loss: 2.302521 (2.277645) Accuracy: 0.156250 (0.166667)\n",
      "[60/251] Loss: 2.298560 (2.279020) Accuracy: 0.093750 (0.164447)\n",
      "[70/251] Loss: 2.241463 (2.278448) Accuracy: 0.187500 (0.165053)\n",
      "[80/251] Loss: 2.286094 (2.277447) Accuracy: 0.125000 (0.168210)\n",
      "[90/251] Loss: 2.274435 (2.278730) Accuracy: 0.250000 (0.167239)\n",
      "[100/251] Loss: 2.263981 (2.279359) Accuracy: 0.156250 (0.165532)\n",
      "[110/251] Loss: 2.329397 (2.279359) Accuracy: 0.093750 (0.165541)\n",
      "[120/251] Loss: 2.267818 (2.279415) Accuracy: 0.187500 (0.165289)\n",
      "[130/251] Loss: 2.300136 (2.279445) Accuracy: 0.125000 (0.165553)\n",
      "[140/251] Loss: 2.288583 (2.279548) Accuracy: 0.125000 (0.164894)\n",
      "[150/251] Loss: 2.279113 (2.280039) Accuracy: 0.218750 (0.164528)\n",
      "[160/251] Loss: 2.217808 (2.279822) Accuracy: 0.312500 (0.164790)\n",
      "[170/251] Loss: 2.266795 (2.278832) Accuracy: 0.218750 (0.166849)\n",
      "[180/251] Loss: 2.261610 (2.279125) Accuracy: 0.250000 (0.166436)\n",
      "[190/251] Loss: 2.257409 (2.279750) Accuracy: 0.156250 (0.164594)\n",
      "[200/251] Loss: 2.261859 (2.280099) Accuracy: 0.156250 (0.162780)\n",
      "[210/251] Loss: 2.303517 (2.279743) Accuracy: 0.125000 (0.163951)\n",
      "[220/251] Loss: 2.291593 (2.280016) Accuracy: 0.125000 (0.163037)\n",
      "[230/251] Loss: 2.203584 (2.279506) Accuracy: 0.250000 (0.163555)\n",
      "[240/251] Loss: 2.279164 (2.279161) Accuracy: 0.125000 (0.163511)\n",
      "[250/251] Loss: 2.353560 (2.279620) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291639 (2.291639) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294593 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326183 (2.310317) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307302 (2.309540) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298031 (2.308971) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308632 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327878 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 98/100, Train Loss: 2.2796, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.200334 (2.200334) Accuracy: 0.343750 (0.343750)\n",
      "[10/251] Loss: 2.271087 (2.262246) Accuracy: 0.218750 (0.196023)\n",
      "[20/251] Loss: 2.256827 (2.269200) Accuracy: 0.187500 (0.171131)\n",
      "[30/251] Loss: 2.265720 (2.275445) Accuracy: 0.218750 (0.161290)\n",
      "[40/251] Loss: 2.311230 (2.276200) Accuracy: 0.125000 (0.164634)\n",
      "[50/251] Loss: 2.248963 (2.277519) Accuracy: 0.218750 (0.162990)\n",
      "[60/251] Loss: 2.282197 (2.276131) Accuracy: 0.187500 (0.168545)\n",
      "[70/251] Loss: 2.297234 (2.275974) Accuracy: 0.093750 (0.167694)\n",
      "[80/251] Loss: 2.267240 (2.275674) Accuracy: 0.187500 (0.170525)\n",
      "[90/251] Loss: 2.298021 (2.277131) Accuracy: 0.187500 (0.168613)\n",
      "[100/251] Loss: 2.242706 (2.277566) Accuracy: 0.187500 (0.166151)\n",
      "[110/251] Loss: 2.258831 (2.278526) Accuracy: 0.187500 (0.165541)\n",
      "[120/251] Loss: 2.282459 (2.280558) Accuracy: 0.156250 (0.160640)\n",
      "[130/251] Loss: 2.276644 (2.281422) Accuracy: 0.125000 (0.156966)\n",
      "[140/251] Loss: 2.278146 (2.281106) Accuracy: 0.125000 (0.158023)\n",
      "[150/251] Loss: 2.286338 (2.281397) Accuracy: 0.187500 (0.157906)\n",
      "[160/251] Loss: 2.291697 (2.280581) Accuracy: 0.250000 (0.161297)\n",
      "[170/251] Loss: 2.272604 (2.280675) Accuracy: 0.187500 (0.160819)\n",
      "[180/251] Loss: 2.310882 (2.280062) Accuracy: 0.062500 (0.161084)\n",
      "[190/251] Loss: 2.316496 (2.279947) Accuracy: 0.062500 (0.162304)\n",
      "[200/251] Loss: 2.300385 (2.279486) Accuracy: 0.125000 (0.163713)\n",
      "[210/251] Loss: 2.277673 (2.279066) Accuracy: 0.218750 (0.163359)\n",
      "[220/251] Loss: 2.313665 (2.279562) Accuracy: 0.031250 (0.163037)\n",
      "[230/251] Loss: 2.309779 (2.279612) Accuracy: 0.093750 (0.163149)\n",
      "[240/251] Loss: 2.283032 (2.279418) Accuracy: 0.156250 (0.163511)\n",
      "[250/251] Loss: 2.303065 (2.279425) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291639 (2.291639) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294592 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326183 (2.310317) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307302 (2.309540) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298031 (2.308971) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308632 (2.310125) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327878 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 99/100, Train Loss: 2.2794, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "[0/251] Loss: 2.257402 (2.257402) Accuracy: 0.281250 (0.281250)\n",
      "[10/251] Loss: 2.251050 (2.269144) Accuracy: 0.281250 (0.204545)\n",
      "[20/251] Loss: 2.241648 (2.273605) Accuracy: 0.281250 (0.178571)\n",
      "[30/251] Loss: 2.253474 (2.274425) Accuracy: 0.250000 (0.178427)\n",
      "[40/251] Loss: 2.250578 (2.275856) Accuracy: 0.218750 (0.173018)\n",
      "[50/251] Loss: 2.277227 (2.276206) Accuracy: 0.125000 (0.167892)\n",
      "[60/251] Loss: 2.291081 (2.277699) Accuracy: 0.156250 (0.167008)\n",
      "[70/251] Loss: 2.298416 (2.278774) Accuracy: 0.093750 (0.163732)\n",
      "[80/251] Loss: 2.242797 (2.279717) Accuracy: 0.250000 (0.163194)\n",
      "[90/251] Loss: 2.283913 (2.278217) Accuracy: 0.125000 (0.165865)\n",
      "[100/251] Loss: 2.299803 (2.277992) Accuracy: 0.125000 (0.164604)\n",
      "[110/251] Loss: 2.249612 (2.277603) Accuracy: 0.250000 (0.165259)\n",
      "[120/251] Loss: 2.290460 (2.278196) Accuracy: 0.062500 (0.163223)\n",
      "[130/251] Loss: 2.282428 (2.278075) Accuracy: 0.156250 (0.163168)\n",
      "[140/251] Loss: 2.305547 (2.277915) Accuracy: 0.093750 (0.165337)\n",
      "[150/251] Loss: 2.289401 (2.278236) Accuracy: 0.187500 (0.163700)\n",
      "[160/251] Loss: 2.296606 (2.277486) Accuracy: 0.125000 (0.165373)\n",
      "[170/251] Loss: 2.292892 (2.277877) Accuracy: 0.093750 (0.164474)\n",
      "[180/251] Loss: 2.284440 (2.278627) Accuracy: 0.156250 (0.163674)\n",
      "[190/251] Loss: 2.296420 (2.279108) Accuracy: 0.125000 (0.163122)\n",
      "[200/251] Loss: 2.280399 (2.278740) Accuracy: 0.218750 (0.163246)\n",
      "[210/251] Loss: 2.314908 (2.278566) Accuracy: 0.062500 (0.163359)\n",
      "[220/251] Loss: 2.276460 (2.278025) Accuracy: 0.156250 (0.164876)\n",
      "[230/251] Loss: 2.308583 (2.277598) Accuracy: 0.156250 (0.166126)\n",
      "[240/251] Loss: 2.285802 (2.278688) Accuracy: 0.156250 (0.164289)\n",
      "[250/251] Loss: 2.398855 (2.279795) Accuracy: 0.000000 (0.162226)\n",
      "[0/63] Loss: 2.291639 (2.291639) Accuracy: 0.156250 (0.156250)\n",
      "[10/63] Loss: 2.294592 (2.311460) Accuracy: 0.125000 (0.099432)\n",
      "[20/63] Loss: 2.326183 (2.310317) Accuracy: 0.062500 (0.101190)\n",
      "[30/63] Loss: 2.307302 (2.309540) Accuracy: 0.093750 (0.100806)\n",
      "[40/63] Loss: 2.298031 (2.308971) Accuracy: 0.093750 (0.103659)\n",
      "[50/63] Loss: 2.308632 (2.310126) Accuracy: 0.125000 (0.099877)\n",
      "[60/63] Loss: 2.327878 (2.310615) Accuracy: 0.093750 (0.097848)\n",
      "Epoch: 100/100, Train Loss: 2.2798, Train Acc: 0.1622, Val. Loss: 2.3109, Val. Acc: 0.0983\n",
      "Done training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x1000 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from train import main\n",
    "\n",
    "# 配置训练参数的不同配置\n",
    "configs = [\n",
    "    {'input_length': 5, 'input_dim': 1, 'num_classes': 10, 'num_hidden': 128,\n",
    "     'batch_size': 32, 'learning_rate': 0.001, 'max_epoch': 100,\n",
    "     'max_norm': 10.0, 'data_size': 10000, 'portion_train': 0.8},\n",
    "\n",
    "    {'input_length': 12, 'input_dim': 1, 'num_classes': 10, 'num_hidden': 128,\n",
    "     'batch_size': 32, 'learning_rate': 0.001, 'max_epoch': 100,\n",
    "     'max_norm': 10.0, 'data_size': 10000, 'portion_train': 0.8},\n",
    "\n",
    "    {'input_length': 19, 'input_dim': 1, 'num_classes': 10, 'num_hidden': 128,\n",
    "     'batch_size': 32, 'learning_rate': 0.001, 'max_epoch': 100,\n",
    "     'max_norm': 10.0, 'data_size': 10000, 'portion_train': 0.8}\n",
    "]\n",
    "\n",
    "# 存储每个配置的训练和验证损失及准确率\n",
    "results = []\n",
    "\n",
    "# 对每个配置进行训练和评估\n",
    "for config_dict in configs:\n",
    "    class Config:\n",
    "        def __init__(self, **entries):\n",
    "            self.__dict__.update(entries)\n",
    "\n",
    "    config = Config(**config_dict)\n",
    "    train_loss, train_acc, val_loss, val_acc = main(config)\n",
    "    results.append((config.input_length, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "# 绘图\n",
    "epochs = range(1, configs[0]['max_epoch'] + 1)\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# 绘制损失\n",
    "plt.subplot(2, 1, 1)\n",
    "for i, (input_length, train_loss, train_acc, val_loss, val_acc) in enumerate(results):\n",
    "    plt.plot(epochs, train_loss, label=f'Input Length {input_length} Training Loss')\n",
    "    plt.plot(epochs, val_loss, label=f'Input Length {input_length} Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# 绘制准确率\n",
    "plt.subplot(2, 1, 2)\n",
    "for i, (input_length, train_loss, train_acc, val_loss, val_acc) in enumerate(results):\n",
    "    plt.plot(epochs, train_acc, label=f'Input Length {input_length} Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, label=f'Input Length {input_length} Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T06:26:32.412695Z",
     "start_time": "2024-05-15T06:15:47.139992Z"
    }
   },
   "id": "abdac339c21a6ae9",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of dataset:  10000\n",
      "[0/251] Loss: 2.303253 (2.303253) Accuracy: 0.031250 (0.031250)\n",
      "[10/251] Loss: 2.298433 (2.309720) Accuracy: 0.156250 (0.096591)\n",
      "[20/251] Loss: 2.293939 (2.308058) Accuracy: 0.125000 (0.105655)\n",
      "[30/251] Loss: 2.314428 (2.304428) Accuracy: 0.093750 (0.110887)\n",
      "[40/251] Loss: 2.194096 (2.291275) Accuracy: 0.281250 (0.128811)\n",
      "[50/251] Loss: 2.204958 (2.281140) Accuracy: 0.218750 (0.140319)\n",
      "[60/251] Loss: 2.225648 (2.269761) Accuracy: 0.218750 (0.156250)\n",
      "[70/251] Loss: 2.222024 (2.257235) Accuracy: 0.187500 (0.173856)\n",
      "[80/251] Loss: 2.197973 (2.245622) Accuracy: 0.250000 (0.187114)\n",
      "[90/251] Loss: 2.054672 (2.230131) Accuracy: 0.437500 (0.209135)\n",
      "[100/251] Loss: 2.226313 (2.218585) Accuracy: 0.218750 (0.222463)\n",
      "[110/251] Loss: 2.039681 (2.207271) Accuracy: 0.468750 (0.235923)\n",
      "[120/251] Loss: 2.014127 (2.194705) Accuracy: 0.437500 (0.248192)\n",
      "[130/251] Loss: 2.117261 (2.186481) Accuracy: 0.375000 (0.259065)\n",
      "[140/251] Loss: 2.073618 (2.175462) Accuracy: 0.531250 (0.271498)\n",
      "[150/251] Loss: 1.977622 (2.165121) Accuracy: 0.500000 (0.282699)\n",
      "[160/251] Loss: 2.074814 (2.155369) Accuracy: 0.375000 (0.294837)\n",
      "[170/251] Loss: 1.971398 (2.145121) Accuracy: 0.500000 (0.307566)\n",
      "[180/251] Loss: 1.986275 (2.134422) Accuracy: 0.468750 (0.319924)\n",
      "[190/251] Loss: 1.968000 (2.125712) Accuracy: 0.531250 (0.330170)\n",
      "[200/251] Loss: 2.063806 (2.119085) Accuracy: 0.437500 (0.337531)\n",
      "[210/251] Loss: 1.989985 (2.111833) Accuracy: 0.562500 (0.344935)\n",
      "[220/251] Loss: 1.822016 (2.104520) Accuracy: 0.687500 (0.353365)\n",
      "[230/251] Loss: 1.956294 (2.098337) Accuracy: 0.500000 (0.360795)\n",
      "[240/251] Loss: 1.905796 (2.090608) Accuracy: 0.500000 (0.370332)\n",
      "[250/251] Loss: 1.487483 (2.081213) Accuracy: 1.000000 (0.381723)\n",
      "[0/63] Loss: 1.980070 (1.980070) Accuracy: 0.500000 (0.500000)\n",
      "[10/63] Loss: 1.939400 (1.952102) Accuracy: 0.593750 (0.568182)\n",
      "[20/63] Loss: 1.970464 (1.964853) Accuracy: 0.531250 (0.547619)\n",
      "[30/63] Loss: 2.097059 (1.969893) Accuracy: 0.437500 (0.545363)\n",
      "[40/63] Loss: 1.975584 (1.970891) Accuracy: 0.500000 (0.542683)\n",
      "[50/63] Loss: 2.008765 (1.973410) Accuracy: 0.500000 (0.535539)\n",
      "[60/63] Loss: 1.981856 (1.979927) Accuracy: 0.500000 (0.526127)\n",
      "Epoch: 1/100, Train Loss: 2.0812, Train Acc: 0.3817, Val. Loss: 1.9787, Val. Acc: 0.5278\n",
      "[0/251] Loss: 2.011025 (2.011025) Accuracy: 0.500000 (0.500000)\n",
      "[10/251] Loss: 1.912091 (1.951831) Accuracy: 0.531250 (0.542614)\n",
      "[20/251] Loss: 1.803394 (1.918051) Accuracy: 0.656250 (0.566964)\n",
      "[30/251] Loss: 1.899233 (1.898171) Accuracy: 0.562500 (0.585685)\n",
      "[40/251] Loss: 1.857133 (1.889536) Accuracy: 0.625000 (0.593750)\n",
      "[50/251] Loss: 1.776155 (1.890985) Accuracy: 0.718750 (0.590686)\n",
      "[60/251] Loss: 1.980091 (1.885893) Accuracy: 0.437500 (0.594775)\n",
      "[70/251] Loss: 1.959150 (1.883683) Accuracy: 0.500000 (0.596391)\n",
      "[80/251] Loss: 1.765274 (1.880622) Accuracy: 0.718750 (0.599151)\n",
      "[90/251] Loss: 1.877314 (1.878472) Accuracy: 0.562500 (0.597871)\n",
      "[100/251] Loss: 1.904990 (1.880242) Accuracy: 0.562500 (0.593131)\n",
      "[110/251] Loss: 1.763803 (1.878993) Accuracy: 0.718750 (0.593468)\n",
      "[120/251] Loss: 1.842062 (1.874764) Accuracy: 0.593750 (0.596333)\n",
      "[130/251] Loss: 1.784860 (1.871083) Accuracy: 0.718750 (0.598998)\n",
      "[140/251] Loss: 1.879833 (1.870293) Accuracy: 0.625000 (0.600177)\n",
      "[150/251] Loss: 1.814266 (1.868103) Accuracy: 0.625000 (0.602235)\n",
      "[160/251] Loss: 1.817075 (1.866774) Accuracy: 0.656250 (0.603261)\n",
      "[170/251] Loss: 1.890830 (1.865708) Accuracy: 0.531250 (0.604167)\n",
      "[180/251] Loss: 2.004269 (1.864269) Accuracy: 0.468750 (0.605836)\n",
      "[190/251] Loss: 1.839771 (1.862406) Accuracy: 0.625000 (0.608639)\n",
      "[200/251] Loss: 1.816082 (1.862280) Accuracy: 0.656250 (0.609453)\n",
      "[210/251] Loss: 1.869914 (1.859933) Accuracy: 0.687500 (0.612115)\n",
      "[220/251] Loss: 1.717718 (1.856880) Accuracy: 0.750000 (0.615667)\n",
      "[230/251] Loss: 1.909830 (1.856942) Accuracy: 0.593750 (0.616071)\n",
      "[240/251] Loss: 1.961942 (1.854746) Accuracy: 0.468750 (0.618646)\n",
      "[250/251] Loss: 1.466300 (1.851800) Accuracy: 1.000000 (0.621638)\n",
      "[0/63] Loss: 1.724908 (1.724908) Accuracy: 0.750000 (0.750000)\n",
      "[10/63] Loss: 1.735399 (1.758159) Accuracy: 0.750000 (0.730114)\n",
      "[20/63] Loss: 1.797793 (1.777106) Accuracy: 0.718750 (0.706845)\n",
      "[30/63] Loss: 1.927363 (1.791046) Accuracy: 0.531250 (0.688508)\n",
      "[40/63] Loss: 1.861927 (1.787028) Accuracy: 0.625000 (0.695122)\n",
      "[50/63] Loss: 1.812796 (1.784515) Accuracy: 0.687500 (0.702819)\n",
      "[60/63] Loss: 1.759664 (1.785428) Accuracy: 0.781250 (0.704918)\n",
      "Epoch: 2/100, Train Loss: 1.8518, Train Acc: 0.6216, Val. Loss: 1.7874, Val. Acc: 0.7009\n",
      "[0/251] Loss: 1.745504 (1.745504) Accuracy: 0.750000 (0.750000)\n",
      "[10/251] Loss: 2.035413 (1.807421) Accuracy: 0.375000 (0.693182)\n",
      "[20/251] Loss: 1.752061 (1.808834) Accuracy: 0.718750 (0.683036)\n",
      "[30/251] Loss: 1.674144 (1.798472) Accuracy: 0.843750 (0.691532)\n",
      "[40/251] Loss: 1.856120 (1.790305) Accuracy: 0.593750 (0.696646)\n",
      "[50/251] Loss: 1.823623 (1.791712) Accuracy: 0.656250 (0.694853)\n",
      "[60/251] Loss: 1.769526 (1.792036) Accuracy: 0.656250 (0.695184)\n",
      "[70/251] Loss: 1.797357 (1.780257) Accuracy: 0.718750 (0.711268)\n",
      "[80/251] Loss: 1.981156 (1.777691) Accuracy: 0.406250 (0.715278)\n",
      "[90/251] Loss: 1.676645 (1.775078) Accuracy: 0.843750 (0.719093)\n",
      "[100/251] Loss: 1.726646 (1.771543) Accuracy: 0.812500 (0.723700)\n",
      "[110/251] Loss: 1.697974 (1.764414) Accuracy: 0.812500 (0.735079)\n",
      "[120/251] Loss: 1.698080 (1.758820) Accuracy: 0.812500 (0.742252)\n",
      "[130/251] Loss: 1.749723 (1.753630) Accuracy: 0.750000 (0.749046)\n",
      "[140/251] Loss: 1.766459 (1.750059) Accuracy: 0.750000 (0.752660)\n",
      "[150/251] Loss: 1.735520 (1.746532) Accuracy: 0.718750 (0.756829)\n",
      "[160/251] Loss: 1.612072 (1.742122) Accuracy: 0.875000 (0.761064)\n",
      "[170/251] Loss: 2.161308 (1.742588) Accuracy: 0.281250 (0.758955)\n",
      "[180/251] Loss: 1.750387 (1.745321) Accuracy: 0.718750 (0.754834)\n",
      "[190/251] Loss: 1.623024 (1.744291) Accuracy: 0.906250 (0.755726)\n",
      "[200/251] Loss: 1.732657 (1.741552) Accuracy: 0.718750 (0.757618)\n",
      "[210/251] Loss: 1.559904 (1.736906) Accuracy: 0.937500 (0.761404)\n",
      "[220/251] Loss: 1.577763 (1.732454) Accuracy: 0.906250 (0.765271)\n",
      "[230/251] Loss: 1.692917 (1.729060) Accuracy: 0.812500 (0.768669)\n",
      "[240/251] Loss: 1.619002 (1.726374) Accuracy: 0.843750 (0.771006)\n",
      "[250/251] Loss: 1.512833 (1.723470) Accuracy: 1.000000 (0.773157)\n",
      "[0/63] Loss: 1.634289 (1.634289) Accuracy: 0.875000 (0.875000)\n",
      "[10/63] Loss: 1.679358 (1.662544) Accuracy: 0.781250 (0.829545)\n",
      "[20/63] Loss: 1.742535 (1.666859) Accuracy: 0.718750 (0.834821)\n",
      "[30/63] Loss: 1.637444 (1.671896) Accuracy: 0.906250 (0.829637)\n",
      "[40/63] Loss: 1.766052 (1.670665) Accuracy: 0.718750 (0.830793)\n",
      "[50/63] Loss: 1.644180 (1.670503) Accuracy: 0.875000 (0.829044)\n",
      "[60/63] Loss: 1.699863 (1.672101) Accuracy: 0.750000 (0.825307)\n",
      "Epoch: 3/100, Train Loss: 1.7235, Train Acc: 0.7732, Val. Loss: 1.6718, Val. Acc: 0.8263\n",
      "[0/251] Loss: 1.676377 (1.676377) Accuracy: 0.843750 (0.843750)\n",
      "[10/251] Loss: 1.645582 (1.631108) Accuracy: 0.843750 (0.875000)\n",
      "[20/251] Loss: 1.626897 (1.624814) Accuracy: 0.875000 (0.877976)\n",
      "[30/251] Loss: 1.607453 (1.625827) Accuracy: 0.906250 (0.881048)\n",
      "[40/251] Loss: 1.682578 (1.629822) Accuracy: 0.812500 (0.875000)\n",
      "[50/251] Loss: 1.609698 (1.629741) Accuracy: 0.968750 (0.876225)\n",
      "[60/251] Loss: 1.772176 (1.632390) Accuracy: 0.687500 (0.874488)\n",
      "[70/251] Loss: 1.613840 (1.630703) Accuracy: 0.906250 (0.875880)\n",
      "[80/251] Loss: 1.691592 (1.627655) Accuracy: 0.781250 (0.878472)\n",
      "[90/251] Loss: 1.567299 (1.628767) Accuracy: 0.968750 (0.879808)\n",
      "[100/251] Loss: 1.596855 (1.625745) Accuracy: 0.906250 (0.883973)\n",
      "[110/251] Loss: 1.635765 (1.623180) Accuracy: 0.843750 (0.887950)\n",
      "[120/251] Loss: 1.675160 (1.623842) Accuracy: 0.937500 (0.890754)\n",
      "[130/251] Loss: 1.589220 (1.627329) Accuracy: 0.875000 (0.888120)\n",
      "[140/251] Loss: 1.603774 (1.623683) Accuracy: 0.937500 (0.892509)\n",
      "[150/251] Loss: 1.597170 (1.622284) Accuracy: 0.968750 (0.894454)\n",
      "[160/251] Loss: 1.584613 (1.620267) Accuracy: 0.937500 (0.897321)\n",
      "[170/251] Loss: 1.613390 (1.617067) Accuracy: 0.843750 (0.900950)\n",
      "[180/251] Loss: 1.673374 (1.618098) Accuracy: 0.781250 (0.898481)\n",
      "[190/251] Loss: 1.576080 (1.615796) Accuracy: 0.968750 (0.901996)\n",
      "[200/251] Loss: 1.532697 (1.616154) Accuracy: 1.000000 (0.901586)\n",
      "[210/251] Loss: 1.539348 (1.614384) Accuracy: 1.000000 (0.904473)\n",
      "[220/251] Loss: 1.551335 (1.612501) Accuracy: 0.906250 (0.906533)\n",
      "[230/251] Loss: 1.603821 (1.610641) Accuracy: 0.968750 (0.909091)\n",
      "[240/251] Loss: 1.596078 (1.608089) Accuracy: 0.875000 (0.911048)\n",
      "[250/251] Loss: 1.492640 (1.605362) Accuracy: 1.000000 (0.914467)\n",
      "[0/63] Loss: 1.521222 (1.521222) Accuracy: 0.968750 (0.968750)\n",
      "[10/63] Loss: 1.565589 (1.550088) Accuracy: 0.937500 (0.974432)\n",
      "[20/63] Loss: 1.567588 (1.546665) Accuracy: 1.000000 (0.985119)\n",
      "[30/63] Loss: 1.536571 (1.548014) Accuracy: 1.000000 (0.989919)\n",
      "[40/63] Loss: 1.562677 (1.546444) Accuracy: 0.968750 (0.989329)\n",
      "[50/63] Loss: 1.539291 (1.545015) Accuracy: 0.968750 (0.990196)\n",
      "[60/63] Loss: 1.535722 (1.543702) Accuracy: 1.000000 (0.990779)\n",
      "Epoch: 4/100, Train Loss: 1.6054, Train Acc: 0.9145, Val. Loss: 1.5438, Val. Acc: 0.9911\n",
      "[0/251] Loss: 1.531792 (1.531792) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.511725 (1.532069) Accuracy: 1.000000 (0.997159)\n",
      "[20/251] Loss: 1.517045 (1.540835) Accuracy: 1.000000 (0.983631)\n",
      "[30/251] Loss: 1.518241 (1.535538) Accuracy: 1.000000 (0.987903)\n",
      "[40/251] Loss: 1.562672 (1.533412) Accuracy: 0.937500 (0.989329)\n",
      "[50/251] Loss: 1.546057 (1.550694) Accuracy: 0.968750 (0.965686)\n",
      "[60/251] Loss: 1.519022 (1.545931) Accuracy: 1.000000 (0.970799)\n",
      "[70/251] Loss: 1.510225 (1.542343) Accuracy: 1.000000 (0.973151)\n",
      "[80/251] Loss: 1.512351 (1.539225) Accuracy: 1.000000 (0.976080)\n",
      "[90/251] Loss: 1.502430 (1.538112) Accuracy: 1.000000 (0.976305)\n",
      "[100/251] Loss: 1.509303 (1.535618) Accuracy: 1.000000 (0.978342)\n",
      "[110/251] Loss: 1.499015 (1.532866) Accuracy: 1.000000 (0.980293)\n",
      "[120/251] Loss: 1.502555 (1.530132) Accuracy: 1.000000 (0.981921)\n",
      "[130/251] Loss: 1.502806 (1.528110) Accuracy: 1.000000 (0.983302)\n",
      "[140/251] Loss: 1.488469 (1.526088) Accuracy: 1.000000 (0.984486)\n",
      "[150/251] Loss: 1.497423 (1.524421) Accuracy: 1.000000 (0.985513)\n",
      "[160/251] Loss: 1.488675 (1.522649) Accuracy: 1.000000 (0.986413)\n",
      "[170/251] Loss: 1.484070 (1.520980) Accuracy: 1.000000 (0.987208)\n",
      "[180/251] Loss: 1.799819 (1.527275) Accuracy: 0.687500 (0.979109)\n",
      "[190/251] Loss: 1.488270 (1.531255) Accuracy: 1.000000 (0.973168)\n",
      "[200/251] Loss: 1.497267 (1.529302) Accuracy: 1.000000 (0.974502)\n",
      "[210/251] Loss: 1.491029 (1.527375) Accuracy: 1.000000 (0.975711)\n",
      "[220/251] Loss: 1.483165 (1.525543) Accuracy: 1.000000 (0.976810)\n",
      "[230/251] Loss: 1.489088 (1.523850) Accuracy: 1.000000 (0.977814)\n",
      "[240/251] Loss: 1.504832 (1.522551) Accuracy: 1.000000 (0.978734)\n",
      "[250/251] Loss: 1.476925 (1.523380) Accuracy: 1.000000 (0.976718)\n",
      "[0/63] Loss: 1.480483 (1.480483) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.493278 (1.485817) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.493011 (1.485651) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.484342 (1.486136) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.495583 (1.486134) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.486203 (1.486198) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.485217 (1.485955) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 5/100, Train Loss: 1.5234, Train Acc: 0.9767, Val. Loss: 1.4860, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.482747 (1.482747) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.489148 (1.484036) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.475974 (1.483051) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.478700 (1.482326) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.484761 (1.481767) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.483445 (1.481516) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.477546 (1.481138) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.483744 (1.480648) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.475969 (1.480225) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.490551 (1.480011) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.644458 (1.503518) Accuracy: 0.843750 (0.975557)\n",
      "[110/251] Loss: 1.477876 (1.506213) Accuracy: 1.000000 (0.973255)\n",
      "[120/251] Loss: 1.471794 (1.503664) Accuracy: 1.000000 (0.975465)\n",
      "[130/251] Loss: 1.473356 (1.501493) Accuracy: 1.000000 (0.977338)\n",
      "[140/251] Loss: 1.471211 (1.499595) Accuracy: 1.000000 (0.978945)\n",
      "[150/251] Loss: 1.471789 (1.497809) Accuracy: 1.000000 (0.980339)\n",
      "[160/251] Loss: 1.472126 (1.496303) Accuracy: 1.000000 (0.981561)\n",
      "[170/251] Loss: 1.473572 (1.494999) Accuracy: 1.000000 (0.982639)\n",
      "[180/251] Loss: 1.471236 (1.493712) Accuracy: 1.000000 (0.983598)\n",
      "[190/251] Loss: 1.475252 (1.492672) Accuracy: 1.000000 (0.984457)\n",
      "[200/251] Loss: 1.472253 (1.491639) Accuracy: 1.000000 (0.985230)\n",
      "[210/251] Loss: 1.472167 (1.490680) Accuracy: 1.000000 (0.985930)\n",
      "[220/251] Loss: 1.473126 (1.489794) Accuracy: 1.000000 (0.986567)\n",
      "[230/251] Loss: 1.467820 (1.488973) Accuracy: 1.000000 (0.987148)\n",
      "[240/251] Loss: 1.470765 (1.488192) Accuracy: 1.000000 (0.987682)\n",
      "[250/251] Loss: 1.464406 (1.487471) Accuracy: 1.000000 (0.988172)\n",
      "[0/63] Loss: 1.468744 (1.468744) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.472434 (1.470042) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.471584 (1.469818) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.470312 (1.469933) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.472088 (1.469933) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.470259 (1.469897) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.468238 (1.469847) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 6/100, Train Loss: 1.4875, Train Acc: 0.9882, Val. Loss: 1.4698, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.469833 (1.469833) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.467775 (1.469514) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.466069 (1.469077) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.468038 (1.468712) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.467645 (1.468557) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.469337 (1.468606) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.468244 (1.468558) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.467723 (1.468398) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.468462 (1.468349) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.469523 (1.468222) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.468698 (1.468093) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.466506 (1.467991) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.467107 (1.467878) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.464598 (1.467760) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.466256 (1.467661) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.465476 (1.467527) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.465164 (1.467408) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.465828 (1.467296) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.464127 (1.467182) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.465185 (1.467059) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.464652 (1.466955) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.465192 (1.466858) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.465240 (1.466779) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.465285 (1.466673) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.463772 (1.466568) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.471654 (1.466504) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.464719 (1.464719) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.471766 (1.469574) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.477872 (1.471134) Accuracy: 1.000000 (0.997024)\n",
      "[30/63] Loss: 1.465505 (1.471203) Accuracy: 1.000000 (0.995968)\n",
      "[40/63] Loss: 1.474878 (1.471084) Accuracy: 1.000000 (0.996951)\n",
      "[50/63] Loss: 1.470070 (1.472216) Accuracy: 1.000000 (0.996936)\n",
      "[60/63] Loss: 1.482579 (1.472053) Accuracy: 1.000000 (0.996926)\n",
      "Epoch: 7/100, Train Loss: 1.4665, Train Acc: 1.0000, Val. Loss: 1.4721, Val. Acc: 0.9970\n",
      "[0/251] Loss: 1.494023 (1.494023) Accuracy: 0.968750 (0.968750)\n",
      "[10/251] Loss: 1.697233 (1.722303) Accuracy: 0.750000 (0.735795)\n",
      "[20/251] Loss: 1.543619 (1.666378) Accuracy: 0.906250 (0.791667)\n",
      "[30/251] Loss: 1.464794 (1.605831) Accuracy: 1.000000 (0.854839)\n",
      "[40/251] Loss: 1.464731 (1.571422) Accuracy: 1.000000 (0.890244)\n",
      "[50/251] Loss: 1.463842 (1.550452) Accuracy: 1.000000 (0.911765)\n",
      "[60/251] Loss: 1.464183 (1.536281) Accuracy: 1.000000 (0.926230)\n",
      "[70/251] Loss: 1.464677 (1.526112) Accuracy: 1.000000 (0.936620)\n",
      "[80/251] Loss: 1.462680 (1.518445) Accuracy: 1.000000 (0.944444)\n",
      "[90/251] Loss: 1.464150 (1.512467) Accuracy: 1.000000 (0.950549)\n",
      "[100/251] Loss: 1.464294 (1.507654) Accuracy: 1.000000 (0.955446)\n",
      "[110/251] Loss: 1.464600 (1.503694) Accuracy: 1.000000 (0.959459)\n",
      "[120/251] Loss: 1.464184 (1.500405) Accuracy: 1.000000 (0.962810)\n",
      "[130/251] Loss: 1.462961 (1.497614) Accuracy: 1.000000 (0.965649)\n",
      "[140/251] Loss: 1.464396 (1.495197) Accuracy: 1.000000 (0.968085)\n",
      "[150/251] Loss: 1.463062 (1.493101) Accuracy: 1.000000 (0.970199)\n",
      "[160/251] Loss: 1.463772 (1.491276) Accuracy: 1.000000 (0.972050)\n",
      "[170/251] Loss: 1.462939 (1.489654) Accuracy: 1.000000 (0.973684)\n",
      "[180/251] Loss: 1.463084 (1.488207) Accuracy: 1.000000 (0.975138)\n",
      "[190/251] Loss: 1.463116 (1.486900) Accuracy: 1.000000 (0.976440)\n",
      "[200/251] Loss: 1.463492 (1.485737) Accuracy: 1.000000 (0.977612)\n",
      "[210/251] Loss: 1.463231 (1.484672) Accuracy: 1.000000 (0.978673)\n",
      "[220/251] Loss: 1.463223 (1.483695) Accuracy: 1.000000 (0.979638)\n",
      "[230/251] Loss: 1.462664 (1.482812) Accuracy: 1.000000 (0.980519)\n",
      "[240/251] Loss: 1.463883 (1.481996) Accuracy: 1.000000 (0.981328)\n",
      "[250/251] Loss: 1.465537 (1.481252) Accuracy: 1.000000 (0.982072)\n",
      "[0/63] Loss: 1.462932 (1.462932) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.463503 (1.463142) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.463434 (1.463116) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.463639 (1.463152) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.463377 (1.463151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.463060 (1.463133) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.462683 (1.463126) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 8/100, Train Loss: 1.4813, Train Acc: 0.9821, Val. Loss: 1.4631, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.463163 (1.463163) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.463712 (1.463008) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.462549 (1.462920) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.463090 (1.462963) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.462720 (1.462936) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.462733 (1.462924) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.462504 (1.462910) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.462797 (1.462894) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.462519 (1.462870) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.462778 (1.462839) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.462481 (1.462817) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.462233 (1.462792) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.462544 (1.462779) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.462772 (1.462765) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.462613 (1.462749) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.462312 (1.462730) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.462145 (1.462711) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.462548 (1.462690) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461858 (1.462664) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.462166 (1.462643) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.462594 (1.462625) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.462144 (1.462607) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.462247 (1.462591) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.462107 (1.462569) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.462128 (1.462556) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.465109 (1.462551) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.462130 (1.462130) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.462959 (1.462431) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.462969 (1.462391) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.462351 (1.462387) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.463046 (1.462404) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.462761 (1.462435) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.462297 (1.462426) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 9/100, Train Loss: 1.4626, Train Acc: 1.0000, Val. Loss: 1.4624, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.462163 (1.462163) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.462000 (1.462232) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.462051 (1.462146) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461986 (1.462105) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461853 (1.462068) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461925 (1.462054) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.462119 (1.462034) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461706 (1.462022) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461859 (1.462003) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461786 (1.461990) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461844 (1.461972) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461859 (1.461962) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461938 (1.461948) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461961 (1.461937) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461779 (1.461928) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461848 (1.461917) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461894 (1.461909) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461666 (1.461895) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461787 (1.461888) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461662 (1.461878) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461634 (1.461866) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461580 (1.461856) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461465 (1.461845) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461699 (1.461836) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461441 (1.461825) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461707 (1.461815) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461564 (1.461564) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461674 (1.461587) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461644 (1.461581) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461691 (1.461590) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461643 (1.461593) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461595 (1.461589) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461494 (1.461591) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 10/100, Train Loss: 1.4618, Train Acc: 1.0000, Val. Loss: 1.4616, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461723 (1.461723) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461463 (1.461573) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461561 (1.461569) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461556 (1.461546) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461573 (1.461540) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461526 (1.461542) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461594 (1.461541) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461559 (1.461537) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461415 (1.461529) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461548 (1.461526) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461492 (1.461522) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461420 (1.461513) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461486 (1.461508) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461421 (1.461502) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461379 (1.461497) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461395 (1.461492) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461411 (1.461486) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461411 (1.461480) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461389 (1.461475) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461382 (1.461470) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461432 (1.461466) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461329 (1.461461) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461350 (1.461456) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461351 (1.461451) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461372 (1.461447) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461515 (1.461443) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461323 (1.461323) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461378 (1.461338) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461390 (1.461337) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461381 (1.461344) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461365 (1.461345) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461351 (1.461345) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461290 (1.461343) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 11/100, Train Loss: 1.4614, Train Acc: 1.0000, Val. Loss: 1.4613, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461342 (1.461342) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461390 (1.461349) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461310 (1.461338) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461309 (1.461330) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461334 (1.461326) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461290 (1.461320) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461285 (1.461317) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461341 (1.461314) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461311 (1.461312) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461283 (1.461309) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461321 (1.461306) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461283 (1.461304) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461268 (1.461302) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461298 (1.461300) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461308 (1.461298) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461289 (1.461296) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461235 (1.461294) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461229 (1.461291) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461225 (1.461289) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461212 (1.461287) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461246 (1.461284) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461247 (1.461282) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461231 (1.461280) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461221 (1.461278) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461219 (1.461276) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461152 (1.461273) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461221 (1.461221) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461244 (1.461226) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461241 (1.461226) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461249 (1.461228) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461238 (1.461229) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461230 (1.461229) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461208 (1.461228) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 12/100, Train Loss: 1.4613, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461214 (1.461214) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461196 (1.461225) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461232 (1.461224) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461206 (1.461221) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461213 (1.461220) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461217 (1.461219) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461218 (1.461219) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461217 (1.461218) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461229 (1.461217) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461214 (1.461216) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461199 (1.461215) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461205 (1.461214) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461197 (1.461212) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461200 (1.461212) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461199 (1.461211) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461181 (1.461210) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461184 (1.461209) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461195 (1.461208) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461193 (1.461207) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461185 (1.461206) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461180 (1.461205) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461190 (1.461205) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461187 (1.461204) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461179 (1.461203) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461177 (1.461202) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461206 (1.461201) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461180 (1.461180) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461191 (1.461182) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461191 (1.461182) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461189 (1.461183) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461187 (1.461183) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461185 (1.461183) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461174 (1.461183) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 13/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461176 (1.461176) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461184 (1.461181) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461187 (1.461180) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461177 (1.461179) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461179 (1.461179) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461173 (1.461178) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461180 (1.461178) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461171 (1.461178) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461174 (1.461178) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461169 (1.461177) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461184 (1.461177) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461179 (1.461176) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461171 (1.461176) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461171 (1.461176) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461168 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461165 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461162 (1.461174) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461168 (1.461174) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461169 (1.461174) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461169 (1.461173) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461169 (1.461173) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461163 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461170 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461162 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461159 (1.461171) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461153 (1.461171) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461162 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461166 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461166 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461166 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461163 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461164 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461160 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 14/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461164 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461163 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461163 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461162 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461163 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461162 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461161 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461160 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461158 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461162 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461159 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461157 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461160 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461159 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461156 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461159 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461158 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461156 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461157 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461157 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461157 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461156 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461156 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461155 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461157 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461159 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461157 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461158 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461154 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 15/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461157 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461152 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461152 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461155 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 16/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461153 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461153 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461153 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 17/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461152 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 18/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 19/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 20/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 21/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 22/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 23/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 24/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 25/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 26/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 27/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 28/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 29/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 30/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 31/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 32/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 33/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 34/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 35/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 36/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 37/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 38/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 39/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 40/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 41/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 42/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 43/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 44/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 45/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 46/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 47/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 48/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 49/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 50/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 51/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 52/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 53/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 54/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 55/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 56/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 57/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 58/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 59/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 60/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 61/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 62/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 63/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 64/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 65/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 66/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 67/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 68/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 69/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 70/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 71/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 72/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 73/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 74/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 75/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 76/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 77/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 78/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 79/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 80/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 81/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 82/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 83/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 84/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 85/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 86/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 87/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 88/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 89/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 90/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 91/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 92/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 93/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 94/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 95/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 96/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 97/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 98/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 99/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 100/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "Done training.\n",
      "Total length of dataset:  10000\n",
      "[0/251] Loss: 2.297417 (2.297417) Accuracy: 0.125000 (0.125000)\n",
      "[10/251] Loss: 2.310038 (2.311424) Accuracy: 0.031250 (0.088068)\n",
      "[20/251] Loss: 2.250686 (2.296187) Accuracy: 0.187500 (0.120536)\n",
      "[30/251] Loss: 2.226024 (2.287730) Accuracy: 0.281250 (0.133065)\n",
      "[40/251] Loss: 2.301672 (2.283570) Accuracy: 0.093750 (0.137195)\n",
      "[50/251] Loss: 2.181477 (2.271816) Accuracy: 0.312500 (0.153799)\n",
      "[60/251] Loss: 2.131289 (2.258099) Accuracy: 0.281250 (0.172131)\n",
      "[70/251] Loss: 2.095247 (2.244935) Accuracy: 0.468750 (0.191461)\n",
      "[80/251] Loss: 2.185100 (2.228999) Accuracy: 0.281250 (0.211420)\n",
      "[90/251] Loss: 2.013300 (2.213964) Accuracy: 0.531250 (0.232830)\n",
      "[100/251] Loss: 2.072921 (2.200094) Accuracy: 0.468750 (0.250928)\n",
      "[110/251] Loss: 1.977392 (2.184595) Accuracy: 0.531250 (0.273367)\n",
      "[120/251] Loss: 1.952878 (2.173275) Accuracy: 0.562500 (0.287448)\n",
      "[130/251] Loss: 2.021107 (2.163739) Accuracy: 0.593750 (0.300095)\n",
      "[140/251] Loss: 2.011264 (2.151448) Accuracy: 0.562500 (0.318262)\n",
      "[150/251] Loss: 1.934811 (2.137276) Accuracy: 0.625000 (0.337748)\n",
      "[160/251] Loss: 1.827352 (2.122847) Accuracy: 0.750000 (0.357143)\n",
      "[170/251] Loss: 1.821804 (2.111372) Accuracy: 0.687500 (0.371711)\n",
      "[180/251] Loss: 1.922609 (2.099340) Accuracy: 0.656250 (0.387776)\n",
      "[190/251] Loss: 1.852277 (2.087227) Accuracy: 0.750000 (0.404287)\n",
      "[200/251] Loss: 1.984117 (2.075728) Accuracy: 0.531250 (0.418377)\n",
      "[210/251] Loss: 1.858993 (2.063299) Accuracy: 0.687500 (0.433501)\n",
      "[220/251] Loss: 1.817247 (2.053509) Accuracy: 0.718750 (0.444853)\n",
      "[230/251] Loss: 1.828526 (2.043182) Accuracy: 0.656250 (0.455898)\n",
      "[240/251] Loss: 1.684740 (2.033437) Accuracy: 0.843750 (0.467064)\n",
      "[250/251] Loss: 2.388982 (2.026351) Accuracy: 0.000000 (0.474975)\n",
      "[0/63] Loss: 1.750094 (1.750094) Accuracy: 0.781250 (0.781250)\n",
      "[10/63] Loss: 1.865365 (1.858132) Accuracy: 0.656250 (0.625000)\n",
      "[20/63] Loss: 1.781687 (1.859771) Accuracy: 0.718750 (0.625000)\n",
      "[30/63] Loss: 1.817221 (1.845660) Accuracy: 0.687500 (0.645161)\n",
      "[40/63] Loss: 1.829870 (1.847170) Accuracy: 0.687500 (0.643293)\n",
      "[50/63] Loss: 1.917655 (1.848727) Accuracy: 0.531250 (0.639093)\n",
      "[60/63] Loss: 1.876554 (1.849411) Accuracy: 0.562500 (0.639344)\n",
      "Epoch: 1/100, Train Loss: 2.0264, Train Acc: 0.4750, Val. Loss: 1.8501, Val. Acc: 0.6397\n",
      "[0/251] Loss: 1.830896 (1.830896) Accuracy: 0.750000 (0.750000)\n",
      "[10/251] Loss: 1.877424 (1.770248) Accuracy: 0.625000 (0.769886)\n",
      "[20/251] Loss: 1.836344 (1.768845) Accuracy: 0.656250 (0.769345)\n",
      "[30/251] Loss: 1.690766 (1.757299) Accuracy: 0.843750 (0.776210)\n",
      "[40/251] Loss: 1.765031 (1.753893) Accuracy: 0.781250 (0.775915)\n",
      "[50/251] Loss: 1.798734 (1.756759) Accuracy: 0.718750 (0.775123)\n",
      "[60/251] Loss: 1.674215 (1.747450) Accuracy: 0.875000 (0.785348)\n",
      "[70/251] Loss: 1.643741 (1.739716) Accuracy: 0.875000 (0.791373)\n",
      "[80/251] Loss: 1.705946 (1.734542) Accuracy: 0.812500 (0.794753)\n",
      "[90/251] Loss: 1.633634 (1.734759) Accuracy: 0.875000 (0.792239)\n",
      "[100/251] Loss: 1.677864 (1.733587) Accuracy: 0.812500 (0.790532)\n",
      "[110/251] Loss: 1.775939 (1.732100) Accuracy: 0.718750 (0.789414)\n",
      "[120/251] Loss: 1.707200 (1.731084) Accuracy: 0.781250 (0.788223)\n",
      "[130/251] Loss: 1.683788 (1.729186) Accuracy: 0.812500 (0.788645)\n",
      "[140/251] Loss: 1.734865 (1.725702) Accuracy: 0.750000 (0.791223)\n",
      "[150/251] Loss: 1.602255 (1.721688) Accuracy: 0.906250 (0.795530)\n",
      "[160/251] Loss: 1.671422 (1.717799) Accuracy: 0.906250 (0.799107)\n",
      "[170/251] Loss: 1.707860 (1.714311) Accuracy: 0.781250 (0.802266)\n",
      "[180/251] Loss: 1.741228 (1.714236) Accuracy: 0.781250 (0.800932)\n",
      "[190/251] Loss: 1.578186 (1.710284) Accuracy: 0.968750 (0.805465)\n",
      "[200/251] Loss: 1.666030 (1.708141) Accuracy: 0.875000 (0.807680)\n",
      "[210/251] Loss: 1.646022 (1.705582) Accuracy: 0.875000 (0.811463)\n",
      "[220/251] Loss: 1.577740 (1.702066) Accuracy: 0.937500 (0.815187)\n",
      "[230/251] Loss: 1.646130 (1.698143) Accuracy: 0.906250 (0.819399)\n",
      "[240/251] Loss: 1.551408 (1.694186) Accuracy: 0.968750 (0.823133)\n",
      "[250/251] Loss: 1.472119 (1.690747) Accuracy: 1.000000 (0.826071)\n",
      "[0/63] Loss: 1.586756 (1.586756) Accuracy: 0.906250 (0.906250)\n",
      "[10/63] Loss: 1.595877 (1.595577) Accuracy: 0.937500 (0.920455)\n",
      "[20/63] Loss: 1.588530 (1.607573) Accuracy: 0.906250 (0.903274)\n",
      "[30/63] Loss: 1.629785 (1.601553) Accuracy: 0.875000 (0.907258)\n",
      "[40/63] Loss: 1.656967 (1.604017) Accuracy: 0.843750 (0.904726)\n",
      "[50/63] Loss: 1.619636 (1.602955) Accuracy: 0.906250 (0.906250)\n",
      "[60/63] Loss: 1.600196 (1.600419) Accuracy: 0.906250 (0.909324)\n",
      "Epoch: 2/100, Train Loss: 1.6907, Train Acc: 0.8261, Val. Loss: 1.5990, Val. Acc: 0.9111\n",
      "[0/251] Loss: 1.627320 (1.627320) Accuracy: 0.875000 (0.875000)\n",
      "[10/251] Loss: 1.640428 (1.602521) Accuracy: 0.843750 (0.903409)\n",
      "[20/251] Loss: 1.562590 (1.603175) Accuracy: 0.937500 (0.900298)\n",
      "[30/251] Loss: 1.590295 (1.602159) Accuracy: 0.906250 (0.899194)\n",
      "[40/251] Loss: 1.592403 (1.603275) Accuracy: 0.906250 (0.895579)\n",
      "[50/251] Loss: 1.530198 (1.596416) Accuracy: 0.968750 (0.901348)\n",
      "[60/251] Loss: 1.528778 (1.591638) Accuracy: 0.968750 (0.904713)\n",
      "[70/251] Loss: 1.554813 (1.588120) Accuracy: 0.937500 (0.907130)\n",
      "[80/251] Loss: 1.545751 (1.586866) Accuracy: 0.937500 (0.907022)\n",
      "[90/251] Loss: 1.540906 (1.586210) Accuracy: 0.937500 (0.906593)\n",
      "[100/251] Loss: 1.578538 (1.588009) Accuracy: 0.906250 (0.903775)\n",
      "[110/251] Loss: 1.576187 (1.587766) Accuracy: 0.906250 (0.902872)\n",
      "[120/251] Loss: 1.601313 (1.588000) Accuracy: 0.875000 (0.901860)\n",
      "[130/251] Loss: 1.544330 (1.586049) Accuracy: 0.937500 (0.902910)\n",
      "[140/251] Loss: 1.569670 (1.584278) Accuracy: 0.906250 (0.903812)\n",
      "[150/251] Loss: 1.506537 (1.582408) Accuracy: 0.968750 (0.904801)\n",
      "[160/251] Loss: 1.508324 (1.581352) Accuracy: 0.968750 (0.905085)\n",
      "[170/251] Loss: 1.657739 (1.581750) Accuracy: 0.812500 (0.903874)\n",
      "[180/251] Loss: 1.533805 (1.581982) Accuracy: 0.937500 (0.902797)\n",
      "[190/251] Loss: 1.686560 (1.581944) Accuracy: 0.781250 (0.902160)\n",
      "[200/251] Loss: 1.625749 (1.582540) Accuracy: 0.843750 (0.900808)\n",
      "[210/251] Loss: 1.531552 (1.582448) Accuracy: 0.937500 (0.900178)\n",
      "[220/251] Loss: 1.595085 (1.580633) Accuracy: 0.875000 (0.901442)\n",
      "[230/251] Loss: 1.565842 (1.580953) Accuracy: 0.906250 (0.900568)\n",
      "[240/251] Loss: 1.470672 (1.579085) Accuracy: 1.000000 (0.901971)\n",
      "[250/251] Loss: 1.474864 (1.577299) Accuracy: 1.000000 (0.903262)\n",
      "[0/63] Loss: 1.561732 (1.561732) Accuracy: 0.906250 (0.906250)\n",
      "[10/63] Loss: 1.533914 (1.549965) Accuracy: 0.937500 (0.920455)\n",
      "[20/63] Loss: 1.562323 (1.565963) Accuracy: 0.906250 (0.903274)\n",
      "[30/63] Loss: 1.591176 (1.561757) Accuracy: 0.875000 (0.907258)\n",
      "[40/63] Loss: 1.622879 (1.564315) Accuracy: 0.843750 (0.904726)\n",
      "[50/63] Loss: 1.563185 (1.562835) Accuracy: 0.906250 (0.906250)\n",
      "[60/63] Loss: 1.561320 (1.559927) Accuracy: 0.906250 (0.909324)\n",
      "Epoch: 3/100, Train Loss: 1.5773, Train Acc: 0.9033, Val. Loss: 1.5582, Val. Acc: 0.9111\n",
      "[0/251] Loss: 1.531951 (1.531951) Accuracy: 0.937500 (0.937500)\n",
      "[10/251] Loss: 1.591908 (1.560840) Accuracy: 0.875000 (0.906250)\n",
      "[20/251] Loss: 1.561604 (1.552984) Accuracy: 0.906250 (0.913690)\n",
      "[30/251] Loss: 1.558955 (1.552057) Accuracy: 0.906250 (0.914315)\n",
      "[40/251] Loss: 1.558977 (1.557384) Accuracy: 0.906250 (0.908537)\n",
      "[50/251] Loss: 1.560979 (1.558954) Accuracy: 0.906250 (0.906863)\n",
      "[60/251] Loss: 1.556608 (1.556822) Accuracy: 0.906250 (0.908811)\n",
      "[70/251] Loss: 1.529558 (1.557484) Accuracy: 0.937500 (0.908011)\n",
      "[80/251] Loss: 1.556886 (1.557508) Accuracy: 0.906250 (0.907793)\n",
      "[90/251] Loss: 1.555716 (1.557374) Accuracy: 0.906250 (0.907967)\n",
      "[100/251] Loss: 1.646385 (1.558222) Accuracy: 0.812500 (0.906869)\n",
      "[110/251] Loss: 1.620977 (1.558495) Accuracy: 0.843750 (0.906532)\n",
      "[120/251] Loss: 1.526008 (1.557953) Accuracy: 0.937500 (0.907025)\n",
      "[130/251] Loss: 1.586094 (1.558794) Accuracy: 0.875000 (0.906011)\n",
      "[140/251] Loss: 1.497398 (1.556713) Accuracy: 0.968750 (0.908023)\n",
      "[150/251] Loss: 1.526244 (1.557071) Accuracy: 0.937500 (0.907492)\n",
      "[160/251] Loss: 1.587493 (1.557620) Accuracy: 0.875000 (0.906832)\n",
      "[170/251] Loss: 1.555139 (1.557878) Accuracy: 0.906250 (0.906433)\n",
      "[180/251] Loss: 1.585618 (1.558912) Accuracy: 0.875000 (0.905214)\n",
      "[190/251] Loss: 1.616728 (1.557946) Accuracy: 0.843750 (0.906086)\n",
      "[200/251] Loss: 1.659313 (1.559302) Accuracy: 0.812500 (0.904695)\n",
      "[210/251] Loss: 1.590264 (1.569234) Accuracy: 0.906250 (0.894698)\n",
      "[220/251] Loss: 1.556009 (1.570965) Accuracy: 0.906250 (0.892958)\n",
      "[230/251] Loss: 1.584792 (1.569869) Accuracy: 0.875000 (0.893939)\n",
      "[240/251] Loss: 1.644458 (1.569344) Accuracy: 0.812500 (0.894321)\n",
      "[250/251] Loss: 1.461833 (1.568856) Accuracy: 1.000000 (0.894671)\n",
      "[0/63] Loss: 1.553975 (1.553975) Accuracy: 0.906250 (0.906250)\n",
      "[10/63] Loss: 1.525859 (1.541391) Accuracy: 0.937500 (0.920455)\n",
      "[20/63] Loss: 1.554171 (1.557735) Accuracy: 0.906250 (0.903274)\n",
      "[30/63] Loss: 1.585104 (1.553787) Accuracy: 0.875000 (0.907258)\n",
      "[40/63] Loss: 1.614501 (1.556215) Accuracy: 0.843750 (0.904726)\n",
      "[50/63] Loss: 1.555704 (1.554763) Accuracy: 0.906250 (0.906250)\n",
      "[60/63] Loss: 1.554299 (1.551798) Accuracy: 0.906250 (0.909324)\n",
      "Epoch: 4/100, Train Loss: 1.5689, Train Acc: 0.8947, Val. Loss: 1.5500, Val. Acc: 0.9111\n",
      "[0/251] Loss: 1.554145 (1.554145) Accuracy: 0.906250 (0.906250)\n",
      "[10/251] Loss: 1.524371 (1.546142) Accuracy: 0.937500 (0.914773)\n",
      "[20/251] Loss: 1.553971 (1.557107) Accuracy: 0.906250 (0.903274)\n",
      "[30/251] Loss: 1.615318 (1.564041) Accuracy: 0.843750 (0.896169)\n",
      "[40/251] Loss: 1.523977 (1.566074) Accuracy: 0.937500 (0.894055)\n",
      "[50/251] Loss: 1.493263 (1.555978) Accuracy: 0.968750 (0.904412)\n",
      "[60/251] Loss: 1.554353 (1.555615) Accuracy: 0.906250 (0.904713)\n",
      "[70/251] Loss: 1.613907 (1.559187) Accuracy: 0.843750 (0.900968)\n",
      "[80/251] Loss: 1.553698 (1.559642) Accuracy: 0.906250 (0.900463)\n",
      "[90/251] Loss: 1.583266 (1.560990) Accuracy: 0.875000 (0.899038)\n",
      "[100/251] Loss: 1.613649 (1.561137) Accuracy: 0.843750 (0.898824)\n",
      "[110/251] Loss: 1.553218 (1.562112) Accuracy: 0.906250 (0.897804)\n",
      "[120/251] Loss: 1.553130 (1.559161) Accuracy: 0.906250 (0.900826)\n",
      "[130/251] Loss: 1.523350 (1.558494) Accuracy: 0.937500 (0.901479)\n",
      "[140/251] Loss: 1.705272 (1.558562) Accuracy: 0.750000 (0.901374)\n",
      "[150/251] Loss: 1.582952 (1.557463) Accuracy: 0.875000 (0.902525)\n",
      "[160/251] Loss: 1.553693 (1.557393) Accuracy: 0.906250 (0.902562)\n",
      "[170/251] Loss: 1.584419 (1.557157) Accuracy: 0.875000 (0.902778)\n",
      "[180/251] Loss: 1.553246 (1.557612) Accuracy: 0.906250 (0.902279)\n",
      "[190/251] Loss: 1.552828 (1.558492) Accuracy: 0.906250 (0.901342)\n",
      "[200/251] Loss: 1.643105 (1.558967) Accuracy: 0.812500 (0.900808)\n",
      "[210/251] Loss: 1.462545 (1.558539) Accuracy: 1.000000 (0.901214)\n",
      "[220/251] Loss: 1.492750 (1.556916) Accuracy: 0.968750 (0.902856)\n",
      "[230/251] Loss: 1.522717 (1.556613) Accuracy: 0.937500 (0.903139)\n",
      "[240/251] Loss: 1.492987 (1.556600) Accuracy: 0.968750 (0.903138)\n",
      "[250/251] Loss: 1.461287 (1.556452) Accuracy: 1.000000 (0.903262)\n",
      "[0/63] Loss: 1.552392 (1.552392) Accuracy: 0.906250 (0.906250)\n",
      "[10/63] Loss: 1.523165 (1.539231) Accuracy: 0.937500 (0.920455)\n",
      "[20/63] Loss: 1.552586 (1.555661) Accuracy: 0.906250 (0.903274)\n",
      "[30/63] Loss: 1.582905 (1.551785) Accuracy: 0.875000 (0.907258)\n",
      "[40/63] Loss: 1.612734 (1.554222) Accuracy: 0.843750 (0.904726)\n",
      "[50/63] Loss: 1.553119 (1.552755) Accuracy: 0.906250 (0.906250)\n",
      "[60/63] Loss: 1.552632 (1.549802) Accuracy: 0.906250 (0.909324)\n",
      "Epoch: 5/100, Train Loss: 1.5565, Train Acc: 0.9033, Val. Loss: 1.5481, Val. Acc: 0.9111\n",
      "[0/251] Loss: 1.522647 (1.522647) Accuracy: 0.937500 (0.937500)\n",
      "[10/251] Loss: 1.612442 (1.568904) Accuracy: 0.843750 (0.889205)\n",
      "[20/251] Loss: 1.540002 (1.555426) Accuracy: 0.906250 (0.901786)\n",
      "[30/251] Loss: 1.647178 (1.574863) Accuracy: 0.781250 (0.879032)\n",
      "[40/251] Loss: 1.496544 (1.570868) Accuracy: 1.000000 (0.894055)\n",
      "[50/251] Loss: 1.507764 (1.557377) Accuracy: 1.000000 (0.914828)\n",
      "[60/251] Loss: 1.489493 (1.546224) Accuracy: 1.000000 (0.928791)\n",
      "[70/251] Loss: 1.488196 (1.538170) Accuracy: 1.000000 (0.938820)\n",
      "[80/251] Loss: 1.493615 (1.531468) Accuracy: 1.000000 (0.946373)\n",
      "[90/251] Loss: 1.481667 (1.526052) Accuracy: 1.000000 (0.952266)\n",
      "[100/251] Loss: 1.501097 (1.521903) Accuracy: 1.000000 (0.956993)\n",
      "[110/251] Loss: 1.481379 (1.518119) Accuracy: 1.000000 (0.960867)\n",
      "[120/251] Loss: 1.477110 (1.514987) Accuracy: 1.000000 (0.964101)\n",
      "[130/251] Loss: 1.481680 (1.512035) Accuracy: 1.000000 (0.966842)\n",
      "[140/251] Loss: 1.477169 (1.509415) Accuracy: 1.000000 (0.969193)\n",
      "[150/251] Loss: 1.478033 (1.507277) Accuracy: 1.000000 (0.971233)\n",
      "[160/251] Loss: 1.469379 (1.505228) Accuracy: 1.000000 (0.973020)\n",
      "[170/251] Loss: 1.477857 (1.503381) Accuracy: 1.000000 (0.974598)\n",
      "[180/251] Loss: 1.471431 (1.501731) Accuracy: 1.000000 (0.976001)\n",
      "[190/251] Loss: 1.469502 (1.500090) Accuracy: 1.000000 (0.977258)\n",
      "[200/251] Loss: 1.466282 (1.498608) Accuracy: 1.000000 (0.978389)\n",
      "[210/251] Loss: 1.468939 (1.497284) Accuracy: 1.000000 (0.979414)\n",
      "[220/251] Loss: 1.470757 (1.496053) Accuracy: 1.000000 (0.980345)\n",
      "[230/251] Loss: 1.467611 (1.494918) Accuracy: 1.000000 (0.981196)\n",
      "[240/251] Loss: 1.630952 (1.494647) Accuracy: 0.812500 (0.981198)\n",
      "[250/251] Loss: 1.461221 (1.498010) Accuracy: 1.000000 (0.977092)\n",
      "[0/63] Loss: 1.474310 (1.474310) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.495569 (1.477374) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.473538 (1.476837) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.482169 (1.476328) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.475082 (1.475876) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.489383 (1.475791) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.469570 (1.474862) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 6/100, Train Loss: 1.4980, Train Acc: 0.9771, Val. Loss: 1.4746, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.471314 (1.471314) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.471834 (1.470791) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.463296 (1.469537) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.466617 (1.469056) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.466131 (1.468644) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.467832 (1.468414) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.467630 (1.468269) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.465350 (1.468050) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.467185 (1.467813) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.465582 (1.467611) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.465281 (1.467396) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.465677 (1.467252) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.466585 (1.467102) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.465903 (1.467003) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.464692 (1.466861) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.466534 (1.466752) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.463325 (1.466640) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.464500 (1.466514) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.464998 (1.466390) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.463542 (1.466286) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.464198 (1.466177) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.463183 (1.466076) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.464512 (1.465991) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.463561 (1.465909) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.463003 (1.465832) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461654 (1.465735) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.463141 (1.463141) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.465030 (1.463855) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.463057 (1.463856) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.464343 (1.463784) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.464088 (1.463766) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.465278 (1.463787) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.463336 (1.463743) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 7/100, Train Loss: 1.4657, Train Acc: 1.0000, Val. Loss: 1.4637, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.463081 (1.463081) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.463122 (1.463420) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.463671 (1.463334) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.463553 (1.463290) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.463699 (1.463311) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.463597 (1.463296) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.464532 (1.463290) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.462987 (1.463248) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.463357 (1.463219) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.463064 (1.463210) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.462821 (1.463162) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.462390 (1.463094) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.462611 (1.463062) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.462430 (1.463030) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.462533 (1.462982) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.462110 (1.462941) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.462237 (1.462910) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.462325 (1.462880) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.462679 (1.462849) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.462067 (1.462823) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.462445 (1.462792) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461855 (1.462762) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461869 (1.462735) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461983 (1.462705) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461878 (1.462682) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461222 (1.462656) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461797 (1.461797) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.462481 (1.462093) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461820 (1.462070) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.462141 (1.462043) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.462086 (1.462036) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.462527 (1.462043) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461888 (1.462032) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 8/100, Train Loss: 1.4627, Train Acc: 1.0000, Val. Loss: 1.4620, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461806 (1.461806) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.462057 (1.461912) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461834 (1.461911) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.462198 (1.461932) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.462149 (1.461928) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461954 (1.461911) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.462008 (1.461913) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461534 (1.461882) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461888 (1.461863) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461716 (1.461848) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461571 (1.461836) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461770 (1.461816) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461602 (1.461803) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461622 (1.461789) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461680 (1.461775) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461539 (1.461765) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461748 (1.461756) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461416 (1.461748) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461444 (1.461736) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461473 (1.461724) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461700 (1.461712) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461353 (1.461701) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461617 (1.461693) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461513 (1.461684) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461543 (1.461675) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461822 (1.461666) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461398 (1.461398) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461543 (1.461475) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461400 (1.461481) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461509 (1.461472) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461503 (1.461473) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461710 (1.461478) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461397 (1.461472) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 9/100, Train Loss: 1.4617, Train Acc: 1.0000, Val. Loss: 1.4615, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461442 (1.461442) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461482 (1.461448) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461407 (1.461432) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461355 (1.461420) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461357 (1.461412) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461426 (1.461403) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461389 (1.461395) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461330 (1.461391) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461309 (1.461385) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461321 (1.461383) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461316 (1.461380) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461333 (1.461375) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461344 (1.461374) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461340 (1.461370) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461299 (1.461368) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461318 (1.461364) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461235 (1.461360) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461308 (1.461357) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461268 (1.461353) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461281 (1.461351) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461244 (1.461347) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461230 (1.461343) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461284 (1.461340) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461255 (1.461337) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461276 (1.461333) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461298 (1.461331) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461232 (1.461232) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461288 (1.461261) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461233 (1.461260) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461272 (1.461257) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461267 (1.461258) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461315 (1.461259) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461238 (1.461258) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 10/100, Train Loss: 1.4613, Train Acc: 1.0000, Val. Loss: 1.4613, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461261 (1.461261) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461240 (1.461253) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461253 (1.461254) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461239 (1.461251) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461267 (1.461252) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461213 (1.461249) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461230 (1.461246) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461209 (1.461243) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461221 (1.461241) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461198 (1.461239) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461222 (1.461237) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461223 (1.461236) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461221 (1.461234) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461197 (1.461232) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461197 (1.461230) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461206 (1.461229) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461216 (1.461228) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461207 (1.461226) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461192 (1.461224) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461201 (1.461223) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461187 (1.461222) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461193 (1.461220) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461200 (1.461219) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461199 (1.461218) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461182 (1.461217) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461216 (1.461216) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461183 (1.461183) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461200 (1.461192) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461183 (1.461192) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461194 (1.461191) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461194 (1.461191) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461212 (1.461191) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461182 (1.461191) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 11/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461189 (1.461189) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461187 (1.461189) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461182 (1.461186) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461186 (1.461186) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461181 (1.461184) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461190 (1.461184) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461185 (1.461184) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461186 (1.461184) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461178 (1.461183) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461186 (1.461182) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461177 (1.461182) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461171 (1.461181) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461177 (1.461181) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461171 (1.461180) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461172 (1.461180) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461168 (1.461179) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461175 (1.461179) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461173 (1.461178) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461172 (1.461178) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461179 (1.461177) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461163 (1.461177) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461168 (1.461176) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461164 (1.461176) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461168 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461162 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461168 (1.461174) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461162 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461172 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461163 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461168 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461166 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461174 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461164 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 12/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461164 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461164 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461161 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461164 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461162 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461158 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461161 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461160 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461161 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461159 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461161 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461159 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461160 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461159 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461160 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461159 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461161 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461156 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461155 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461156 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461157 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461156 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461157 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461158 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461156 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461159) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461158 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461159 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 13/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461156 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461156 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461153 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461153 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461153 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461152 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461152 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461154 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461153 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 14/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461153 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461153 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461153 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461154 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461184 (1.461184) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461154 (1.461174) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461170 (1.461178) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461243 (1.461187) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461326 (1.461193) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461179 (1.461211) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461168 (1.461207) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 15/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461246 (1.461246) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461174) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461152 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461152 (1.461158) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461152 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 16/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 17/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 18/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 19/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 20/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 21/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 22/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 23/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 24/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 25/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 26/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 27/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 28/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 29/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 30/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 31/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 32/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 33/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 34/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 35/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 36/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 37/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 38/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 39/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 40/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 41/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 42/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 43/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 44/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 45/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 46/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 47/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 48/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 49/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 50/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 51/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 52/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 53/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 54/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 55/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 56/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 57/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 58/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 59/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 60/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 61/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 62/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 63/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 64/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 65/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 66/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 67/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 68/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 69/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 70/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 71/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 72/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 73/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 74/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 75/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 76/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 77/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 78/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 79/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 80/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 81/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 82/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 83/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 84/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 85/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 86/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 87/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 88/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 89/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 90/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 91/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 92/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 93/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 94/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 95/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 96/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 97/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 98/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 99/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 100/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "Done training.\n",
      "Total length of dataset:  10000\n",
      "[0/251] Loss: 2.286860 (2.286860) Accuracy: 0.218750 (0.218750)\n",
      "[10/251] Loss: 2.309301 (2.305691) Accuracy: 0.125000 (0.139205)\n",
      "[20/251] Loss: 2.182343 (2.273700) Accuracy: 0.218750 (0.174107)\n",
      "[30/251] Loss: 2.203381 (2.245071) Accuracy: 0.218750 (0.199597)\n",
      "[40/251] Loss: 2.197927 (2.221161) Accuracy: 0.250000 (0.235518)\n",
      "[50/251] Loss: 2.080657 (2.200978) Accuracy: 0.343750 (0.250000)\n",
      "[60/251] Loss: 2.060262 (2.183763) Accuracy: 0.468750 (0.272029)\n",
      "[70/251] Loss: 2.017341 (2.165882) Accuracy: 0.468750 (0.293574)\n",
      "[80/251] Loss: 2.006871 (2.150834) Accuracy: 0.406250 (0.307870)\n",
      "[90/251] Loss: 2.032718 (2.139351) Accuracy: 0.406250 (0.316964)\n",
      "[100/251] Loss: 2.031499 (2.128382) Accuracy: 0.468750 (0.325804)\n",
      "[110/251] Loss: 2.030322 (2.115945) Accuracy: 0.437500 (0.340372)\n",
      "[120/251] Loss: 2.075185 (2.106334) Accuracy: 0.343750 (0.351498)\n",
      "[130/251] Loss: 1.860556 (2.098413) Accuracy: 0.656250 (0.359256)\n",
      "[140/251] Loss: 2.008039 (2.087087) Accuracy: 0.468750 (0.373227)\n",
      "[150/251] Loss: 1.863647 (2.076900) Accuracy: 0.625000 (0.385348)\n",
      "[160/251] Loss: 1.874831 (2.066142) Accuracy: 0.781250 (0.399651)\n",
      "[170/251] Loss: 1.863671 (2.056986) Accuracy: 0.625000 (0.411367)\n",
      "[180/251] Loss: 1.766021 (2.047897) Accuracy: 0.718750 (0.425242)\n",
      "[190/251] Loss: 1.849172 (2.036731) Accuracy: 0.687500 (0.439136)\n",
      "[200/251] Loss: 1.864124 (2.025778) Accuracy: 0.625000 (0.452736)\n",
      "[210/251] Loss: 1.950501 (2.017603) Accuracy: 0.562500 (0.463566)\n",
      "[220/251] Loss: 1.724661 (2.007321) Accuracy: 0.812500 (0.476103)\n",
      "[230/251] Loss: 1.683004 (1.998170) Accuracy: 0.812500 (0.486472)\n",
      "[240/251] Loss: 1.646013 (1.988491) Accuracy: 0.875000 (0.497147)\n",
      "[250/251] Loss: 2.412242 (1.980713) Accuracy: 0.000000 (0.505478)\n",
      "[0/63] Loss: 1.815703 (1.815703) Accuracy: 0.687500 (0.687500)\n",
      "[10/63] Loss: 1.708595 (1.751772) Accuracy: 0.812500 (0.755682)\n",
      "[20/63] Loss: 1.794573 (1.766088) Accuracy: 0.718750 (0.742560)\n",
      "[30/63] Loss: 1.898242 (1.779582) Accuracy: 0.593750 (0.727823)\n",
      "[40/63] Loss: 1.719775 (1.784628) Accuracy: 0.781250 (0.721037)\n",
      "[50/63] Loss: 1.713620 (1.788932) Accuracy: 0.812500 (0.716299)\n",
      "[60/63] Loss: 1.749116 (1.794678) Accuracy: 0.750000 (0.710041)\n",
      "Epoch: 1/100, Train Loss: 1.9807, Train Acc: 0.5055, Val. Loss: 1.7953, Val. Acc: 0.7095\n",
      "[0/251] Loss: 1.857092 (1.857092) Accuracy: 0.656250 (0.656250)\n",
      "[10/251] Loss: 1.752937 (1.798579) Accuracy: 0.750000 (0.696023)\n",
      "[20/251] Loss: 1.803812 (1.792115) Accuracy: 0.687500 (0.696429)\n",
      "[30/251] Loss: 1.728757 (1.796252) Accuracy: 0.750000 (0.688508)\n",
      "[40/251] Loss: 1.666213 (1.783213) Accuracy: 0.812500 (0.701220)\n",
      "[50/251] Loss: 1.698637 (1.783024) Accuracy: 0.781250 (0.699755)\n",
      "[60/251] Loss: 1.783363 (1.779936) Accuracy: 0.687500 (0.701844)\n",
      "[70/251] Loss: 1.777223 (1.779042) Accuracy: 0.687500 (0.701585)\n",
      "[80/251] Loss: 1.819909 (1.779323) Accuracy: 0.656250 (0.699460)\n",
      "[90/251] Loss: 1.684759 (1.779833) Accuracy: 0.812500 (0.697802)\n",
      "[100/251] Loss: 1.803149 (1.777809) Accuracy: 0.656250 (0.699567)\n",
      "[110/251] Loss: 1.990827 (1.777195) Accuracy: 0.437500 (0.699887)\n",
      "[120/251] Loss: 1.798504 (1.772831) Accuracy: 0.656250 (0.706353)\n",
      "[130/251] Loss: 1.698813 (1.771253) Accuracy: 0.812500 (0.710162)\n",
      "[140/251] Loss: 1.637522 (1.768260) Accuracy: 0.906250 (0.715426)\n",
      "[150/251] Loss: 1.632066 (1.762679) Accuracy: 0.875000 (0.723510)\n",
      "[160/251] Loss: 1.680000 (1.759612) Accuracy: 0.875000 (0.728067)\n",
      "[170/251] Loss: 1.673121 (1.757791) Accuracy: 0.843750 (0.731542)\n",
      "[180/251] Loss: 1.697661 (1.755166) Accuracy: 0.812500 (0.735325)\n",
      "[190/251] Loss: 1.659054 (1.751085) Accuracy: 0.843750 (0.740347)\n",
      "[200/251] Loss: 1.618329 (1.748661) Accuracy: 0.875000 (0.743159)\n",
      "[210/251] Loss: 1.741674 (1.745154) Accuracy: 0.750000 (0.746742)\n",
      "[220/251] Loss: 1.659734 (1.742620) Accuracy: 0.843750 (0.749293)\n",
      "[230/251] Loss: 1.621535 (1.739150) Accuracy: 0.875000 (0.753382)\n",
      "[240/251] Loss: 1.596626 (1.736377) Accuracy: 0.875000 (0.755705)\n",
      "[250/251] Loss: 1.472271 (1.734619) Accuracy: 1.000000 (0.756972)\n",
      "[0/63] Loss: 1.669693 (1.669693) Accuracy: 0.812500 (0.812500)\n",
      "[10/63] Loss: 1.608096 (1.647429) Accuracy: 0.875000 (0.832386)\n",
      "[20/63] Loss: 1.668609 (1.639865) Accuracy: 0.812500 (0.842262)\n",
      "[30/63] Loss: 1.704818 (1.656269) Accuracy: 0.781250 (0.824597)\n",
      "[40/63] Loss: 1.629990 (1.661454) Accuracy: 0.843750 (0.818598)\n",
      "[50/63] Loss: 1.581293 (1.669377) Accuracy: 0.906250 (0.810049)\n",
      "[60/63] Loss: 1.611402 (1.673728) Accuracy: 0.875000 (0.805840)\n",
      "Epoch: 2/100, Train Loss: 1.7346, Train Acc: 0.7570, Val. Loss: 1.6734, Val. Acc: 0.8063\n",
      "[0/251] Loss: 1.754786 (1.754786) Accuracy: 0.718750 (0.718750)\n",
      "[10/251] Loss: 1.619472 (1.699947) Accuracy: 0.875000 (0.778409)\n",
      "[20/251] Loss: 1.640265 (1.686198) Accuracy: 0.843750 (0.791667)\n",
      "[30/251] Loss: 1.535981 (1.661546) Accuracy: 0.968750 (0.815524)\n",
      "[40/251] Loss: 1.732819 (1.675891) Accuracy: 0.750000 (0.809451)\n",
      "[50/251] Loss: 1.633864 (1.670987) Accuracy: 0.875000 (0.824142)\n",
      "[60/251] Loss: 1.630166 (1.661349) Accuracy: 0.906250 (0.838627)\n",
      "[70/251] Loss: 1.578259 (1.650529) Accuracy: 0.937500 (0.852553)\n",
      "[80/251] Loss: 1.556077 (1.643068) Accuracy: 0.968750 (0.861883)\n",
      "[90/251] Loss: 1.557147 (1.637029) Accuracy: 0.937500 (0.868132)\n",
      "[100/251] Loss: 1.645238 (1.634295) Accuracy: 0.843750 (0.870359)\n",
      "[110/251] Loss: 1.618540 (1.632141) Accuracy: 0.875000 (0.871903)\n",
      "[120/251] Loss: 1.557318 (1.628374) Accuracy: 0.937500 (0.874742)\n",
      "[130/251] Loss: 1.541916 (1.624100) Accuracy: 0.968750 (0.878101)\n",
      "[140/251] Loss: 1.618113 (1.620118) Accuracy: 0.875000 (0.881206)\n",
      "[150/251] Loss: 1.546817 (1.616817) Accuracy: 0.937500 (0.883692)\n",
      "[160/251] Loss: 1.519397 (1.614601) Accuracy: 0.968750 (0.884899)\n",
      "[170/251] Loss: 1.550828 (1.611437) Accuracy: 0.937500 (0.887061)\n",
      "[180/251] Loss: 1.624365 (1.609550) Accuracy: 0.843750 (0.887949)\n",
      "[190/251] Loss: 1.597568 (1.609231) Accuracy: 0.875000 (0.887271)\n",
      "[200/251] Loss: 1.508479 (1.607467) Accuracy: 0.968750 (0.888060)\n",
      "[210/251] Loss: 1.603017 (1.607650) Accuracy: 0.875000 (0.886848)\n",
      "[220/251] Loss: 1.568342 (1.606242) Accuracy: 0.906250 (0.887443)\n",
      "[230/251] Loss: 1.567937 (1.604921) Accuracy: 0.906250 (0.887852)\n",
      "[240/251] Loss: 1.566344 (1.602811) Accuracy: 0.906250 (0.889134)\n",
      "[250/251] Loss: 1.461337 (1.602008) Accuracy: 1.000000 (0.889069)\n",
      "[0/63] Loss: 1.566366 (1.566366) Accuracy: 0.906250 (0.906250)\n",
      "[10/63] Loss: 1.505644 (1.566531) Accuracy: 0.968750 (0.903409)\n",
      "[20/63] Loss: 1.537589 (1.554684) Accuracy: 0.937500 (0.916667)\n",
      "[30/63] Loss: 1.653318 (1.564891) Accuracy: 0.812500 (0.906250)\n",
      "[40/63] Loss: 1.618637 (1.566497) Accuracy: 0.843750 (0.904726)\n",
      "[50/63] Loss: 1.533856 (1.567587) Accuracy: 0.937500 (0.903799)\n",
      "[60/63] Loss: 1.506693 (1.568839) Accuracy: 0.968750 (0.902664)\n",
      "Epoch: 3/100, Train Loss: 1.6020, Train Acc: 0.8891, Val. Loss: 1.5679, Val. Acc: 0.9037\n",
      "[0/251] Loss: 1.505621 (1.505621) Accuracy: 0.968750 (0.968750)\n",
      "[10/251] Loss: 1.535266 (1.554527) Accuracy: 0.937500 (0.917614)\n",
      "[20/251] Loss: 1.595815 (1.563800) Accuracy: 0.875000 (0.907738)\n",
      "[30/251] Loss: 1.591422 (1.565442) Accuracy: 0.875000 (0.905242)\n",
      "[40/251] Loss: 1.592008 (1.564744) Accuracy: 0.875000 (0.905488)\n",
      "[50/251] Loss: 1.590596 (1.565395) Accuracy: 0.875000 (0.904412)\n",
      "[60/251] Loss: 1.533074 (1.561726) Accuracy: 0.937500 (0.907787)\n",
      "[70/251] Loss: 1.530944 (1.563714) Accuracy: 0.937500 (0.905370)\n",
      "[80/251] Loss: 1.528910 (1.560705) Accuracy: 0.937500 (0.908179)\n",
      "[90/251] Loss: 1.530224 (1.563346) Accuracy: 0.937500 (0.905220)\n",
      "[100/251] Loss: 1.559285 (1.562758) Accuracy: 0.906250 (0.905631)\n",
      "[110/251] Loss: 1.589610 (1.563281) Accuracy: 0.875000 (0.904842)\n",
      "[120/251] Loss: 1.557189 (1.564188) Accuracy: 0.906250 (0.903667)\n",
      "[130/251] Loss: 1.499131 (1.562610) Accuracy: 0.968750 (0.905057)\n",
      "[140/251] Loss: 1.615701 (1.562080) Accuracy: 0.843750 (0.905363)\n",
      "[150/251] Loss: 1.558196 (1.560992) Accuracy: 0.906250 (0.906250)\n",
      "[160/251] Loss: 1.589432 (1.560563) Accuracy: 0.875000 (0.906444)\n",
      "[170/251] Loss: 1.618126 (1.560227) Accuracy: 0.843750 (0.906615)\n",
      "[180/251] Loss: 1.617759 (1.560760) Accuracy: 0.843750 (0.905905)\n",
      "[190/251] Loss: 1.525231 (1.559114) Accuracy: 0.937500 (0.907395)\n",
      "[200/251] Loss: 1.496026 (1.558666) Accuracy: 0.968750 (0.907649)\n",
      "[210/251] Loss: 1.620732 (1.559877) Accuracy: 0.843750 (0.906250)\n",
      "[220/251] Loss: 1.615319 (1.559710) Accuracy: 0.843750 (0.906250)\n",
      "[230/251] Loss: 1.528248 (1.560092) Accuracy: 0.937500 (0.905709)\n",
      "[240/251] Loss: 1.495054 (1.560562) Accuracy: 0.968750 (0.905083)\n",
      "[250/251] Loss: 1.469267 (1.560149) Accuracy: 1.000000 (0.905378)\n",
      "[0/63] Loss: 1.555449 (1.555449) Accuracy: 0.906250 (0.906250)\n",
      "[10/63] Loss: 1.495913 (1.558133) Accuracy: 0.968750 (0.903409)\n",
      "[20/63] Loss: 1.525822 (1.545563) Accuracy: 0.937500 (0.916667)\n",
      "[30/63] Loss: 1.646121 (1.555614) Accuracy: 0.812500 (0.906250)\n",
      "[40/63] Loss: 1.615063 (1.557081) Accuracy: 0.843750 (0.904726)\n",
      "[50/63] Loss: 1.526032 (1.557969) Accuracy: 0.937500 (0.903799)\n",
      "[60/63] Loss: 1.495645 (1.559090) Accuracy: 0.968750 (0.902664)\n",
      "Epoch: 4/100, Train Loss: 1.5601, Train Acc: 0.9054, Val. Loss: 1.5581, Val. Acc: 0.9037\n",
      "[0/251] Loss: 1.614793 (1.614793) Accuracy: 0.843750 (0.843750)\n",
      "[10/251] Loss: 1.494150 (1.552880) Accuracy: 0.968750 (0.909091)\n",
      "[20/251] Loss: 1.526720 (1.558587) Accuracy: 0.937500 (0.903274)\n",
      "[30/251] Loss: 1.555289 (1.553628) Accuracy: 0.906250 (0.908266)\n",
      "[40/251] Loss: 1.524448 (1.555427) Accuracy: 0.937500 (0.906250)\n",
      "[50/251] Loss: 1.525504 (1.553568) Accuracy: 0.937500 (0.908088)\n",
      "[60/251] Loss: 1.614183 (1.555235) Accuracy: 0.843750 (0.906250)\n",
      "[70/251] Loss: 1.523348 (1.557244) Accuracy: 0.937500 (0.904049)\n",
      "[80/251] Loss: 1.524864 (1.557690) Accuracy: 0.937500 (0.903549)\n",
      "[90/251] Loss: 1.494044 (1.554296) Accuracy: 0.968750 (0.906937)\n",
      "[100/251] Loss: 1.523940 (1.555235) Accuracy: 0.937500 (0.905941)\n",
      "[110/251] Loss: 1.583019 (1.554600) Accuracy: 0.875000 (0.906532)\n",
      "[120/251] Loss: 1.584075 (1.554048) Accuracy: 0.875000 (0.907025)\n",
      "[130/251] Loss: 1.612678 (1.554509) Accuracy: 0.843750 (0.906489)\n",
      "[140/251] Loss: 1.548003 (1.556082) Accuracy: 0.906250 (0.904699)\n",
      "[150/251] Loss: 1.513724 (1.559485) Accuracy: 0.937500 (0.900869)\n",
      "[160/251] Loss: 1.525422 (1.557362) Accuracy: 1.000000 (0.905862)\n",
      "[170/251] Loss: 1.528820 (1.555148) Accuracy: 0.906250 (0.910636)\n",
      "[180/251] Loss: 1.502250 (1.552754) Accuracy: 1.000000 (0.915573)\n",
      "[190/251] Loss: 1.524545 (1.549878) Accuracy: 1.000000 (0.919993)\n",
      "[200/251] Loss: 1.505776 (1.547436) Accuracy: 1.000000 (0.923507)\n",
      "[210/251] Loss: 1.494662 (1.545082) Accuracy: 1.000000 (0.927133)\n",
      "[220/251] Loss: 1.500073 (1.542859) Accuracy: 1.000000 (0.930430)\n",
      "[230/251] Loss: 1.501431 (1.541405) Accuracy: 1.000000 (0.932900)\n",
      "[240/251] Loss: 1.488088 (1.539042) Accuracy: 1.000000 (0.935685)\n",
      "[250/251] Loss: 1.491477 (1.537094) Accuracy: 1.000000 (0.938247)\n",
      "[0/63] Loss: 1.492289 (1.492289) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.476639 (1.492266) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.489943 (1.490112) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.506682 (1.492595) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.494601 (1.492309) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.487333 (1.492423) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.474288 (1.492382) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 5/100, Train Loss: 1.5371, Train Acc: 0.9382, Val. Loss: 1.4922, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.491322 (1.491322) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.481177 (1.489708) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.482169 (1.486372) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.475258 (1.484719) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.471993 (1.483546) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.477861 (1.482918) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.475046 (1.482197) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.472566 (1.481336) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.467540 (1.480471) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.470027 (1.480165) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.471610 (1.479609) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.473441 (1.479126) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.476015 (1.478670) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.468732 (1.478238) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.472251 (1.477741) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.468670 (1.477351) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.470501 (1.476979) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.469641 (1.476506) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.476460 (1.476218) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.468047 (1.475904) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.469730 (1.475587) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.468095 (1.475235) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.467932 (1.474925) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.467669 (1.474619) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.467267 (1.474336) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.484186 (1.474108) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.473431 (1.473431) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.471894 (1.471286) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.476575 (1.472346) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.468680 (1.472772) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.468648 (1.472392) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.477995 (1.472382) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.466147 (1.472112) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 6/100, Train Loss: 1.4741, Train Acc: 1.0000, Val. Loss: 1.4720, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.478034 (1.478034) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.463651 (1.498750) Accuracy: 1.000000 (0.974432)\n",
      "[20/251] Loss: 1.465844 (1.483673) Accuracy: 1.000000 (0.986607)\n",
      "[30/251] Loss: 1.466979 (1.478343) Accuracy: 1.000000 (0.990927)\n",
      "[40/251] Loss: 1.466665 (1.475205) Accuracy: 1.000000 (0.993140)\n",
      "[50/251] Loss: 1.465720 (1.473501) Accuracy: 1.000000 (0.994485)\n",
      "[60/251] Loss: 1.464300 (1.472242) Accuracy: 1.000000 (0.995389)\n",
      "[70/251] Loss: 1.465826 (1.471231) Accuracy: 1.000000 (0.996039)\n",
      "[80/251] Loss: 1.466531 (1.470556) Accuracy: 1.000000 (0.996528)\n",
      "[90/251] Loss: 1.465318 (1.469941) Accuracy: 1.000000 (0.996909)\n",
      "[100/251] Loss: 1.465485 (1.469470) Accuracy: 1.000000 (0.997215)\n",
      "[110/251] Loss: 1.462910 (1.468984) Accuracy: 1.000000 (0.997466)\n",
      "[120/251] Loss: 1.464502 (1.468642) Accuracy: 1.000000 (0.997676)\n",
      "[130/251] Loss: 1.465175 (1.468337) Accuracy: 1.000000 (0.997853)\n",
      "[140/251] Loss: 1.464466 (1.468058) Accuracy: 1.000000 (0.998005)\n",
      "[150/251] Loss: 1.463890 (1.467814) Accuracy: 1.000000 (0.998137)\n",
      "[160/251] Loss: 1.464160 (1.467598) Accuracy: 1.000000 (0.998253)\n",
      "[170/251] Loss: 1.464427 (1.467385) Accuracy: 1.000000 (0.998355)\n",
      "[180/251] Loss: 1.462825 (1.467200) Accuracy: 1.000000 (0.998446)\n",
      "[190/251] Loss: 1.465040 (1.467022) Accuracy: 1.000000 (0.998527)\n",
      "[200/251] Loss: 1.463308 (1.466858) Accuracy: 1.000000 (0.998601)\n",
      "[210/251] Loss: 1.462528 (1.466710) Accuracy: 1.000000 (0.998667)\n",
      "[220/251] Loss: 1.464516 (1.466584) Accuracy: 1.000000 (0.998727)\n",
      "[230/251] Loss: 1.463965 (1.466446) Accuracy: 1.000000 (0.998782)\n",
      "[240/251] Loss: 1.463986 (1.466310) Accuracy: 1.000000 (0.998833)\n",
      "[250/251] Loss: 1.463047 (1.466190) Accuracy: 1.000000 (0.998879)\n",
      "[0/63] Loss: 1.463584 (1.463584) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.462457 (1.463179) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.463226 (1.463147) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.463839 (1.463247) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.463110 (1.463229) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.462931 (1.463219) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.462394 (1.463217) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 7/100, Train Loss: 1.4662, Train Acc: 0.9989, Val. Loss: 1.4632, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.463263 (1.463263) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.463844 (1.463260) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.462641 (1.463196) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.462825 (1.463071) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.463205 (1.463035) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.462312 (1.463010) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.462496 (1.463021) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.462503 (1.462986) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.462956 (1.462948) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.462596 (1.462894) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.462840 (1.462889) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.462158 (1.462870) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.462277 (1.462812) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461937 (1.462775) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.462341 (1.462737) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.462269 (1.462709) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.462283 (1.462676) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.462956 (1.462649) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461761 (1.462622) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.462080 (1.462597) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461693 (1.462568) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461869 (1.462544) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.462137 (1.462521) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.462004 (1.462494) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461856 (1.462470) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461643 (1.462449) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461946 (1.461946) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461618 (1.461846) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461858 (1.461836) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.462084 (1.461871) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461801 (1.461864) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461755 (1.461860) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461600 (1.461860) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 8/100, Train Loss: 1.4624, Train Acc: 1.0000, Val. Loss: 1.4619, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461746 (1.461746) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461618 (1.461868) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461691 (1.461839) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461981 (1.461861) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461599 (1.461845) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461605 (1.461833) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461815 (1.461811) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461785 (1.461792) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461689 (1.461777) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461441 (1.461760) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461574 (1.461752) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461549 (1.461737) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461582 (1.461726) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461664 (1.461717) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461646 (1.461707) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461411 (1.461699) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461501 (1.461685) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461472 (1.461676) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461465 (1.461668) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461516 (1.461660) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461433 (1.461649) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461418 (1.461640) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461362 (1.461632) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461537 (1.461623) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461343 (1.461615) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461571 (1.461608) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461432 (1.461432) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461347 (1.461412) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461420 (1.461404) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461484 (1.461417) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461400 (1.461415) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461390 (1.461415) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461309 (1.461414) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 9/100, Train Loss: 1.4616, Train Acc: 1.0000, Val. Loss: 1.4614, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461596 (1.461596) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461452 (1.461408) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461317 (1.461416) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461367 (1.461400) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461446 (1.461398) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461308 (1.461394) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461447 (1.461386) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461295 (1.461382) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461300 (1.461376) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461395 (1.461372) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461338 (1.461368) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461267 (1.461363) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461257 (1.461357) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461296 (1.461354) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461284 (1.461351) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461259 (1.461347) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461305 (1.461343) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461249 (1.461340) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461278 (1.461335) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461332 (1.461332) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461306 (1.461329) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461290 (1.461326) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461261 (1.461324) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461252 (1.461320) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461202 (1.461318) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461236 (1.461315) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461259 (1.461259) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461219 (1.461241) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461248 (1.461243) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461265 (1.461246) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461233 (1.461245) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461242 (1.461245) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461215 (1.461245) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 10/100, Train Loss: 1.4613, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461253 (1.461253) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461252 (1.461250) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461219 (1.461248) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461258 (1.461245) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461235 (1.461242) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461215 (1.461239) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461229 (1.461238) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461237 (1.461237) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461221 (1.461236) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461206 (1.461234) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461209 (1.461232) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461204 (1.461230) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461196 (1.461228) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461208 (1.461227) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461226 (1.461226) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461211 (1.461224) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461202 (1.461223) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461211 (1.461222) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461214 (1.461220) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461200 (1.461219) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461194 (1.461218) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461195 (1.461217) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461200 (1.461216) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461185 (1.461215) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461189 (1.461214) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461212) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461189 (1.461189) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461177 (1.461185) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461186 (1.461185) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461196 (1.461186) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461182 (1.461186) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461183 (1.461186) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461176 (1.461186) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 11/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461195 (1.461195) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461175 (1.461187) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461176 (1.461186) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461194 (1.461187) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461182 (1.461185) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461190 (1.461184) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461182 (1.461184) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461172 (1.461183) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461182 (1.461183) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461175 (1.461182) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461182 (1.461182) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461169 (1.461181) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461179 (1.461180) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461174 (1.461180) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461171 (1.461179) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461175 (1.461179) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461165 (1.461178) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461169 (1.461178) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461164 (1.461177) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461171 (1.461177) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461163 (1.461176) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461168 (1.461176) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461171 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461168 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461164 (1.461175) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461152 (1.461174) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461165 (1.461165) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461161 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461165 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461169 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461162 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461163 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461161 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 12/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461169 (1.461169) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461162 (1.461166) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461164 (1.461165) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461166 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461159 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461160 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461159 (1.461164) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461162 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461159 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461161 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461158 (1.461163) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461160 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461159 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461160 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461157 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461157 (1.461162) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461158 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461157 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461156 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461157 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461155 (1.461161) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461158 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461156 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461156 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461157 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461160) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461158 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 13/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461157 (1.461157) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461158 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461158 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461154 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461156 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461154 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461155 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461156 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461153 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461153 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461153 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461154 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461153 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461155 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461154 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461152 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461155 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461173 (1.461173) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461154 (1.461173) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461172 (1.461169) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461183 (1.461171) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461179 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461157 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461154 (1.461172) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 14/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461193 (1.461193) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461153 (1.461158) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461153 (1.461156) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461153 (1.461155) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461152 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461153 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461152 (1.461154) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461152 (1.461153) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 15/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461152 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461152) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 16/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 17/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 18/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 19/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 20/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 21/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 22/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 23/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 24/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 25/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 26/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 27/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 28/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 29/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 30/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 31/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 32/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 33/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 34/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 35/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 36/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 37/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 38/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 39/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 40/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 41/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 42/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 43/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 44/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 45/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 46/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 47/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 48/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 49/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 50/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 51/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 52/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 53/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 54/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 55/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 56/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 57/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 58/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 59/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 60/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 61/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 62/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 63/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 64/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 65/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 66/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 67/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 68/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 69/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 70/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 71/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 72/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 73/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 74/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 75/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 76/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 77/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 78/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 79/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 80/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 81/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 82/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 83/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 84/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 85/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 86/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 87/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 88/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 89/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 90/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 91/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 92/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 93/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 94/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 95/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 96/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 97/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 98/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 99/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "[0/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[70/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[80/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[90/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[100/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[110/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[120/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[130/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[140/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[150/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[160/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[170/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[180/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[190/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[200/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[210/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[220/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[230/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[240/251] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[250/251] Loss: 1.461150 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[0/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[10/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[20/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[30/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[40/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[50/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "[60/63] Loss: 1.461151 (1.461151) Accuracy: 1.000000 (1.000000)\n",
      "Epoch: 100/100, Train Loss: 1.4612, Train Acc: 1.0000, Val. Loss: 1.4612, Val. Acc: 1.0000\n",
      "Done training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x1000 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from train import main\n",
    "\n",
    "# 配置训练参数的不同配置\n",
    "configs = [\n",
    "    {'input_length': 5, 'input_dim': 1, 'num_classes': 10, 'num_hidden': 128,\n",
    "     'batch_size': 32, 'learning_rate': 0.001, 'max_epoch': 100,\n",
    "     'max_norm': 10.0, 'data_size': 10000, 'portion_train': 0.8},\n",
    "\n",
    "    {'input_length': 4, 'input_dim': 1, 'num_classes': 10, 'num_hidden': 128,\n",
    "     'batch_size': 32, 'learning_rate': 0.001, 'max_epoch': 100,\n",
    "     'max_norm': 10.0, 'data_size': 10000, 'portion_train': 0.8},\n",
    "\n",
    "    {'input_length': 3, 'input_dim': 1, 'num_classes': 10, 'num_hidden': 128,\n",
    "     'batch_size': 32, 'learning_rate': 0.001, 'max_epoch': 100,\n",
    "     'max_norm': 10.0, 'data_size': 10000, 'portion_train': 0.8}\n",
    "]\n",
    "\n",
    "# 存储每个配置的训练和验证损失及准确率\n",
    "results = []\n",
    "\n",
    "# 对每个配置进行训练和评估\n",
    "for config_dict in configs:\n",
    "    class Config:\n",
    "        def __init__(self, **entries):\n",
    "            self.__dict__.update(entries)\n",
    "\n",
    "    config = Config(**config_dict)\n",
    "    train_loss, train_acc, val_loss, val_acc = main(config)\n",
    "    results.append((config.input_length, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "# 绘图\n",
    "epochs = range(1, configs[0]['max_epoch'] + 1)\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# 绘制损失\n",
    "plt.subplot(2, 1, 1)\n",
    "for i, (input_length, train_loss, train_acc, val_loss, val_acc) in enumerate(results):\n",
    "    plt.plot(epochs, train_loss, label=f'Input Length {input_length} Training Loss')\n",
    "    plt.plot(epochs, val_loss, label=f'Input Length {input_length} Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# 绘制准确率\n",
    "plt.subplot(2, 1, 2)\n",
    "for i, (input_length, train_loss, train_acc, val_loss, val_acc) in enumerate(results):\n",
    "    plt.plot(epochs, train_acc, label=f'Input Length {input_length} Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, label=f'Input Length {input_length} Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T06:33:18.305555Z",
     "start_time": "2024-05-15T06:27:48.132879Z"
    }
   },
   "id": "67d08e561c9cfd2e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T06:26:32.419956Z",
     "start_time": "2024-05-15T06:26:32.417705Z"
    }
   },
   "id": "db4e8dc58581c8e1",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T06:26:32.423303Z",
     "start_time": "2024-05-15T06:26:32.421075Z"
    }
   },
   "id": "7b31736af9645577",
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
